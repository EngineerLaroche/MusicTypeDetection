{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload chaque module spécifié par %aimport\n",
    "%autoreload 1\n",
    "\n",
    "%aimport src.kerasCallbacks\n",
    "%aimport src.results\n",
    "\n",
    "from IPython.display import display\n",
    "from os import path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from src import constants\n",
    "from src.kerasCallbacks import BestWeightsRestorer\n",
    "import src.results as resultsUtil\n",
    "import src.runUtil as runUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_feature_set():\n",
    "\n",
    "    file_path = path.join(constants.DATA_PATH, 'jmirmfccs_base_split.csv')\n",
    "    \n",
    "    ftrs = np.array(pd.read_csv(file_path, header=None).values[:,2:-1])\n",
    "    lbls = np.array(pd.read_csv(file_path, header=None).values[:,-1])\n",
    "    \n",
    "    return ftrs, lbls\n",
    "\n",
    "def scale_features(ftrs):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    ftrs_scld = scaler.fit_transform(ftrs)\n",
    "    \n",
    "    return ftrs_scld\n",
    "\n",
    "def one_hot_labels(lbls):\n",
    "    \n",
    "    lbls_1d = lbls.reshape(len(lbls), 1)\n",
    "        \n",
    "    oh_encoder = OneHotEncoder(sparse=False)\n",
    "    lbls_oh = oh_encoder.fit_transform(lbls_1d)\n",
    "    \n",
    "    return lbls_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143644, 26)\n"
     ]
    }
   ],
   "source": [
    "# Load the features and the labels\n",
    "raw_ftrs, raw_lbls = load_feature_set()\n",
    "\n",
    "lbls_oh = one_hot_labels(raw_lbls)\n",
    "lbl_encoder = LabelEncoder()\n",
    "lbl_encoded_lbls = lbl_encoder.fit_transform(raw_lbls)\n",
    "\n",
    "ftrs = scale_features(raw_ftrs)\n",
    "\n",
    "print(ftrs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    \n",
    "    inputs = Input(shape=input_shape, name='mlp_mfccs_input')\n",
    "    \n",
    "    layer = Dense(26, activation='relu', name='mlp_mfccs_dense_1')(inputs)\n",
    "    layer = Dense(60, activation='relu', name='mlp_mfccs_dense_2')(layer)\n",
    "    layer = Dense(45, activation='relu', name='mlp_mfccs_dense_3')(layer)\n",
    "    layer = Dense(30, activation='relu', name='mlp_mfccs_dense_4')(layer)\n",
    "    \n",
    "    outputs = Dense(25, activation='softmax', name='mlp_mfccs_output')(layer)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(), \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=[\n",
    "            CategoricalAccuracy(name='accuracy')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_number = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Train on 114904 samples, validate on 28740 samples\n",
      "Epoch 1/150\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 3.1174 - accuracy: 0.0834 - val_loss: 2.9474 - val_accuracy: 0.1275\n",
      "Epoch 2/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.8725 - accuracy: 0.1421 - val_loss: 2.7607 - val_accuracy: 0.1682\n",
      "Epoch 3/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.7584 - accuracy: 0.1686 - val_loss: 2.6967 - val_accuracy: 0.1836\n",
      "Epoch 4/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.7105 - accuracy: 0.1817 - val_loss: 2.6619 - val_accuracy: 0.1945\n",
      "Epoch 5/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.6778 - accuracy: 0.1902 - val_loss: 2.6342 - val_accuracy: 0.2017\n",
      "Epoch 6/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.6545 - accuracy: 0.1968 - val_loss: 2.6127 - val_accuracy: 0.2080\n",
      "Epoch 7/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.6356 - accuracy: 0.2025 - val_loss: 2.5973 - val_accuracy: 0.2112\n",
      "Epoch 8/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.6216 - accuracy: 0.2069 - val_loss: 2.5835 - val_accuracy: 0.2148\n",
      "Epoch 9/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.6072 - accuracy: 0.2115 - val_loss: 2.5747 - val_accuracy: 0.2173\n",
      "Epoch 10/150\n",
      "114904/114904 [==============================] - 0s 3us/sample - loss: 2.5987 - accuracy: 0.2152 - val_loss: 2.5658 - val_accuracy: 0.2195\n",
      "Epoch 11/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5903 - accuracy: 0.2159 - val_loss: 2.5595 - val_accuracy: 0.2219\n",
      "Epoch 12/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5837 - accuracy: 0.2185 - val_loss: 2.5534 - val_accuracy: 0.2220\n",
      "Epoch 13/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5763 - accuracy: 0.2211 - val_loss: 2.5491 - val_accuracy: 0.2239\n",
      "Epoch 14/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5723 - accuracy: 0.2220 - val_loss: 2.5448 - val_accuracy: 0.2268\n",
      "Epoch 15/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5688 - accuracy: 0.2226 - val_loss: 2.5412 - val_accuracy: 0.2257\n",
      "Epoch 16/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5639 - accuracy: 0.2242 - val_loss: 2.5371 - val_accuracy: 0.2286\n",
      "Epoch 17/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5590 - accuracy: 0.2248 - val_loss: 2.5358 - val_accuracy: 0.2287\n",
      "Epoch 18/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5566 - accuracy: 0.2266 - val_loss: 2.5328 - val_accuracy: 0.2283\n",
      "Epoch 19/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5541 - accuracy: 0.2276 - val_loss: 2.5309 - val_accuracy: 0.2283\n",
      "Epoch 20/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5500 - accuracy: 0.2274 - val_loss: 2.5283 - val_accuracy: 0.2309\n",
      "Epoch 21/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5485 - accuracy: 0.2263 - val_loss: 2.5298 - val_accuracy: 0.2319\n",
      "Epoch 22/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5465 - accuracy: 0.2294 - val_loss: 2.5260 - val_accuracy: 0.2318\n",
      "Epoch 23/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5437 - accuracy: 0.2298 - val_loss: 2.5237 - val_accuracy: 0.2319\n",
      "Epoch 24/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5417 - accuracy: 0.2300 - val_loss: 2.5219 - val_accuracy: 0.2317\n",
      "Epoch 25/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5382 - accuracy: 0.2315 - val_loss: 2.5197 - val_accuracy: 0.2358\n",
      "Epoch 26/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5365 - accuracy: 0.2316 - val_loss: 2.5193 - val_accuracy: 0.2351\n",
      "Epoch 27/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5348 - accuracy: 0.2327 - val_loss: 2.5185 - val_accuracy: 0.2335\n",
      "Epoch 28/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5344 - accuracy: 0.2320 - val_loss: 2.5171 - val_accuracy: 0.2349\n",
      "Epoch 29/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5327 - accuracy: 0.2327 - val_loss: 2.5138 - val_accuracy: 0.2356\n",
      "Epoch 30/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5301 - accuracy: 0.2329 - val_loss: 2.5151 - val_accuracy: 0.2342\n",
      "Epoch 31/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5285 - accuracy: 0.2335 - val_loss: 2.5118 - val_accuracy: 0.2354\n",
      "Epoch 32/150\n",
      "114904/114904 [==============================] - 0s 3us/sample - loss: 2.5262 - accuracy: 0.2344 - val_loss: 2.5118 - val_accuracy: 0.2330\n",
      "Epoch 33/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5261 - accuracy: 0.2344 - val_loss: 2.5103 - val_accuracy: 0.2359\n",
      "Epoch 34/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5243 - accuracy: 0.2347 - val_loss: 2.5094 - val_accuracy: 0.2362\n",
      "Epoch 35/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5241 - accuracy: 0.2350 - val_loss: 2.5085 - val_accuracy: 0.2367\n",
      "Epoch 36/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5203 - accuracy: 0.2358 - val_loss: 2.5067 - val_accuracy: 0.2381\n",
      "Epoch 37/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5209 - accuracy: 0.2368 - val_loss: 2.5067 - val_accuracy: 0.2369\n",
      "Epoch 38/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5197 - accuracy: 0.2372 - val_loss: 2.5047 - val_accuracy: 0.2390\n",
      "Epoch 39/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5186 - accuracy: 0.2357 - val_loss: 2.5044 - val_accuracy: 0.2378\n",
      "Epoch 40/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5158 - accuracy: 0.2379 - val_loss: 2.5043 - val_accuracy: 0.2379\n",
      "Epoch 41/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5150 - accuracy: 0.2374 - val_loss: 2.5029 - val_accuracy: 0.2384\n",
      "Epoch 42/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5148 - accuracy: 0.2374 - val_loss: 2.5025 - val_accuracy: 0.2385\n",
      "Epoch 43/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5146 - accuracy: 0.2374 - val_loss: 2.5021 - val_accuracy: 0.2391\n",
      "Epoch 44/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5101 - accuracy: 0.2385 - val_loss: 2.5015 - val_accuracy: 0.2378\n",
      "Epoch 45/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5110 - accuracy: 0.2385 - val_loss: 2.5009 - val_accuracy: 0.2403\n",
      "Epoch 46/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5106 - accuracy: 0.2394 - val_loss: 2.4990 - val_accuracy: 0.2400\n",
      "Epoch 47/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5094 - accuracy: 0.2400 - val_loss: 2.4994 - val_accuracy: 0.2390\n",
      "Epoch 48/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5089 - accuracy: 0.2387 - val_loss: 2.4974 - val_accuracy: 0.2387\n",
      "Epoch 49/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5073 - accuracy: 0.2400 - val_loss: 2.4980 - val_accuracy: 0.2392\n",
      "Epoch 50/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5049 - accuracy: 0.2397 - val_loss: 2.4968 - val_accuracy: 0.2398\n",
      "Epoch 51/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5059 - accuracy: 0.2393 - val_loss: 2.4988 - val_accuracy: 0.2402\n",
      "Epoch 52/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5046 - accuracy: 0.2388 - val_loss: 2.4976 - val_accuracy: 0.2399\n",
      "Epoch 53/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5030 - accuracy: 0.2404 - val_loss: 2.4966 - val_accuracy: 0.2395\n",
      "Epoch 54/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5032 - accuracy: 0.2416 - val_loss: 2.4955 - val_accuracy: 0.2397\n",
      "Epoch 55/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5033 - accuracy: 0.2414 - val_loss: 2.4961 - val_accuracy: 0.2397\n",
      "Epoch 56/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5009 - accuracy: 0.2416 - val_loss: 2.4934 - val_accuracy: 0.2417\n",
      "Epoch 57/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5003 - accuracy: 0.2423 - val_loss: 2.4929 - val_accuracy: 0.2405\n",
      "Epoch 58/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.5011 - accuracy: 0.2416 - val_loss: 2.4933 - val_accuracy: 0.2406\n",
      "Epoch 59/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4986 - accuracy: 0.2423 - val_loss: 2.4927 - val_accuracy: 0.2405\n",
      "Epoch 60/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4993 - accuracy: 0.2413 - val_loss: 2.4907 - val_accuracy: 0.2402\n",
      "Epoch 61/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4994 - accuracy: 0.2418 - val_loss: 2.4929 - val_accuracy: 0.2405\n",
      "Epoch 62/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4957 - accuracy: 0.2428 - val_loss: 2.4926 - val_accuracy: 0.2416\n",
      "Epoch 63/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4964 - accuracy: 0.2429 - val_loss: 2.4906 - val_accuracy: 0.2414\n",
      "Epoch 64/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4949 - accuracy: 0.2438 - val_loss: 2.4904 - val_accuracy: 0.2400\n",
      "Epoch 65/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4948 - accuracy: 0.2412 - val_loss: 2.4904 - val_accuracy: 0.2417\n",
      "Epoch 66/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4940 - accuracy: 0.2429 - val_loss: 2.4897 - val_accuracy: 0.2419\n",
      "Epoch 67/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4930 - accuracy: 0.2435 - val_loss: 2.4886 - val_accuracy: 0.2422\n",
      "Epoch 68/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4945 - accuracy: 0.2421 - val_loss: 2.4890 - val_accuracy: 0.2420\n",
      "Epoch 69/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4923 - accuracy: 0.2453 - val_loss: 2.4881 - val_accuracy: 0.2435\n",
      "Epoch 70/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4907 - accuracy: 0.2447 - val_loss: 2.4878 - val_accuracy: 0.2422\n",
      "Epoch 71/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4924 - accuracy: 0.2433 - val_loss: 2.4873 - val_accuracy: 0.2421\n",
      "Epoch 72/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4910 - accuracy: 0.2442 - val_loss: 2.4871 - val_accuracy: 0.2428\n",
      "Epoch 73/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4901 - accuracy: 0.2428 - val_loss: 2.4854 - val_accuracy: 0.2441\n",
      "Epoch 74/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4901 - accuracy: 0.2450 - val_loss: 2.4868 - val_accuracy: 0.2428\n",
      "Epoch 75/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4902 - accuracy: 0.2436 - val_loss: 2.4859 - val_accuracy: 0.2431\n",
      "Epoch 76/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4904 - accuracy: 0.2439 - val_loss: 2.4870 - val_accuracy: 0.2426\n",
      "Epoch 77/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4884 - accuracy: 0.2457 - val_loss: 2.4854 - val_accuracy: 0.2436\n",
      "Epoch 78/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4867 - accuracy: 0.2451 - val_loss: 2.4847 - val_accuracy: 0.2432\n",
      "Epoch 79/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4862 - accuracy: 0.2463 - val_loss: 2.4855 - val_accuracy: 0.2440\n",
      "Epoch 80/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4869 - accuracy: 0.2462 - val_loss: 2.4837 - val_accuracy: 0.2425\n",
      "Epoch 81/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4870 - accuracy: 0.2444 - val_loss: 2.4843 - val_accuracy: 0.2437\n",
      "Epoch 82/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4875 - accuracy: 0.2442 - val_loss: 2.4845 - val_accuracy: 0.2438\n",
      "Epoch 83/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4840 - accuracy: 0.2465 - val_loss: 2.4829 - val_accuracy: 0.2433\n",
      "Epoch 84/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4851 - accuracy: 0.2458 - val_loss: 2.4828 - val_accuracy: 0.2434\n",
      "Epoch 85/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4844 - accuracy: 0.2441 - val_loss: 2.4829 - val_accuracy: 0.2452\n",
      "Epoch 86/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4850 - accuracy: 0.2456 - val_loss: 2.4843 - val_accuracy: 0.2435\n",
      "Epoch 87/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4830 - accuracy: 0.2458 - val_loss: 2.4834 - val_accuracy: 0.2427\n",
      "Epoch 88/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4836 - accuracy: 0.2450 - val_loss: 2.4827 - val_accuracy: 0.2439\n",
      "Epoch 89/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4830 - accuracy: 0.2466 - val_loss: 2.4824 - val_accuracy: 0.2436\n",
      "Epoch 90/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4841 - accuracy: 0.2465 - val_loss: 2.4832 - val_accuracy: 0.2453\n",
      "Epoch 91/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4827 - accuracy: 0.2461 - val_loss: 2.4817 - val_accuracy: 0.2443\n",
      "Epoch 92/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4821 - accuracy: 0.2466 - val_loss: 2.4816 - val_accuracy: 0.2448\n",
      "Epoch 93/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4786 - accuracy: 0.2472 - val_loss: 2.4822 - val_accuracy: 0.2438\n",
      "Epoch 94/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4812 - accuracy: 0.2470 - val_loss: 2.4830 - val_accuracy: 0.2441\n",
      "Epoch 95/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4831 - accuracy: 0.2454 - val_loss: 2.4806 - val_accuracy: 0.2447\n",
      "Epoch 96/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4810 - accuracy: 0.2457 - val_loss: 2.4819 - val_accuracy: 0.2441\n",
      "Epoch 97/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4799 - accuracy: 0.2473 - val_loss: 2.4816 - val_accuracy: 0.2445\n",
      "Epoch 98/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4799 - accuracy: 0.2461 - val_loss: 2.4816 - val_accuracy: 0.2458\n",
      "Epoch 99/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4787 - accuracy: 0.2464 - val_loss: 2.4814 - val_accuracy: 0.2432\n",
      "Epoch 100/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4786 - accuracy: 0.2470 - val_loss: 2.4805 - val_accuracy: 0.2453\n",
      "Epoch 101/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4786 - accuracy: 0.2458 - val_loss: 2.4803 - val_accuracy: 0.2437\n",
      "Epoch 102/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4777 - accuracy: 0.2470 - val_loss: 2.4806 - val_accuracy: 0.2447\n",
      "Epoch 103/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4769 - accuracy: 0.2471 - val_loss: 2.4806 - val_accuracy: 0.2451\n",
      "Epoch 104/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4769 - accuracy: 0.2472 - val_loss: 2.4796 - val_accuracy: 0.2454\n",
      "Epoch 105/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4769 - accuracy: 0.2477 - val_loss: 2.4803 - val_accuracy: 0.2463\n",
      "Epoch 106/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4770 - accuracy: 0.2477 - val_loss: 2.4806 - val_accuracy: 0.2437\n",
      "Epoch 107/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4756 - accuracy: 0.2475 - val_loss: 2.4800 - val_accuracy: 0.2443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4776 - accuracy: 0.2473 - val_loss: 2.4791 - val_accuracy: 0.2465\n",
      "Epoch 109/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4767 - accuracy: 0.2463 - val_loss: 2.4797 - val_accuracy: 0.2443\n",
      "Epoch 110/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4773 - accuracy: 0.2475 - val_loss: 2.4801 - val_accuracy: 0.2454\n",
      "Epoch 111/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4773 - accuracy: 0.2462 - val_loss: 2.4795 - val_accuracy: 0.2443\n",
      "Epoch 112/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4749 - accuracy: 0.2481 - val_loss: 2.4787 - val_accuracy: 0.2460\n",
      "Epoch 113/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4755 - accuracy: 0.2481 - val_loss: 2.4787 - val_accuracy: 0.2452\n",
      "Epoch 114/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4751 - accuracy: 0.2479 - val_loss: 2.4798 - val_accuracy: 0.2452\n",
      "Epoch 115/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4736 - accuracy: 0.2489 - val_loss: 2.4774 - val_accuracy: 0.2460\n",
      "Epoch 116/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4758 - accuracy: 0.2484 - val_loss: 2.4791 - val_accuracy: 0.2464\n",
      "Epoch 117/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4746 - accuracy: 0.2478 - val_loss: 2.4789 - val_accuracy: 0.2451\n",
      "Epoch 118/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4718 - accuracy: 0.2483 - val_loss: 2.4785 - val_accuracy: 0.2455\n",
      "Epoch 119/150\n",
      "114904/114904 [==============================] - 0s 3us/sample - loss: 2.4714 - accuracy: 0.2486 - val_loss: 2.4774 - val_accuracy: 0.2448\n",
      "Epoch 120/150\n",
      "114904/114904 [==============================] - 0s 3us/sample - loss: 2.4720 - accuracy: 0.2490 - val_loss: 2.4773 - val_accuracy: 0.2457\n",
      "Epoch 121/150\n",
      "114904/114904 [==============================] - 0s 3us/sample - loss: 2.4724 - accuracy: 0.2495 - val_loss: 2.4783 - val_accuracy: 0.2452\n",
      "Epoch 122/150\n",
      "114904/114904 [==============================] - 0s 3us/sample - loss: 2.4725 - accuracy: 0.2471 - val_loss: 2.4791 - val_accuracy: 0.2445\n",
      "Epoch 123/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4735 - accuracy: 0.2479 - val_loss: 2.4779 - val_accuracy: 0.2463\n",
      "Epoch 124/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4717 - accuracy: 0.2479 - val_loss: 2.4775 - val_accuracy: 0.2465\n",
      "Epoch 125/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4715 - accuracy: 0.2482 - val_loss: 2.4772 - val_accuracy: 0.2460\n",
      "Epoch 126/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4730 - accuracy: 0.2475 - val_loss: 2.4777 - val_accuracy: 0.2446\n",
      "Epoch 127/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4712 - accuracy: 0.2490 - val_loss: 2.4778 - val_accuracy: 0.2456\n",
      "Epoch 128/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4719 - accuracy: 0.2481 - val_loss: 2.4780 - val_accuracy: 0.2426\n",
      "Epoch 129/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4705 - accuracy: 0.2483 - val_loss: 2.4772 - val_accuracy: 0.2430\n",
      "Epoch 130/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4713 - accuracy: 0.2493 - val_loss: 2.4770 - val_accuracy: 0.2457\n",
      "Epoch 131/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4725 - accuracy: 0.2481 - val_loss: 2.4788 - val_accuracy: 0.2422\n",
      "Epoch 132/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4718 - accuracy: 0.2486 - val_loss: 2.4767 - val_accuracy: 0.2452\n",
      "Epoch 133/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4707 - accuracy: 0.2498 - val_loss: 2.4784 - val_accuracy: 0.2454\n",
      "Epoch 134/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4691 - accuracy: 0.2495 - val_loss: 2.4758 - val_accuracy: 0.2448\n",
      "Epoch 135/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4700 - accuracy: 0.2485 - val_loss: 2.4764 - val_accuracy: 0.2447\n",
      "Epoch 136/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4692 - accuracy: 0.2499 - val_loss: 2.4761 - val_accuracy: 0.2447\n",
      "Epoch 137/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4681 - accuracy: 0.2498 - val_loss: 2.4765 - val_accuracy: 0.2456\n",
      "Epoch 138/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4690 - accuracy: 0.2495 - val_loss: 2.4779 - val_accuracy: 0.2447\n",
      "Epoch 139/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4693 - accuracy: 0.2497 - val_loss: 2.4760 - val_accuracy: 0.2451\n",
      "Epoch 140/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4686 - accuracy: 0.2496 - val_loss: 2.4765 - val_accuracy: 0.2458\n",
      "Epoch 141/150\n",
      "114904/114904 [==============================] - 0s 3us/sample - loss: 2.4676 - accuracy: 0.2498 - val_loss: 2.4759 - val_accuracy: 0.2458\n",
      "Epoch 142/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4680 - accuracy: 0.2497 - val_loss: 2.4759 - val_accuracy: 0.2458\n",
      "Epoch 143/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4687 - accuracy: 0.2490 - val_loss: 2.4765 - val_accuracy: 0.2450\n",
      "Epoch 144/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4687 - accuracy: 0.2498 - val_loss: 2.4758 - val_accuracy: 0.2448\n",
      "Epoch 145/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4664 - accuracy: 0.2506 - val_loss: 2.4755 - val_accuracy: 0.2444\n",
      "Epoch 146/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4670 - accuracy: 0.2473 - val_loss: 2.4755 - val_accuracy: 0.2451\n",
      "Epoch 147/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4669 - accuracy: 0.2512 - val_loss: 2.4766 - val_accuracy: 0.2451\n",
      "Epoch 148/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4673 - accuracy: 0.2498 - val_loss: 2.4756 - val_accuracy: 0.2466\n",
      "Epoch 149/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4671 - accuracy: 0.2491 - val_loss: 2.4774 - val_accuracy: 0.2447\n",
      "Epoch 150/150\n",
      "114904/114904 [==============================] - 0s 2us/sample - loss: 2.4654 - accuracy: 0.2491 - val_loss: 2.4764 - val_accuracy: 0.2441\n",
      "Restoring the best weights from epoch 150\n",
      "Training fold 2\n",
      "Train on 114911 samples, validate on 28733 samples\n",
      "Epoch 1/150\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 3.0909 - accuracy: 0.0934 - val_loss: 2.9162 - val_accuracy: 0.1408\n",
      "Epoch 2/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.8514 - accuracy: 0.1462 - val_loss: 2.7465 - val_accuracy: 0.1701\n",
      "Epoch 3/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.7495 - accuracy: 0.1674 - val_loss: 2.6885 - val_accuracy: 0.1868\n",
      "Epoch 4/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.7062 - accuracy: 0.1779 - val_loss: 2.6559 - val_accuracy: 0.1955\n",
      "Epoch 5/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.6789 - accuracy: 0.1873 - val_loss: 2.6296 - val_accuracy: 0.2030\n",
      "Epoch 6/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.6549 - accuracy: 0.1952 - val_loss: 2.6093 - val_accuracy: 0.2094\n",
      "Epoch 7/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.6376 - accuracy: 0.2005 - val_loss: 2.5945 - val_accuracy: 0.2128\n",
      "Epoch 8/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.6242 - accuracy: 0.2061 - val_loss: 2.5811 - val_accuracy: 0.2168\n",
      "Epoch 9/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.6123 - accuracy: 0.2093 - val_loss: 2.5700 - val_accuracy: 0.2218\n",
      "Epoch 10/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.6049 - accuracy: 0.2112 - val_loss: 2.5644 - val_accuracy: 0.2245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5974 - accuracy: 0.2135 - val_loss: 2.5546 - val_accuracy: 0.2263\n",
      "Epoch 12/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5880 - accuracy: 0.2153 - val_loss: 2.5486 - val_accuracy: 0.2281\n",
      "Epoch 13/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5824 - accuracy: 0.2180 - val_loss: 2.5433 - val_accuracy: 0.2296\n",
      "Epoch 14/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5785 - accuracy: 0.2181 - val_loss: 2.5403 - val_accuracy: 0.2309\n",
      "Epoch 15/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5740 - accuracy: 0.2199 - val_loss: 2.5358 - val_accuracy: 0.2321\n",
      "Epoch 16/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5684 - accuracy: 0.2219 - val_loss: 2.5331 - val_accuracy: 0.2320\n",
      "Epoch 17/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5646 - accuracy: 0.2236 - val_loss: 2.5284 - val_accuracy: 0.2327\n",
      "Epoch 18/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5604 - accuracy: 0.2243 - val_loss: 2.5249 - val_accuracy: 0.2356\n",
      "Epoch 19/150\n",
      "114911/114911 [==============================] - 0s 3us/sample - loss: 2.5581 - accuracy: 0.2252 - val_loss: 2.5220 - val_accuracy: 0.2364\n",
      "Epoch 20/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5529 - accuracy: 0.2255 - val_loss: 2.5201 - val_accuracy: 0.2360\n",
      "Epoch 21/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5529 - accuracy: 0.2263 - val_loss: 2.5175 - val_accuracy: 0.2364\n",
      "Epoch 22/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5507 - accuracy: 0.2268 - val_loss: 2.5140 - val_accuracy: 0.2362\n",
      "Epoch 23/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5450 - accuracy: 0.2276 - val_loss: 2.5115 - val_accuracy: 0.2366\n",
      "Epoch 24/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5444 - accuracy: 0.2277 - val_loss: 2.5095 - val_accuracy: 0.2375\n",
      "Epoch 25/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5402 - accuracy: 0.2280 - val_loss: 2.5079 - val_accuracy: 0.2384\n",
      "Epoch 26/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5405 - accuracy: 0.2289 - val_loss: 2.5092 - val_accuracy: 0.2378\n",
      "Epoch 27/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5371 - accuracy: 0.2297 - val_loss: 2.5058 - val_accuracy: 0.2396\n",
      "Epoch 28/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5351 - accuracy: 0.2294 - val_loss: 2.5038 - val_accuracy: 0.2396\n",
      "Epoch 29/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5334 - accuracy: 0.2316 - val_loss: 2.5020 - val_accuracy: 0.2401\n",
      "Epoch 30/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5327 - accuracy: 0.2309 - val_loss: 2.5005 - val_accuracy: 0.2402\n",
      "Epoch 31/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5309 - accuracy: 0.2323 - val_loss: 2.5001 - val_accuracy: 0.2413\n",
      "Epoch 32/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5270 - accuracy: 0.2322 - val_loss: 2.4989 - val_accuracy: 0.2409\n",
      "Epoch 33/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5268 - accuracy: 0.2315 - val_loss: 2.4971 - val_accuracy: 0.2418\n",
      "Epoch 34/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5238 - accuracy: 0.2330 - val_loss: 2.4972 - val_accuracy: 0.2415\n",
      "Epoch 35/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5235 - accuracy: 0.2337 - val_loss: 2.4957 - val_accuracy: 0.2432\n",
      "Epoch 36/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5221 - accuracy: 0.2334 - val_loss: 2.4950 - val_accuracy: 0.2414\n",
      "Epoch 37/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5200 - accuracy: 0.2327 - val_loss: 2.4940 - val_accuracy: 0.2417\n",
      "Epoch 38/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5188 - accuracy: 0.2342 - val_loss: 2.4942 - val_accuracy: 0.2415\n",
      "Epoch 39/150\n",
      "114911/114911 [==============================] - 0s 3us/sample - loss: 2.5170 - accuracy: 0.2358 - val_loss: 2.4947 - val_accuracy: 0.2394\n",
      "Epoch 40/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5161 - accuracy: 0.2358 - val_loss: 2.4922 - val_accuracy: 0.2433\n",
      "Epoch 41/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5166 - accuracy: 0.2341 - val_loss: 2.4913 - val_accuracy: 0.2425\n",
      "Epoch 42/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5153 - accuracy: 0.2361 - val_loss: 2.4907 - val_accuracy: 0.2423\n",
      "Epoch 43/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5149 - accuracy: 0.2361 - val_loss: 2.4887 - val_accuracy: 0.2425\n",
      "Epoch 44/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5117 - accuracy: 0.2374 - val_loss: 2.4886 - val_accuracy: 0.2439\n",
      "Epoch 45/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5132 - accuracy: 0.2367 - val_loss: 2.4887 - val_accuracy: 0.2433\n",
      "Epoch 46/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5099 - accuracy: 0.2375 - val_loss: 2.4885 - val_accuracy: 0.2432\n",
      "Epoch 47/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5082 - accuracy: 0.2372 - val_loss: 2.4884 - val_accuracy: 0.2418\n",
      "Epoch 48/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5080 - accuracy: 0.2365 - val_loss: 2.4881 - val_accuracy: 0.2449\n",
      "Epoch 49/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5075 - accuracy: 0.2371 - val_loss: 2.4874 - val_accuracy: 0.2435\n",
      "Epoch 50/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5073 - accuracy: 0.2374 - val_loss: 2.4866 - val_accuracy: 0.2432\n",
      "Epoch 51/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5061 - accuracy: 0.2374 - val_loss: 2.4863 - val_accuracy: 0.2433\n",
      "Epoch 52/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5041 - accuracy: 0.2378 - val_loss: 2.4861 - val_accuracy: 0.2451\n",
      "Epoch 53/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5053 - accuracy: 0.2391 - val_loss: 2.4845 - val_accuracy: 0.2448\n",
      "Epoch 54/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5036 - accuracy: 0.2396 - val_loss: 2.4849 - val_accuracy: 0.2435\n",
      "Epoch 55/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5030 - accuracy: 0.2371 - val_loss: 2.4859 - val_accuracy: 0.2437\n",
      "Epoch 56/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5015 - accuracy: 0.2384 - val_loss: 2.4856 - val_accuracy: 0.2430\n",
      "Epoch 57/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.5011 - accuracy: 0.2392 - val_loss: 2.4832 - val_accuracy: 0.2441\n",
      "Epoch 58/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4999 - accuracy: 0.2388 - val_loss: 2.4835 - val_accuracy: 0.2444\n",
      "Epoch 59/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4995 - accuracy: 0.2392 - val_loss: 2.4818 - val_accuracy: 0.2446\n",
      "Epoch 60/150\n",
      "114911/114911 [==============================] - 0s 3us/sample - loss: 2.5000 - accuracy: 0.2395 - val_loss: 2.4826 - val_accuracy: 0.2450\n",
      "Epoch 61/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4987 - accuracy: 0.2403 - val_loss: 2.4842 - val_accuracy: 0.2432\n",
      "Epoch 62/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4963 - accuracy: 0.2401 - val_loss: 2.4816 - val_accuracy: 0.2450\n",
      "Epoch 63/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4959 - accuracy: 0.2404 - val_loss: 2.4826 - val_accuracy: 0.2448\n",
      "Epoch 64/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4970 - accuracy: 0.2397 - val_loss: 2.4820 - val_accuracy: 0.2447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4964 - accuracy: 0.2402 - val_loss: 2.4810 - val_accuracy: 0.2436\n",
      "Epoch 66/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4947 - accuracy: 0.2415 - val_loss: 2.4813 - val_accuracy: 0.2451\n",
      "Epoch 67/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4941 - accuracy: 0.2414 - val_loss: 2.4804 - val_accuracy: 0.2454\n",
      "Epoch 68/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4946 - accuracy: 0.2407 - val_loss: 2.4803 - val_accuracy: 0.2463\n",
      "Epoch 69/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4932 - accuracy: 0.2422 - val_loss: 2.4803 - val_accuracy: 0.2461\n",
      "Epoch 70/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4929 - accuracy: 0.2416 - val_loss: 2.4803 - val_accuracy: 0.2464\n",
      "Epoch 71/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4924 - accuracy: 0.2413 - val_loss: 2.4795 - val_accuracy: 0.2447\n",
      "Epoch 72/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4919 - accuracy: 0.2405 - val_loss: 2.4802 - val_accuracy: 0.2452\n",
      "Epoch 73/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4918 - accuracy: 0.2418 - val_loss: 2.4795 - val_accuracy: 0.2459\n",
      "Epoch 74/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4890 - accuracy: 0.2424 - val_loss: 2.4797 - val_accuracy: 0.2464\n",
      "Epoch 75/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4880 - accuracy: 0.2419 - val_loss: 2.4784 - val_accuracy: 0.2454\n",
      "Epoch 76/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4894 - accuracy: 0.2426 - val_loss: 2.4794 - val_accuracy: 0.2472\n",
      "Epoch 77/150\n",
      "114911/114911 [==============================] - 0s 3us/sample - loss: 2.4890 - accuracy: 0.2426 - val_loss: 2.4789 - val_accuracy: 0.2469\n",
      "Epoch 78/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4887 - accuracy: 0.2426 - val_loss: 2.4789 - val_accuracy: 0.2457\n",
      "Epoch 79/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4893 - accuracy: 0.2421 - val_loss: 2.4783 - val_accuracy: 0.2472\n",
      "Epoch 80/150\n",
      "114911/114911 [==============================] - 0s 3us/sample - loss: 2.4885 - accuracy: 0.2433 - val_loss: 2.4775 - val_accuracy: 0.2459\n",
      "Epoch 81/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4852 - accuracy: 0.2439 - val_loss: 2.4768 - val_accuracy: 0.2461\n",
      "Epoch 82/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4860 - accuracy: 0.2429 - val_loss: 2.4782 - val_accuracy: 0.2455\n",
      "Epoch 83/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4859 - accuracy: 0.2433 - val_loss: 2.4775 - val_accuracy: 0.2458\n",
      "Epoch 84/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4844 - accuracy: 0.2434 - val_loss: 2.4783 - val_accuracy: 0.2463\n",
      "Epoch 85/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4848 - accuracy: 0.2430 - val_loss: 2.4780 - val_accuracy: 0.2469\n",
      "Epoch 86/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4846 - accuracy: 0.2435 - val_loss: 2.4767 - val_accuracy: 0.2475\n",
      "Epoch 87/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4860 - accuracy: 0.2424 - val_loss: 2.4781 - val_accuracy: 0.2474\n",
      "Epoch 88/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4845 - accuracy: 0.2440 - val_loss: 2.4772 - val_accuracy: 0.2466\n",
      "Epoch 89/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4832 - accuracy: 0.2443 - val_loss: 2.4759 - val_accuracy: 0.2484\n",
      "Epoch 90/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4845 - accuracy: 0.2431 - val_loss: 2.4762 - val_accuracy: 0.2466\n",
      "Epoch 91/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4820 - accuracy: 0.2442 - val_loss: 2.4762 - val_accuracy: 0.2464\n",
      "Epoch 92/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4827 - accuracy: 0.2439 - val_loss: 2.4767 - val_accuracy: 0.2472\n",
      "Epoch 93/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4826 - accuracy: 0.2450 - val_loss: 2.4754 - val_accuracy: 0.2472\n",
      "Epoch 94/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4809 - accuracy: 0.2447 - val_loss: 2.4771 - val_accuracy: 0.2457\n",
      "Epoch 95/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4810 - accuracy: 0.2434 - val_loss: 2.4772 - val_accuracy: 0.2467\n",
      "Epoch 96/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4826 - accuracy: 0.2449 - val_loss: 2.4761 - val_accuracy: 0.2463\n",
      "Epoch 97/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4803 - accuracy: 0.2449 - val_loss: 2.4749 - val_accuracy: 0.2459\n",
      "Epoch 98/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4815 - accuracy: 0.2435 - val_loss: 2.4767 - val_accuracy: 0.2458\n",
      "Epoch 99/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4799 - accuracy: 0.2456 - val_loss: 2.4763 - val_accuracy: 0.2471\n",
      "Epoch 100/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4805 - accuracy: 0.2454 - val_loss: 2.4754 - val_accuracy: 0.2464\n",
      "Epoch 101/150\n",
      "114911/114911 [==============================] - 0s 3us/sample - loss: 2.4799 - accuracy: 0.2450 - val_loss: 2.4761 - val_accuracy: 0.2464\n",
      "Epoch 102/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4796 - accuracy: 0.2448 - val_loss: 2.4754 - val_accuracy: 0.2471\n",
      "Epoch 103/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4784 - accuracy: 0.2443 - val_loss: 2.4747 - val_accuracy: 0.2461\n",
      "Epoch 104/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4782 - accuracy: 0.2444 - val_loss: 2.4748 - val_accuracy: 0.2472\n",
      "Epoch 105/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4790 - accuracy: 0.2453 - val_loss: 2.4770 - val_accuracy: 0.2474\n",
      "Epoch 106/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4782 - accuracy: 0.2465 - val_loss: 2.4753 - val_accuracy: 0.2477\n",
      "Epoch 107/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4782 - accuracy: 0.2451 - val_loss: 2.4741 - val_accuracy: 0.2472\n",
      "Epoch 108/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4782 - accuracy: 0.2450 - val_loss: 2.4745 - val_accuracy: 0.2480\n",
      "Epoch 109/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4770 - accuracy: 0.2452 - val_loss: 2.4745 - val_accuracy: 0.2474\n",
      "Epoch 110/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4778 - accuracy: 0.2461 - val_loss: 2.4746 - val_accuracy: 0.2455\n",
      "Epoch 111/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4760 - accuracy: 0.2459 - val_loss: 2.4738 - val_accuracy: 0.2480\n",
      "Epoch 112/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4747 - accuracy: 0.2462 - val_loss: 2.4736 - val_accuracy: 0.2471\n",
      "Epoch 113/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4760 - accuracy: 0.2464 - val_loss: 2.4754 - val_accuracy: 0.2467\n",
      "Epoch 114/150\n",
      "114911/114911 [==============================] - 0s 3us/sample - loss: 2.4760 - accuracy: 0.2466 - val_loss: 2.4730 - val_accuracy: 0.2482\n",
      "Epoch 115/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4750 - accuracy: 0.2461 - val_loss: 2.4727 - val_accuracy: 0.2475\n",
      "Epoch 116/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4746 - accuracy: 0.2467 - val_loss: 2.4734 - val_accuracy: 0.2486\n",
      "Epoch 117/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4752 - accuracy: 0.2457 - val_loss: 2.4741 - val_accuracy: 0.2469\n",
      "Epoch 118/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4751 - accuracy: 0.2467 - val_loss: 2.4735 - val_accuracy: 0.2460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4730 - accuracy: 0.2470 - val_loss: 2.4730 - val_accuracy: 0.2464\n",
      "Epoch 120/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4738 - accuracy: 0.2462 - val_loss: 2.4738 - val_accuracy: 0.2478\n",
      "Epoch 121/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4732 - accuracy: 0.2480 - val_loss: 2.4740 - val_accuracy: 0.2469\n",
      "Epoch 122/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4723 - accuracy: 0.2467 - val_loss: 2.4732 - val_accuracy: 0.2470\n",
      "Epoch 123/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4718 - accuracy: 0.2466 - val_loss: 2.4721 - val_accuracy: 0.2488\n",
      "Epoch 124/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4737 - accuracy: 0.2458 - val_loss: 2.4722 - val_accuracy: 0.2473\n",
      "Epoch 125/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4719 - accuracy: 0.2481 - val_loss: 2.4728 - val_accuracy: 0.2473\n",
      "Epoch 126/150\n",
      "114911/114911 [==============================] - 0s 3us/sample - loss: 2.4725 - accuracy: 0.2469 - val_loss: 2.4731 - val_accuracy: 0.2469\n",
      "Epoch 127/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4713 - accuracy: 0.2471 - val_loss: 2.4742 - val_accuracy: 0.2482\n",
      "Epoch 128/150\n",
      "114911/114911 [==============================] - 0s 3us/sample - loss: 2.4709 - accuracy: 0.2468 - val_loss: 2.4728 - val_accuracy: 0.2486\n",
      "Epoch 129/150\n",
      "114911/114911 [==============================] - 0s 3us/sample - loss: 2.4713 - accuracy: 0.2477 - val_loss: 2.4716 - val_accuracy: 0.2475\n",
      "Epoch 130/150\n",
      "114911/114911 [==============================] - 0s 3us/sample - loss: 2.4703 - accuracy: 0.2468 - val_loss: 2.4705 - val_accuracy: 0.2470\n",
      "Epoch 131/150\n",
      "114911/114911 [==============================] - 0s 3us/sample - loss: 2.4699 - accuracy: 0.2467 - val_loss: 2.4713 - val_accuracy: 0.2471\n",
      "Epoch 132/150\n",
      "114911/114911 [==============================] - 0s 3us/sample - loss: 2.4715 - accuracy: 0.2475 - val_loss: 2.4718 - val_accuracy: 0.2467\n",
      "Epoch 133/150\n",
      "114911/114911 [==============================] - 0s 3us/sample - loss: 2.4702 - accuracy: 0.2463 - val_loss: 2.4709 - val_accuracy: 0.2477\n",
      "Epoch 134/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4702 - accuracy: 0.2485 - val_loss: 2.4731 - val_accuracy: 0.2462\n",
      "Epoch 135/150\n",
      "114911/114911 [==============================] - 0s 3us/sample - loss: 2.4727 - accuracy: 0.2463 - val_loss: 2.4711 - val_accuracy: 0.2462\n",
      "Epoch 136/150\n",
      "114911/114911 [==============================] - 0s 3us/sample - loss: 2.4707 - accuracy: 0.2477 - val_loss: 2.4722 - val_accuracy: 0.2479\n",
      "Epoch 137/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4695 - accuracy: 0.2473 - val_loss: 2.4727 - val_accuracy: 0.2477\n",
      "Epoch 138/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4690 - accuracy: 0.2478 - val_loss: 2.4710 - val_accuracy: 0.2472\n",
      "Epoch 139/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4683 - accuracy: 0.2481 - val_loss: 2.4724 - val_accuracy: 0.2468\n",
      "Epoch 140/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4684 - accuracy: 0.2491 - val_loss: 2.4726 - val_accuracy: 0.2469\n",
      "Epoch 141/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4689 - accuracy: 0.2483 - val_loss: 2.4722 - val_accuracy: 0.2480\n",
      "Epoch 142/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4680 - accuracy: 0.2482 - val_loss: 2.4720 - val_accuracy: 0.2492\n",
      "Epoch 143/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4680 - accuracy: 0.2470 - val_loss: 2.4707 - val_accuracy: 0.2478\n",
      "Epoch 144/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4672 - accuracy: 0.2491 - val_loss: 2.4717 - val_accuracy: 0.2468\n",
      "Epoch 145/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4662 - accuracy: 0.2476 - val_loss: 2.4714 - val_accuracy: 0.2464\n",
      "Epoch 146/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4682 - accuracy: 0.2479 - val_loss: 2.4703 - val_accuracy: 0.2479\n",
      "Epoch 147/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4689 - accuracy: 0.2471 - val_loss: 2.4726 - val_accuracy: 0.2466\n",
      "Epoch 148/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4671 - accuracy: 0.2497 - val_loss: 2.4718 - val_accuracy: 0.2488\n",
      "Epoch 149/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4653 - accuracy: 0.2492 - val_loss: 2.4707 - val_accuracy: 0.2459\n",
      "Epoch 150/150\n",
      "114911/114911 [==============================] - 0s 2us/sample - loss: 2.4662 - accuracy: 0.2503 - val_loss: 2.4721 - val_accuracy: 0.2488\n",
      "Restoring the best weights from epoch 150\n",
      "Training fold 3\n",
      "Train on 114914 samples, validate on 28730 samples\n",
      "Epoch 1/150\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 3.0987 - accuracy: 0.0890 - val_loss: 2.9099 - val_accuracy: 0.1345\n",
      "Epoch 2/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.8547 - accuracy: 0.1415 - val_loss: 2.7653 - val_accuracy: 0.1726\n",
      "Epoch 3/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.7606 - accuracy: 0.1690 - val_loss: 2.6976 - val_accuracy: 0.1836\n",
      "Epoch 4/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.7045 - accuracy: 0.1856 - val_loss: 2.6550 - val_accuracy: 0.1933\n",
      "Epoch 5/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.6706 - accuracy: 0.1908 - val_loss: 2.6295 - val_accuracy: 0.1986\n",
      "Epoch 6/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.6482 - accuracy: 0.1985 - val_loss: 2.6103 - val_accuracy: 0.2045\n",
      "Epoch 7/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.6306 - accuracy: 0.2045 - val_loss: 2.5953 - val_accuracy: 0.2096\n",
      "Epoch 8/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.6172 - accuracy: 0.2083 - val_loss: 2.5841 - val_accuracy: 0.2150\n",
      "Epoch 9/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.6070 - accuracy: 0.2106 - val_loss: 2.5735 - val_accuracy: 0.2163\n",
      "Epoch 10/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5991 - accuracy: 0.2145 - val_loss: 2.5671 - val_accuracy: 0.2192\n",
      "Epoch 11/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5912 - accuracy: 0.2168 - val_loss: 2.5580 - val_accuracy: 0.2208\n",
      "Epoch 12/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5841 - accuracy: 0.2176 - val_loss: 2.5515 - val_accuracy: 0.2229\n",
      "Epoch 13/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5799 - accuracy: 0.2186 - val_loss: 2.5472 - val_accuracy: 0.2249\n",
      "Epoch 14/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5734 - accuracy: 0.2207 - val_loss: 2.5429 - val_accuracy: 0.2262\n",
      "Epoch 15/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5678 - accuracy: 0.2222 - val_loss: 2.5396 - val_accuracy: 0.2292\n",
      "Epoch 16/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5646 - accuracy: 0.2233 - val_loss: 2.5341 - val_accuracy: 0.2287\n",
      "Epoch 17/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5600 - accuracy: 0.2240 - val_loss: 2.5328 - val_accuracy: 0.2291\n",
      "Epoch 18/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5568 - accuracy: 0.2241 - val_loss: 2.5280 - val_accuracy: 0.2311\n",
      "Epoch 19/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5547 - accuracy: 0.2266 - val_loss: 2.5264 - val_accuracy: 0.2323\n",
      "Epoch 20/150\n",
      "114914/114914 [==============================] - 0s 3us/sample - loss: 2.5518 - accuracy: 0.2281 - val_loss: 2.5234 - val_accuracy: 0.2332\n",
      "Epoch 21/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5493 - accuracy: 0.2270 - val_loss: 2.5224 - val_accuracy: 0.2333\n",
      "Epoch 22/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5471 - accuracy: 0.2282 - val_loss: 2.5209 - val_accuracy: 0.2345\n",
      "Epoch 23/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5427 - accuracy: 0.2297 - val_loss: 2.5169 - val_accuracy: 0.2354\n",
      "Epoch 24/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5413 - accuracy: 0.2297 - val_loss: 2.5149 - val_accuracy: 0.2341\n",
      "Epoch 25/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5384 - accuracy: 0.2304 - val_loss: 2.5130 - val_accuracy: 0.2372\n",
      "Epoch 26/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5367 - accuracy: 0.2298 - val_loss: 2.5115 - val_accuracy: 0.2364\n",
      "Epoch 27/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5349 - accuracy: 0.2311 - val_loss: 2.5107 - val_accuracy: 0.2372\n",
      "Epoch 28/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5352 - accuracy: 0.2311 - val_loss: 2.5084 - val_accuracy: 0.2380\n",
      "Epoch 29/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5303 - accuracy: 0.2327 - val_loss: 2.5091 - val_accuracy: 0.2374\n",
      "Epoch 30/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5297 - accuracy: 0.2316 - val_loss: 2.5071 - val_accuracy: 0.2371\n",
      "Epoch 31/150\n",
      "114914/114914 [==============================] - 0s 3us/sample - loss: 2.5293 - accuracy: 0.2324 - val_loss: 2.5042 - val_accuracy: 0.2387\n",
      "Epoch 32/150\n",
      "114914/114914 [==============================] - 0s 3us/sample - loss: 2.5281 - accuracy: 0.2335 - val_loss: 2.5035 - val_accuracy: 0.2377\n",
      "Epoch 33/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5256 - accuracy: 0.2329 - val_loss: 2.5021 - val_accuracy: 0.2404\n",
      "Epoch 34/150\n",
      "114914/114914 [==============================] - 0s 3us/sample - loss: 2.5251 - accuracy: 0.2336 - val_loss: 2.5016 - val_accuracy: 0.2393\n",
      "Epoch 35/150\n",
      "114914/114914 [==============================] - 0s 3us/sample - loss: 2.5233 - accuracy: 0.2343 - val_loss: 2.5008 - val_accuracy: 0.2380\n",
      "Epoch 36/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5212 - accuracy: 0.2357 - val_loss: 2.5014 - val_accuracy: 0.2406\n",
      "Epoch 37/150\n",
      "114914/114914 [==============================] - 0s 3us/sample - loss: 2.5202 - accuracy: 0.2361 - val_loss: 2.4983 - val_accuracy: 0.2389\n",
      "Epoch 38/150\n",
      "114914/114914 [==============================] - 0s 3us/sample - loss: 2.5188 - accuracy: 0.2359 - val_loss: 2.4983 - val_accuracy: 0.2403\n",
      "Epoch 39/150\n",
      "114914/114914 [==============================] - 0s 3us/sample - loss: 2.5196 - accuracy: 0.2350 - val_loss: 2.4978 - val_accuracy: 0.2398\n",
      "Epoch 40/150\n",
      "114914/114914 [==============================] - 0s 3us/sample - loss: 2.5171 - accuracy: 0.2363 - val_loss: 2.4973 - val_accuracy: 0.2405\n",
      "Epoch 41/150\n",
      "114914/114914 [==============================] - 0s 3us/sample - loss: 2.5179 - accuracy: 0.2364 - val_loss: 2.4958 - val_accuracy: 0.2403\n",
      "Epoch 42/150\n",
      "114914/114914 [==============================] - 0s 3us/sample - loss: 2.5137 - accuracy: 0.2356 - val_loss: 2.4943 - val_accuracy: 0.2408\n",
      "Epoch 43/150\n",
      "114914/114914 [==============================] - 0s 3us/sample - loss: 2.5148 - accuracy: 0.2366 - val_loss: 2.4954 - val_accuracy: 0.2418\n",
      "Epoch 44/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5139 - accuracy: 0.2370 - val_loss: 2.4933 - val_accuracy: 0.2395\n",
      "Epoch 45/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5116 - accuracy: 0.2382 - val_loss: 2.4929 - val_accuracy: 0.2421\n",
      "Epoch 46/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5115 - accuracy: 0.2378 - val_loss: 2.4942 - val_accuracy: 0.2397\n",
      "Epoch 47/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5110 - accuracy: 0.2371 - val_loss: 2.4944 - val_accuracy: 0.2420\n",
      "Epoch 48/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5093 - accuracy: 0.2388 - val_loss: 2.4917 - val_accuracy: 0.2403\n",
      "Epoch 49/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5083 - accuracy: 0.2387 - val_loss: 2.4902 - val_accuracy: 0.2422\n",
      "Epoch 50/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5072 - accuracy: 0.2373 - val_loss: 2.4890 - val_accuracy: 0.2432\n",
      "Epoch 51/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5048 - accuracy: 0.2387 - val_loss: 2.4898 - val_accuracy: 0.2412\n",
      "Epoch 52/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5049 - accuracy: 0.2387 - val_loss: 2.4897 - val_accuracy: 0.2427\n",
      "Epoch 53/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5053 - accuracy: 0.2381 - val_loss: 2.4877 - val_accuracy: 0.2430\n",
      "Epoch 54/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5027 - accuracy: 0.2400 - val_loss: 2.4885 - val_accuracy: 0.2414\n",
      "Epoch 55/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5024 - accuracy: 0.2387 - val_loss: 2.4873 - val_accuracy: 0.2423\n",
      "Epoch 56/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5033 - accuracy: 0.2396 - val_loss: 2.4874 - val_accuracy: 0.2427\n",
      "Epoch 57/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5031 - accuracy: 0.2391 - val_loss: 2.4879 - val_accuracy: 0.2418\n",
      "Epoch 58/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5008 - accuracy: 0.2409 - val_loss: 2.4872 - val_accuracy: 0.2427\n",
      "Epoch 59/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4996 - accuracy: 0.2402 - val_loss: 2.4859 - val_accuracy: 0.2426\n",
      "Epoch 60/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5002 - accuracy: 0.2407 - val_loss: 2.4857 - val_accuracy: 0.2433\n",
      "Epoch 61/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.5002 - accuracy: 0.2417 - val_loss: 2.4849 - val_accuracy: 0.2418\n",
      "Epoch 62/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4986 - accuracy: 0.2405 - val_loss: 2.4852 - val_accuracy: 0.2433\n",
      "Epoch 63/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4988 - accuracy: 0.2404 - val_loss: 2.4853 - val_accuracy: 0.2414\n",
      "Epoch 64/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4956 - accuracy: 0.2420 - val_loss: 2.4861 - val_accuracy: 0.2441\n",
      "Epoch 65/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4966 - accuracy: 0.2398 - val_loss: 2.4858 - val_accuracy: 0.2430\n",
      "Epoch 66/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4970 - accuracy: 0.2409 - val_loss: 2.4833 - val_accuracy: 0.2432\n",
      "Epoch 67/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4950 - accuracy: 0.2417 - val_loss: 2.4836 - val_accuracy: 0.2435\n",
      "Epoch 68/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4964 - accuracy: 0.2406 - val_loss: 2.4827 - val_accuracy: 0.2434\n",
      "Epoch 69/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4945 - accuracy: 0.2421 - val_loss: 2.4838 - val_accuracy: 0.2441\n",
      "Epoch 70/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4947 - accuracy: 0.2425 - val_loss: 2.4844 - val_accuracy: 0.2424\n",
      "Epoch 71/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4934 - accuracy: 0.2417 - val_loss: 2.4825 - val_accuracy: 0.2454\n",
      "Epoch 72/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4942 - accuracy: 0.2420 - val_loss: 2.4830 - val_accuracy: 0.2420\n",
      "Epoch 73/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4940 - accuracy: 0.2428 - val_loss: 2.4830 - val_accuracy: 0.2435\n",
      "Epoch 74/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4923 - accuracy: 0.2423 - val_loss: 2.4839 - val_accuracy: 0.2444\n",
      "Epoch 75/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4924 - accuracy: 0.2424 - val_loss: 2.4809 - val_accuracy: 0.2428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4911 - accuracy: 0.2421 - val_loss: 2.4821 - val_accuracy: 0.2442\n",
      "Epoch 77/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4913 - accuracy: 0.2422 - val_loss: 2.4812 - val_accuracy: 0.2435\n",
      "Epoch 78/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4900 - accuracy: 0.2427 - val_loss: 2.4809 - val_accuracy: 0.2438\n",
      "Epoch 79/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4911 - accuracy: 0.2421 - val_loss: 2.4821 - val_accuracy: 0.2429\n",
      "Epoch 80/150\n",
      "114914/114914 [==============================] - 0s 3us/sample - loss: 2.4897 - accuracy: 0.2438 - val_loss: 2.4805 - val_accuracy: 0.2442\n",
      "Epoch 81/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4893 - accuracy: 0.2438 - val_loss: 2.4803 - val_accuracy: 0.2444\n",
      "Epoch 82/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4878 - accuracy: 0.2430 - val_loss: 2.4804 - val_accuracy: 0.2444\n",
      "Epoch 83/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4884 - accuracy: 0.2434 - val_loss: 2.4806 - val_accuracy: 0.2424\n",
      "Epoch 84/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4880 - accuracy: 0.2431 - val_loss: 2.4802 - val_accuracy: 0.2443\n",
      "Epoch 85/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4869 - accuracy: 0.2444 - val_loss: 2.4795 - val_accuracy: 0.2439\n",
      "Epoch 86/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4875 - accuracy: 0.2436 - val_loss: 2.4797 - val_accuracy: 0.2447\n",
      "Epoch 87/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4878 - accuracy: 0.2442 - val_loss: 2.4796 - val_accuracy: 0.2428\n",
      "Epoch 88/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4859 - accuracy: 0.2439 - val_loss: 2.4791 - val_accuracy: 0.2445\n",
      "Epoch 89/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4850 - accuracy: 0.2450 - val_loss: 2.4795 - val_accuracy: 0.2445\n",
      "Epoch 90/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4853 - accuracy: 0.2450 - val_loss: 2.4791 - val_accuracy: 0.2454\n",
      "Epoch 91/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4856 - accuracy: 0.2433 - val_loss: 2.4797 - val_accuracy: 0.2435\n",
      "Epoch 92/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4855 - accuracy: 0.2444 - val_loss: 2.4790 - val_accuracy: 0.2436\n",
      "Epoch 93/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4858 - accuracy: 0.2442 - val_loss: 2.4789 - val_accuracy: 0.2445\n",
      "Epoch 94/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4834 - accuracy: 0.2446 - val_loss: 2.4791 - val_accuracy: 0.2441\n",
      "Epoch 95/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4843 - accuracy: 0.2430 - val_loss: 2.4781 - val_accuracy: 0.2437\n",
      "Epoch 96/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4844 - accuracy: 0.2442 - val_loss: 2.4796 - val_accuracy: 0.2450\n",
      "Epoch 97/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4850 - accuracy: 0.2451 - val_loss: 2.4778 - val_accuracy: 0.2446\n",
      "Epoch 98/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4833 - accuracy: 0.2447 - val_loss: 2.4786 - val_accuracy: 0.2433\n",
      "Epoch 99/150\n",
      "114914/114914 [==============================] - 0s 2us/sample - loss: 2.4814 - accuracy: 0.2456 - val_loss: 2.4780 - val_accuracy: 0.2436\n",
      "Epoch 100/150\n",
      "114914/114914 [==============================] - 0s 3us/sample - loss: 2.4841 - accuracy: 0.2436 - val_loss: 2.4777 - val_accuracy: 0.2451\n",
      "Epoch 101/150\n",
      "  2000/114914 [..............................] - ETA: 0s - loss: 2.4965 - accuracy: 0.2360"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4dc72aa4ead8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mtrain_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_fold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/marc/data/Ecole/Univ/session9/gti770/tp4/GTI770_tp4/src/results.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(x, y, y_label_encoded, n_splits, train_func, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mstart_train_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mtrain_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_train_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-4dc72aa4ead8>\u001b[0m in \u001b[0;36mtrain_fold\u001b[0;34m(x_train, y_train, x_val, y_val, run_name, fold)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtk_board\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_restorer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml_main/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/ml_main/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml_main/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/ml_main/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_fold(x_train, y_train, x_val, y_val, run_name, fold=None):\n",
    "\n",
    "    input_shape = x_train[0].shape\n",
    "    model = create_model(input_shape)\n",
    "    \n",
    "    if fold is not None:\n",
    "        run_name += '_fold' + str(fold)\n",
    "        print('Training fold ' + str(fold))\n",
    "    \n",
    "    tk_board = TensorBoard(log_dir=constants.LOGS_PATH + run_name)\n",
    "    weight_restorer = BestWeightsRestorer(monitor='val_loss', mode='min', verbose=1)\n",
    "    \n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        validation_data=[x_val, y_val],\n",
    "        epochs=150,\n",
    "        batch_size=2000,\n",
    "        shuffle=True,\n",
    "        callbacks=[tk_board, weight_restorer],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    prob_predictions = model.predict(x_val)\n",
    "    predictions = prob_predictions.argmax(axis=1)\n",
    "    \n",
    "    return {\n",
    "        'history': history.history,\n",
    "        'predictions': predictions\n",
    "    }\n",
    "\n",
    "experiment_number += 1\n",
    "run_name = 'new_base_mlp_mfccs_cv_' + str(experiment_number)\n",
    "\n",
    "results = resultsUtil.cross_validate(\n",
    "    x=ftrs,\n",
    "    y=lbls_oh,\n",
    "    y_label_encoded=lbl_encoded_lbls,\n",
    "    n_splits=5,\n",
    "    train_func=train_fold,\n",
    "    run_name=run_name\n",
    ")\n",
    "\n",
    "runUtil.save_run_results(run_name, results)\n",
    "\n",
    "resultsdf = pd.DataFrame([results]).transpose()\n",
    "\n",
    "display(resultsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "143644/143644 [==============================] - 1s 4us/sample - loss: 3.0892 - accuracy: 0.1060\n",
      "Epoch 2/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.7708 - accuracy: 0.1651\n",
      "Epoch 3/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.6729 - accuracy: 0.1913\n",
      "Epoch 4/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.6281 - accuracy: 0.2053\n",
      "Epoch 5/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.5985 - accuracy: 0.2132\n",
      "Epoch 6/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.5774 - accuracy: 0.2198\n",
      "Epoch 7/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.5617 - accuracy: 0.2236\n",
      "Epoch 8/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.5494 - accuracy: 0.2275\n",
      "Epoch 9/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.5393 - accuracy: 0.2299\n",
      "Epoch 10/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.5305 - accuracy: 0.2318\n",
      "Epoch 11/150\n",
      "143644/143644 [==============================] - 0s 3us/sample - loss: 2.5240 - accuracy: 0.2328\n",
      "Epoch 12/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.5173 - accuracy: 0.2349\n",
      "Epoch 13/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.5128 - accuracy: 0.2361\n",
      "Epoch 14/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.5072 - accuracy: 0.2381\n",
      "Epoch 15/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.5043 - accuracy: 0.2384\n",
      "Epoch 16/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4993 - accuracy: 0.2400\n",
      "Epoch 17/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4960 - accuracy: 0.2412\n",
      "Epoch 18/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4933 - accuracy: 0.2424\n",
      "Epoch 19/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4906 - accuracy: 0.2431\n",
      "Epoch 20/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4868 - accuracy: 0.2439\n",
      "Epoch 21/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4844 - accuracy: 0.2443\n",
      "Epoch 22/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4818 - accuracy: 0.2456\n",
      "Epoch 23/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4799 - accuracy: 0.2456\n",
      "Epoch 24/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4771 - accuracy: 0.2464\n",
      "Epoch 25/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4749 - accuracy: 0.2471\n",
      "Epoch 26/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4729 - accuracy: 0.2475\n",
      "Epoch 27/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4711 - accuracy: 0.2480\n",
      "Epoch 28/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4701 - accuracy: 0.2483\n",
      "Epoch 29/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4676 - accuracy: 0.2491\n",
      "Epoch 30/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4661 - accuracy: 0.2492\n",
      "Epoch 31/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4636 - accuracy: 0.2496\n",
      "Epoch 32/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4629 - accuracy: 0.2495\n",
      "Epoch 33/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4622 - accuracy: 0.2504\n",
      "Epoch 34/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4603 - accuracy: 0.2510\n",
      "Epoch 35/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4593 - accuracy: 0.2506\n",
      "Epoch 36/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4579 - accuracy: 0.2518\n",
      "Epoch 37/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4564 - accuracy: 0.2510\n",
      "Epoch 38/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4549 - accuracy: 0.2517\n",
      "Epoch 39/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4531 - accuracy: 0.2518\n",
      "Epoch 40/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4526 - accuracy: 0.2525\n",
      "Epoch 41/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4514 - accuracy: 0.2534\n",
      "Epoch 42/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4502 - accuracy: 0.2527\n",
      "Epoch 43/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4498 - accuracy: 0.2525\n",
      "Epoch 44/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4482 - accuracy: 0.2537\n",
      "Epoch 45/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4473 - accuracy: 0.2542\n",
      "Epoch 46/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4465 - accuracy: 0.2541\n",
      "Epoch 47/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4455 - accuracy: 0.2536\n",
      "Epoch 48/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4443 - accuracy: 0.2541\n",
      "Epoch 49/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4440 - accuracy: 0.2543\n",
      "Epoch 50/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4426 - accuracy: 0.2548\n",
      "Epoch 51/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4419 - accuracy: 0.2547\n",
      "Epoch 52/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4410 - accuracy: 0.2562\n",
      "Epoch 53/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4403 - accuracy: 0.2558\n",
      "Epoch 54/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4399 - accuracy: 0.2560\n",
      "Epoch 55/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4382 - accuracy: 0.2567\n",
      "Epoch 56/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4374 - accuracy: 0.2565\n",
      "Epoch 57/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4379 - accuracy: 0.2559\n",
      "Epoch 58/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4364 - accuracy: 0.2567\n",
      "Epoch 59/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4357 - accuracy: 0.2573\n",
      "Epoch 60/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4353 - accuracy: 0.2572\n",
      "Epoch 61/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4336 - accuracy: 0.2572\n",
      "Epoch 62/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4343 - accuracy: 0.2566\n",
      "Epoch 63/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4328 - accuracy: 0.2577\n",
      "Epoch 64/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4325 - accuracy: 0.2583\n",
      "Epoch 65/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4319 - accuracy: 0.2578\n",
      "Epoch 66/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4309 - accuracy: 0.2583\n",
      "Epoch 67/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4306 - accuracy: 0.2586\n",
      "Epoch 68/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4295 - accuracy: 0.2583\n",
      "Epoch 69/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4297 - accuracy: 0.2593\n",
      "Epoch 70/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4287 - accuracy: 0.2588\n",
      "Epoch 71/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4283 - accuracy: 0.2590\n",
      "Epoch 72/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4273 - accuracy: 0.2595\n",
      "Epoch 73/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4265 - accuracy: 0.2594\n",
      "Epoch 74/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4263 - accuracy: 0.2594\n",
      "Epoch 75/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4263 - accuracy: 0.2591\n",
      "Epoch 76/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4256 - accuracy: 0.2593\n",
      "Epoch 77/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4246 - accuracy: 0.2602\n",
      "Epoch 78/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4243 - accuracy: 0.2604\n",
      "Epoch 79/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4235 - accuracy: 0.2603\n",
      "Epoch 80/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4227 - accuracy: 0.2608\n",
      "Epoch 81/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4231 - accuracy: 0.2601\n",
      "Epoch 82/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4222 - accuracy: 0.2606\n",
      "Epoch 83/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4219 - accuracy: 0.2610\n",
      "Epoch 84/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4217 - accuracy: 0.2605\n",
      "Epoch 85/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4214 - accuracy: 0.2604\n",
      "Epoch 86/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4207 - accuracy: 0.2617\n",
      "Epoch 87/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4205 - accuracy: 0.2614\n",
      "Epoch 88/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4200 - accuracy: 0.2607\n",
      "Epoch 89/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4184 - accuracy: 0.2616\n",
      "Epoch 90/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4185 - accuracy: 0.2620\n",
      "Epoch 91/150\n",
      "143644/143644 [==============================] - 0s 3us/sample - loss: 2.4174 - accuracy: 0.2614\n",
      "Epoch 92/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4179 - accuracy: 0.2621\n",
      "Epoch 93/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4175 - accuracy: 0.2622\n",
      "Epoch 94/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4163 - accuracy: 0.2625\n",
      "Epoch 95/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4160 - accuracy: 0.2628\n",
      "Epoch 96/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4164 - accuracy: 0.2622\n",
      "Epoch 97/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4156 - accuracy: 0.2626\n",
      "Epoch 98/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4151 - accuracy: 0.2630\n",
      "Epoch 99/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4147 - accuracy: 0.2630\n",
      "Epoch 100/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4138 - accuracy: 0.2641\n",
      "Epoch 101/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4139 - accuracy: 0.2639\n",
      "Epoch 102/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4130 - accuracy: 0.2635\n",
      "Epoch 103/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4129 - accuracy: 0.2642\n",
      "Epoch 104/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4126 - accuracy: 0.2635\n",
      "Epoch 105/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4121 - accuracy: 0.2637\n",
      "Epoch 106/150\n",
      "143644/143644 [==============================] - 0s 3us/sample - loss: 2.4115 - accuracy: 0.2648\n",
      "Epoch 107/150\n",
      "143644/143644 [==============================] - 0s 3us/sample - loss: 2.4106 - accuracy: 0.2645\n",
      "Epoch 108/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4114 - accuracy: 0.2641\n",
      "Epoch 109/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4105 - accuracy: 0.2645\n",
      "Epoch 110/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4103 - accuracy: 0.2641\n",
      "Epoch 111/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4102 - accuracy: 0.2647\n",
      "Epoch 112/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4097 - accuracy: 0.2649\n",
      "Epoch 113/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4089 - accuracy: 0.2650\n",
      "Epoch 114/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4087 - accuracy: 0.2645\n",
      "Epoch 115/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4088 - accuracy: 0.2654\n",
      "Epoch 116/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4086 - accuracy: 0.2651\n",
      "Epoch 117/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4082 - accuracy: 0.2653\n",
      "Epoch 118/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4079 - accuracy: 0.2647\n",
      "Epoch 119/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4080 - accuracy: 0.2657\n",
      "Epoch 120/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4074 - accuracy: 0.2651\n",
      "Epoch 121/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4064 - accuracy: 0.2654\n",
      "Epoch 122/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4058 - accuracy: 0.2659\n",
      "Epoch 123/150\n",
      "143644/143644 [==============================] - 0s 3us/sample - loss: 2.4063 - accuracy: 0.2658\n",
      "Epoch 124/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4058 - accuracy: 0.2661\n",
      "Epoch 125/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4052 - accuracy: 0.2661\n",
      "Epoch 126/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4054 - accuracy: 0.2658\n",
      "Epoch 127/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4052 - accuracy: 0.2657\n",
      "Epoch 128/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4049 - accuracy: 0.2656\n",
      "Epoch 129/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4050 - accuracy: 0.2661\n",
      "Epoch 130/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4036 - accuracy: 0.2668\n",
      "Epoch 131/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4043 - accuracy: 0.2662\n",
      "Epoch 132/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4039 - accuracy: 0.2659\n",
      "Epoch 133/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4038 - accuracy: 0.2661\n",
      "Epoch 134/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4024 - accuracy: 0.2666\n",
      "Epoch 135/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4027 - accuracy: 0.2676\n",
      "Epoch 136/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4025 - accuracy: 0.2671\n",
      "Epoch 137/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4027 - accuracy: 0.2660\n",
      "Epoch 138/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4022 - accuracy: 0.2668\n",
      "Epoch 139/150\n",
      "143644/143644 [==============================] - 0s 3us/sample - loss: 2.4015 - accuracy: 0.2665\n",
      "Epoch 140/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4014 - accuracy: 0.2668\n",
      "Epoch 141/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4015 - accuracy: 0.2669\n",
      "Epoch 142/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4008 - accuracy: 0.2677\n",
      "Epoch 143/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4006 - accuracy: 0.2669\n",
      "Epoch 144/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4007 - accuracy: 0.2667\n",
      "Epoch 145/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4009 - accuracy: 0.2668\n",
      "Epoch 146/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4000 - accuracy: 0.2678\n",
      "Epoch 147/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.3996 - accuracy: 0.2669\n",
      "Epoch 148/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.4000 - accuracy: 0.2670\n",
      "Epoch 149/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.3992 - accuracy: 0.2679\n",
      "Epoch 150/150\n",
      "143644/143644 [==============================] - 0s 2us/sample - loss: 2.3990 - accuracy: 0.2670\n"
     ]
    }
   ],
   "source": [
    "def train_final_model(x, y, run_name):\n",
    "    \n",
    "    save_path = constants.MODELS_PATH + run_name + '.h5' \n",
    "    \n",
    "    input_shape = x[0].shape\n",
    "    model = create_model(input_shape)\n",
    "    \n",
    "    tk_board = TensorBoard(log_dir=constants.LOGS_PATH + run_name)\n",
    "    \n",
    "    model.fit(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        epochs=150,\n",
    "        batch_size=2000,\n",
    "        shuffle=True,\n",
    "        callbacks=[tk_board],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model.save(save_path)\n",
    "\n",
    "\n",
    "train_final_model(ftrs, lbls_oh, 'base_mlp_mfccs_final1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
