{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload chaque module spécifié par %aimport\n",
    "%autoreload 1\n",
    "\n",
    "%aimport src.kerasCallbacks\n",
    "%aimport src.results\n",
    "\n",
    "from IPython.display import display\n",
    "from os import path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "from src import constants\n",
    "from src.kerasCallbacks import BestWeightsRestorer\n",
    "import src.results as resultsUtil\n",
    "import src.runUtil as runUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_feature_set():\n",
    "\n",
    "    file_path = path.join(constants.RAW_TAGGED_FEATURE_SET_PATH, \"msd-marsyas_dev_new/msd-marsyas_dev_new.csv\")\n",
    "    \n",
    "    ftrs = np.array(pd.read_csv(file_path, header=None).values[:,2:-1])\n",
    "    lbls = np.array(pd.read_csv(file_path, header=None).values[:,-1])\n",
    "    \n",
    "    return ftrs, lbls\n",
    "\n",
    "def scale_features(ftrs):\n",
    "    \n",
    "    standardscaler = StandardScaler()\n",
    "    ftrs_scld = standardscaler.fit_transform(ftrs)\n",
    "    \n",
    "    return ftrs_scld\n",
    "\n",
    "def one_hot_labels(lbls):\n",
    "    \n",
    "    lbls_1d = lbls.reshape(len(lbls), 1)\n",
    "        \n",
    "    oh_encoder = OneHotEncoder(sparse=False)\n",
    "    lbls_oh = oh_encoder.fit_transform(lbls_1d)\n",
    "    \n",
    "    return lbls_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the features and the labels\n",
    "raw_ftrs, raw_lbls = load_feature_set()\n",
    "\n",
    "lbls_oh = one_hot_labels(raw_lbls)\n",
    "lbl_encoder = LabelEncoder()\n",
    "lbl_encoded_lbls = lbl_encoder.fit_transform(raw_lbls)\n",
    "\n",
    "ftrs = scale_features(raw_ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    layer = Dense(90, activation='relu')(inputs)\n",
    "    layer = Dense(85, activation='relu')(layer)\n",
    "    \n",
    "    layer = Dropout(0.10)(layer)\n",
    "    \n",
    "    layer = Dense(80, activation='relu')(layer)\n",
    "    layer = Dense(75, activation='relu')(layer)\n",
    "    layer = Dense(70, activation='relu')(layer)\n",
    "    \n",
    "    layer = BatchNormalization()(layer)\n",
    "    \n",
    "    layer = Dense(65, activation='relu')(layer)\n",
    "    layer = Dense(60, activation='relu')(layer)\n",
    "    \n",
    "    layer = BatchNormalization()(layer)\n",
    "    \n",
    "    layer = Dense(55, activation='relu')(layer)\n",
    "    layer = Dense(50, activation='relu')(layer)\n",
    "    \n",
    "    layer = Dropout(0.10)(layer)\n",
    "    \n",
    "    layer = Dense(45, activation='relu')(layer)\n",
    "    layer = Dense(40, activation='relu')(layer)\n",
    "    \n",
    "    layer = Dropout(0.10)(layer)\n",
    "    \n",
    "    layer = Dense(30, activation='relu')(layer)\n",
    "    \n",
    "    outputs = Dense(25, activation='softmax')(layer)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(lr=0.001), \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=[\n",
    "            CategoricalAccuracy(name='accuracy')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Train on 143633 samples, validate on 35922 samples\n",
      "Epoch 1/110\n",
      "143633/143633 [==============================] - 2s 17us/sample - loss: 3.0158 - accuracy: 0.1112 - val_loss: 2.9455 - val_accuracy: 0.1593\n",
      "Epoch 2/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.6599 - accuracy: 0.2007 - val_loss: 2.7164 - val_accuracy: 0.1943\n",
      "Epoch 3/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.5588 - accuracy: 0.2189 - val_loss: 2.5540 - val_accuracy: 0.2261\n",
      "Epoch 4/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.5096 - accuracy: 0.2337 - val_loss: 2.4751 - val_accuracy: 0.2413\n",
      "Epoch 5/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.4767 - accuracy: 0.2447 - val_loss: 2.4383 - val_accuracy: 0.2539\n",
      "Epoch 6/110\n",
      "143633/143633 [==============================] - 1s 6us/sample - loss: 2.4476 - accuracy: 0.2518 - val_loss: 2.4064 - val_accuracy: 0.2634\n",
      "Epoch 7/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.4261 - accuracy: 0.2614 - val_loss: 2.3963 - val_accuracy: 0.2677\n",
      "Epoch 8/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.4066 - accuracy: 0.2668 - val_loss: 2.3721 - val_accuracy: 0.2738\n",
      "Epoch 9/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.3895 - accuracy: 0.2719 - val_loss: 2.3615 - val_accuracy: 0.2747\n",
      "Epoch 10/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.3730 - accuracy: 0.2768 - val_loss: 2.3577 - val_accuracy: 0.2778\n",
      "Epoch 11/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.3626 - accuracy: 0.2792 - val_loss: 2.3415 - val_accuracy: 0.2836\n",
      "Epoch 12/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.3508 - accuracy: 0.2828 - val_loss: 2.3360 - val_accuracy: 0.2834\n",
      "Epoch 13/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.3408 - accuracy: 0.2864 - val_loss: 2.3384 - val_accuracy: 0.2836\n",
      "Epoch 14/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.3322 - accuracy: 0.2881 - val_loss: 2.3257 - val_accuracy: 0.2851\n",
      "Epoch 15/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.3218 - accuracy: 0.2902 - val_loss: 2.3195 - val_accuracy: 0.2887\n",
      "Epoch 16/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.3173 - accuracy: 0.2926 - val_loss: 2.3154 - val_accuracy: 0.2879\n",
      "Epoch 17/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.3105 - accuracy: 0.2934 - val_loss: 2.3157 - val_accuracy: 0.2884\n",
      "Epoch 18/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.3032 - accuracy: 0.2955 - val_loss: 2.3104 - val_accuracy: 0.2898\n",
      "Epoch 19/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2962 - accuracy: 0.2982 - val_loss: 2.2944 - val_accuracy: 0.2941\n",
      "Epoch 20/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2918 - accuracy: 0.2984 - val_loss: 2.3005 - val_accuracy: 0.2935\n",
      "Epoch 21/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2866 - accuracy: 0.3008 - val_loss: 2.2946 - val_accuracy: 0.2945\n",
      "Epoch 22/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2839 - accuracy: 0.3013 - val_loss: 2.2919 - val_accuracy: 0.2944\n",
      "Epoch 23/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2766 - accuracy: 0.3024 - val_loss: 2.2912 - val_accuracy: 0.2961\n",
      "Epoch 24/110\n",
      "143633/143633 [==============================] - 1s 6us/sample - loss: 2.2721 - accuracy: 0.3038 - val_loss: 2.2869 - val_accuracy: 0.2951\n",
      "Epoch 25/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2699 - accuracy: 0.3051 - val_loss: 2.2911 - val_accuracy: 0.2962\n",
      "Epoch 26/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2653 - accuracy: 0.3055 - val_loss: 2.2841 - val_accuracy: 0.2977\n",
      "Epoch 27/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2595 - accuracy: 0.3082 - val_loss: 2.2853 - val_accuracy: 0.3003\n",
      "Epoch 28/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2542 - accuracy: 0.3092 - val_loss: 2.2870 - val_accuracy: 0.2994\n",
      "Epoch 29/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2520 - accuracy: 0.3097 - val_loss: 2.2807 - val_accuracy: 0.2983\n",
      "Epoch 30/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2501 - accuracy: 0.3092 - val_loss: 2.2888 - val_accuracy: 0.2941\n",
      "Epoch 31/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2448 - accuracy: 0.3104 - val_loss: 2.2777 - val_accuracy: 0.2978\n",
      "Epoch 32/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2431 - accuracy: 0.3105 - val_loss: 2.2784 - val_accuracy: 0.2992\n",
      "Epoch 33/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2399 - accuracy: 0.3134 - val_loss: 2.2804 - val_accuracy: 0.2987\n",
      "Epoch 34/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2351 - accuracy: 0.3138 - val_loss: 2.2844 - val_accuracy: 0.2976\n",
      "Epoch 35/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2344 - accuracy: 0.3137 - val_loss: 2.2797 - val_accuracy: 0.2991\n",
      "Epoch 36/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2312 - accuracy: 0.3148 - val_loss: 2.2789 - val_accuracy: 0.3003\n",
      "Epoch 37/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2272 - accuracy: 0.3167 - val_loss: 2.2833 - val_accuracy: 0.2995\n",
      "Epoch 38/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2230 - accuracy: 0.3165 - val_loss: 2.2784 - val_accuracy: 0.3004\n",
      "Epoch 39/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2225 - accuracy: 0.3173 - val_loss: 2.2785 - val_accuracy: 0.3017\n",
      "Epoch 40/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2215 - accuracy: 0.3176 - val_loss: 2.2779 - val_accuracy: 0.3020\n",
      "Epoch 41/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2184 - accuracy: 0.3186 - val_loss: 2.2716 - val_accuracy: 0.3023\n",
      "Epoch 42/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2148 - accuracy: 0.3195 - val_loss: 2.2776 - val_accuracy: 0.3018\n",
      "Epoch 43/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2113 - accuracy: 0.3212 - val_loss: 2.2788 - val_accuracy: 0.3014\n",
      "Epoch 44/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2114 - accuracy: 0.3204 - val_loss: 2.2691 - val_accuracy: 0.2999\n",
      "Epoch 45/110\n",
      "143633/143633 [==============================] - 1s 6us/sample - loss: 2.2075 - accuracy: 0.3205 - val_loss: 2.2682 - val_accuracy: 0.3026\n",
      "Epoch 46/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2028 - accuracy: 0.3231 - val_loss: 2.2772 - val_accuracy: 0.3001\n",
      "Epoch 47/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2047 - accuracy: 0.3231 - val_loss: 2.2672 - val_accuracy: 0.3032\n",
      "Epoch 48/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.2021 - accuracy: 0.3238 - val_loss: 2.2713 - val_accuracy: 0.3015\n",
      "Epoch 49/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1998 - accuracy: 0.3247 - val_loss: 2.2706 - val_accuracy: 0.3030\n",
      "Epoch 50/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1987 - accuracy: 0.3243 - val_loss: 2.2738 - val_accuracy: 0.3033\n",
      "Epoch 51/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1965 - accuracy: 0.3248 - val_loss: 2.2783 - val_accuracy: 0.3005\n",
      "Epoch 52/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1945 - accuracy: 0.3247 - val_loss: 2.2757 - val_accuracy: 0.3024\n",
      "Epoch 53/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1911 - accuracy: 0.3252 - val_loss: 2.2694 - val_accuracy: 0.3023\n",
      "Epoch 54/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1905 - accuracy: 0.3270 - val_loss: 2.2698 - val_accuracy: 0.3017\n",
      "Epoch 55/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1883 - accuracy: 0.3267 - val_loss: 2.2694 - val_accuracy: 0.3038\n",
      "Epoch 56/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1866 - accuracy: 0.3266 - val_loss: 2.2714 - val_accuracy: 0.3003\n",
      "Epoch 57/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1861 - accuracy: 0.3280 - val_loss: 2.2760 - val_accuracy: 0.3043\n",
      "Epoch 58/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1850 - accuracy: 0.3271 - val_loss: 2.2725 - val_accuracy: 0.3014\n",
      "Epoch 59/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1846 - accuracy: 0.3301 - val_loss: 2.2648 - val_accuracy: 0.3040\n",
      "Epoch 60/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1814 - accuracy: 0.3295 - val_loss: 2.2825 - val_accuracy: 0.2997\n",
      "Epoch 61/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1800 - accuracy: 0.3287 - val_loss: 2.2717 - val_accuracy: 0.3025\n",
      "Epoch 62/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1770 - accuracy: 0.3300 - val_loss: 2.2686 - val_accuracy: 0.3006\n",
      "Epoch 63/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1732 - accuracy: 0.3312 - val_loss: 2.2715 - val_accuracy: 0.3031\n",
      "Epoch 64/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1753 - accuracy: 0.3300 - val_loss: 2.2716 - val_accuracy: 0.3001\n",
      "Epoch 65/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1696 - accuracy: 0.3295 - val_loss: 2.2680 - val_accuracy: 0.3023\n",
      "Epoch 66/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1733 - accuracy: 0.3317 - val_loss: 2.2689 - val_accuracy: 0.3027\n",
      "Epoch 67/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1695 - accuracy: 0.3318 - val_loss: 2.2711 - val_accuracy: 0.3018\n",
      "Epoch 68/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1668 - accuracy: 0.3328 - val_loss: 2.2776 - val_accuracy: 0.3025\n",
      "Epoch 69/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1697 - accuracy: 0.3320 - val_loss: 2.2613 - val_accuracy: 0.3057\n",
      "Epoch 70/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1674 - accuracy: 0.3325 - val_loss: 2.2725 - val_accuracy: 0.3004\n",
      "Epoch 71/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1653 - accuracy: 0.3332 - val_loss: 2.2743 - val_accuracy: 0.3030\n",
      "Epoch 72/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1643 - accuracy: 0.3326 - val_loss: 2.2633 - val_accuracy: 0.3030\n",
      "Epoch 73/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1581 - accuracy: 0.3349 - val_loss: 2.2745 - val_accuracy: 0.3029\n",
      "Epoch 74/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1597 - accuracy: 0.3349 - val_loss: 2.2642 - val_accuracy: 0.3023\n",
      "Epoch 75/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1595 - accuracy: 0.3350 - val_loss: 2.2691 - val_accuracy: 0.3023\n",
      "Epoch 76/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1587 - accuracy: 0.3353 - val_loss: 2.2790 - val_accuracy: 0.3026\n",
      "Epoch 77/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1572 - accuracy: 0.3357 - val_loss: 2.2704 - val_accuracy: 0.3048\n",
      "Epoch 78/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1553 - accuracy: 0.3347 - val_loss: 2.2714 - val_accuracy: 0.3037\n",
      "Epoch 79/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1545 - accuracy: 0.3360 - val_loss: 2.2725 - val_accuracy: 0.3030\n",
      "Epoch 80/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1514 - accuracy: 0.3366 - val_loss: 2.2701 - val_accuracy: 0.3025\n",
      "Epoch 81/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1506 - accuracy: 0.3365 - val_loss: 2.2664 - val_accuracy: 0.3028\n",
      "Epoch 82/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1515 - accuracy: 0.3362 - val_loss: 2.2651 - val_accuracy: 0.3032\n",
      "Epoch 83/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1494 - accuracy: 0.3373 - val_loss: 2.2704 - val_accuracy: 0.3042\n",
      "Epoch 84/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1461 - accuracy: 0.3370 - val_loss: 2.2687 - val_accuracy: 0.3062\n",
      "Epoch 85/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1489 - accuracy: 0.3385 - val_loss: 2.2659 - val_accuracy: 0.3045\n",
      "Epoch 86/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1447 - accuracy: 0.3384 - val_loss: 2.2712 - val_accuracy: 0.3046\n",
      "Epoch 87/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1465 - accuracy: 0.3389 - val_loss: 2.2824 - val_accuracy: 0.3019\n",
      "Epoch 88/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1445 - accuracy: 0.3383 - val_loss: 2.2715 - val_accuracy: 0.3044\n",
      "Epoch 89/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1450 - accuracy: 0.3377 - val_loss: 2.2699 - val_accuracy: 0.3040\n",
      "Epoch 90/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1410 - accuracy: 0.3394 - val_loss: 2.2804 - val_accuracy: 0.3054\n",
      "Epoch 91/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1442 - accuracy: 0.3381 - val_loss: 2.2657 - val_accuracy: 0.3048\n",
      "Epoch 92/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1399 - accuracy: 0.3390 - val_loss: 2.2666 - val_accuracy: 0.3056\n",
      "Epoch 93/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1385 - accuracy: 0.3399 - val_loss: 2.2743 - val_accuracy: 0.3044\n",
      "Epoch 94/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1382 - accuracy: 0.3409 - val_loss: 2.2654 - val_accuracy: 0.3037\n",
      "Epoch 95/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1384 - accuracy: 0.3391 - val_loss: 2.2757 - val_accuracy: 0.3031\n",
      "Epoch 96/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1374 - accuracy: 0.3409 - val_loss: 2.2743 - val_accuracy: 0.3031\n",
      "Epoch 97/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1361 - accuracy: 0.3415 - val_loss: 2.2730 - val_accuracy: 0.3042\n",
      "Epoch 98/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1345 - accuracy: 0.3408 - val_loss: 2.2749 - val_accuracy: 0.3045\n",
      "Epoch 99/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1347 - accuracy: 0.3411 - val_loss: 2.2649 - val_accuracy: 0.3034\n",
      "Epoch 100/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1293 - accuracy: 0.3437 - val_loss: 2.2714 - val_accuracy: 0.3041\n",
      "Epoch 101/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1324 - accuracy: 0.3416 - val_loss: 2.2760 - val_accuracy: 0.3046\n",
      "Epoch 102/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1308 - accuracy: 0.3428 - val_loss: 2.2721 - val_accuracy: 0.3053\n",
      "Epoch 103/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1308 - accuracy: 0.3422 - val_loss: 2.2748 - val_accuracy: 0.3037\n",
      "Epoch 104/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1270 - accuracy: 0.3431 - val_loss: 2.2826 - val_accuracy: 0.3041\n",
      "Epoch 105/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1288 - accuracy: 0.3429 - val_loss: 2.2751 - val_accuracy: 0.3053\n",
      "Epoch 106/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1266 - accuracy: 0.3437 - val_loss: 2.2716 - val_accuracy: 0.3043\n",
      "Epoch 107/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1283 - accuracy: 0.3428 - val_loss: 2.2777 - val_accuracy: 0.3051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1283 - accuracy: 0.3447 - val_loss: 2.2818 - val_accuracy: 0.3028\n",
      "Epoch 109/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1260 - accuracy: 0.3436 - val_loss: 2.2712 - val_accuracy: 0.3040\n",
      "Epoch 110/110\n",
      "143633/143633 [==============================] - 1s 5us/sample - loss: 2.1247 - accuracy: 0.3447 - val_loss: 2.2755 - val_accuracy: 0.3022\n",
      "Restoring the best weights from epoch 110\n",
      "Training fold 2\n",
      "Train on 143641 samples, validate on 35914 samples\n",
      "Epoch 1/110\n",
      "143641/143641 [==============================] - 2s 16us/sample - loss: 2.9550 - accuracy: 0.1270 - val_loss: 2.9165 - val_accuracy: 0.1712\n",
      "Epoch 2/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.6450 - accuracy: 0.2021 - val_loss: 2.7201 - val_accuracy: 0.1973\n",
      "Epoch 3/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.5483 - accuracy: 0.2273 - val_loss: 2.5982 - val_accuracy: 0.2203\n",
      "Epoch 4/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.4974 - accuracy: 0.2427 - val_loss: 2.4881 - val_accuracy: 0.2460\n",
      "Epoch 5/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.4651 - accuracy: 0.2516 - val_loss: 2.4444 - val_accuracy: 0.2581\n",
      "Epoch 6/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.4408 - accuracy: 0.2608 - val_loss: 2.4153 - val_accuracy: 0.2653\n",
      "Epoch 7/110\n",
      "143641/143641 [==============================] - 1s 6us/sample - loss: 2.4169 - accuracy: 0.2667 - val_loss: 2.3969 - val_accuracy: 0.2716\n",
      "Epoch 8/110\n",
      "143641/143641 [==============================] - 1s 6us/sample - loss: 2.3995 - accuracy: 0.2705 - val_loss: 2.3878 - val_accuracy: 0.2707\n",
      "Epoch 9/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.3848 - accuracy: 0.2745 - val_loss: 2.3806 - val_accuracy: 0.2752\n",
      "Epoch 10/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.3715 - accuracy: 0.2769 - val_loss: 2.3601 - val_accuracy: 0.2808\n",
      "Epoch 11/110\n",
      "143641/143641 [==============================] - 1s 6us/sample - loss: 2.3592 - accuracy: 0.2813 - val_loss: 2.3561 - val_accuracy: 0.2822\n",
      "Epoch 12/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.3493 - accuracy: 0.2844 - val_loss: 2.3476 - val_accuracy: 0.2811\n",
      "Epoch 13/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.3406 - accuracy: 0.2855 - val_loss: 2.3487 - val_accuracy: 0.2824\n",
      "Epoch 14/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.3315 - accuracy: 0.2890 - val_loss: 2.3447 - val_accuracy: 0.2834\n",
      "Epoch 15/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.3258 - accuracy: 0.2902 - val_loss: 2.3417 - val_accuracy: 0.2826\n",
      "Epoch 16/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.3152 - accuracy: 0.2918 - val_loss: 2.3350 - val_accuracy: 0.2870\n",
      "Epoch 17/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.3095 - accuracy: 0.2933 - val_loss: 2.3307 - val_accuracy: 0.2870\n",
      "Epoch 18/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.3028 - accuracy: 0.2958 - val_loss: 2.3226 - val_accuracy: 0.2888\n",
      "Epoch 19/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2978 - accuracy: 0.2965 - val_loss: 2.3231 - val_accuracy: 0.2886\n",
      "Epoch 20/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2912 - accuracy: 0.2982 - val_loss: 2.3278 - val_accuracy: 0.2881\n",
      "Epoch 21/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2886 - accuracy: 0.2994 - val_loss: 2.3225 - val_accuracy: 0.2913\n",
      "Epoch 22/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2832 - accuracy: 0.3007 - val_loss: 2.3134 - val_accuracy: 0.2921\n",
      "Epoch 23/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2801 - accuracy: 0.3020 - val_loss: 2.3162 - val_accuracy: 0.2913\n",
      "Epoch 24/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2734 - accuracy: 0.3038 - val_loss: 2.3132 - val_accuracy: 0.2927\n",
      "Epoch 25/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2716 - accuracy: 0.3039 - val_loss: 2.3085 - val_accuracy: 0.2924\n",
      "Epoch 26/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2645 - accuracy: 0.3049 - val_loss: 2.3159 - val_accuracy: 0.2949\n",
      "Epoch 27/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2606 - accuracy: 0.3061 - val_loss: 2.3012 - val_accuracy: 0.2959\n",
      "Epoch 28/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2590 - accuracy: 0.3058 - val_loss: 2.3087 - val_accuracy: 0.2962\n",
      "Epoch 29/110\n",
      "143641/143641 [==============================] - 1s 6us/sample - loss: 2.2550 - accuracy: 0.3081 - val_loss: 2.3090 - val_accuracy: 0.2919\n",
      "Epoch 30/110\n",
      "143641/143641 [==============================] - 1s 6us/sample - loss: 2.2522 - accuracy: 0.3087 - val_loss: 2.2995 - val_accuracy: 0.2962\n",
      "Epoch 31/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2495 - accuracy: 0.3110 - val_loss: 2.3106 - val_accuracy: 0.2935\n",
      "Epoch 32/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2461 - accuracy: 0.3113 - val_loss: 2.3024 - val_accuracy: 0.2939\n",
      "Epoch 33/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2426 - accuracy: 0.3118 - val_loss: 2.3048 - val_accuracy: 0.2939\n",
      "Epoch 34/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2392 - accuracy: 0.3128 - val_loss: 2.3033 - val_accuracy: 0.2935\n",
      "Epoch 35/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2354 - accuracy: 0.3132 - val_loss: 2.2966 - val_accuracy: 0.2972\n",
      "Epoch 36/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2341 - accuracy: 0.3125 - val_loss: 2.2973 - val_accuracy: 0.2963\n",
      "Epoch 37/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2312 - accuracy: 0.3145 - val_loss: 2.3045 - val_accuracy: 0.2951\n",
      "Epoch 38/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2298 - accuracy: 0.3143 - val_loss: 2.2982 - val_accuracy: 0.2969\n",
      "Epoch 39/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2253 - accuracy: 0.3158 - val_loss: 2.2969 - val_accuracy: 0.2967\n",
      "Epoch 40/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2216 - accuracy: 0.3177 - val_loss: 2.2937 - val_accuracy: 0.2973\n",
      "Epoch 41/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2207 - accuracy: 0.3168 - val_loss: 2.2938 - val_accuracy: 0.2961\n",
      "Epoch 42/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2175 - accuracy: 0.3186 - val_loss: 2.2989 - val_accuracy: 0.2965\n",
      "Epoch 43/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2147 - accuracy: 0.3199 - val_loss: 2.2971 - val_accuracy: 0.2968\n",
      "Epoch 44/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2127 - accuracy: 0.3199 - val_loss: 2.2966 - val_accuracy: 0.2987\n",
      "Epoch 45/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2128 - accuracy: 0.3183 - val_loss: 2.2926 - val_accuracy: 0.2982\n",
      "Epoch 46/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2090 - accuracy: 0.3202 - val_loss: 2.2950 - val_accuracy: 0.2963\n",
      "Epoch 47/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2056 - accuracy: 0.3214 - val_loss: 2.2951 - val_accuracy: 0.2967\n",
      "Epoch 48/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2048 - accuracy: 0.3217 - val_loss: 2.2864 - val_accuracy: 0.2977\n",
      "Epoch 49/110\n",
      "143641/143641 [==============================] - 1s 6us/sample - loss: 2.2035 - accuracy: 0.3220 - val_loss: 2.2894 - val_accuracy: 0.2994\n",
      "Epoch 50/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.2026 - accuracy: 0.3209 - val_loss: 2.2917 - val_accuracy: 0.2965\n",
      "Epoch 51/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1971 - accuracy: 0.3235 - val_loss: 2.2909 - val_accuracy: 0.2986\n",
      "Epoch 52/110\n",
      "143641/143641 [==============================] - 1s 6us/sample - loss: 2.1969 - accuracy: 0.3236 - val_loss: 2.2916 - val_accuracy: 0.2979\n",
      "Epoch 53/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1938 - accuracy: 0.3233 - val_loss: 2.2939 - val_accuracy: 0.2951\n",
      "Epoch 54/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1926 - accuracy: 0.3240 - val_loss: 2.2912 - val_accuracy: 0.2999\n",
      "Epoch 55/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1928 - accuracy: 0.3243 - val_loss: 2.2939 - val_accuracy: 0.2994\n",
      "Epoch 56/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1904 - accuracy: 0.3242 - val_loss: 2.2881 - val_accuracy: 0.3002\n",
      "Epoch 57/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1893 - accuracy: 0.3246 - val_loss: 2.2936 - val_accuracy: 0.2990\n",
      "Epoch 58/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1878 - accuracy: 0.3257 - val_loss: 2.2871 - val_accuracy: 0.2985\n",
      "Epoch 59/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1847 - accuracy: 0.3273 - val_loss: 2.2957 - val_accuracy: 0.2975\n",
      "Epoch 60/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1829 - accuracy: 0.3272 - val_loss: 2.2883 - val_accuracy: 0.3000\n",
      "Epoch 61/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1790 - accuracy: 0.3284 - val_loss: 2.2887 - val_accuracy: 0.2998\n",
      "Epoch 62/110\n",
      "143641/143641 [==============================] - 1s 6us/sample - loss: 2.1769 - accuracy: 0.3293 - val_loss: 2.2937 - val_accuracy: 0.2983\n",
      "Epoch 63/110\n",
      "143641/143641 [==============================] - 1s 6us/sample - loss: 2.1791 - accuracy: 0.3284 - val_loss: 2.2898 - val_accuracy: 0.2999\n",
      "Epoch 64/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1760 - accuracy: 0.3307 - val_loss: 2.2947 - val_accuracy: 0.2973\n",
      "Epoch 65/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1739 - accuracy: 0.3304 - val_loss: 2.2879 - val_accuracy: 0.2989\n",
      "Epoch 66/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1710 - accuracy: 0.3300 - val_loss: 2.2899 - val_accuracy: 0.3000\n",
      "Epoch 67/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1698 - accuracy: 0.3310 - val_loss: 2.2953 - val_accuracy: 0.3013\n",
      "Epoch 68/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1693 - accuracy: 0.3310 - val_loss: 2.2867 - val_accuracy: 0.3000\n",
      "Epoch 69/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1701 - accuracy: 0.3312 - val_loss: 2.2943 - val_accuracy: 0.3006\n",
      "Epoch 70/110\n",
      "143641/143641 [==============================] - 1s 6us/sample - loss: 2.1668 - accuracy: 0.3309 - val_loss: 2.2875 - val_accuracy: 0.3016\n",
      "Epoch 71/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1631 - accuracy: 0.3340 - val_loss: 2.2937 - val_accuracy: 0.2978\n",
      "Epoch 72/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1640 - accuracy: 0.3326 - val_loss: 2.2878 - val_accuracy: 0.2997\n",
      "Epoch 73/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1644 - accuracy: 0.3335 - val_loss: 2.3007 - val_accuracy: 0.3007\n",
      "Epoch 74/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1634 - accuracy: 0.3335 - val_loss: 2.2883 - val_accuracy: 0.2999\n",
      "Epoch 75/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1589 - accuracy: 0.3340 - val_loss: 2.2886 - val_accuracy: 0.2993\n",
      "Epoch 76/110\n",
      "143641/143641 [==============================] - 1s 6us/sample - loss: 2.1585 - accuracy: 0.3349 - val_loss: 2.2821 - val_accuracy: 0.3018\n",
      "Epoch 77/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1578 - accuracy: 0.3349 - val_loss: 2.2950 - val_accuracy: 0.2984\n",
      "Epoch 78/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1560 - accuracy: 0.3351 - val_loss: 2.2896 - val_accuracy: 0.3008\n",
      "Epoch 79/110\n",
      "143641/143641 [==============================] - 1s 6us/sample - loss: 2.1566 - accuracy: 0.3344 - val_loss: 2.2872 - val_accuracy: 0.3029\n",
      "Epoch 80/110\n",
      "143641/143641 [==============================] - 1s 6us/sample - loss: 2.1535 - accuracy: 0.3357 - val_loss: 2.2939 - val_accuracy: 0.2985\n",
      "Epoch 81/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1524 - accuracy: 0.3363 - val_loss: 2.2890 - val_accuracy: 0.2997\n",
      "Epoch 82/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1520 - accuracy: 0.3356 - val_loss: 2.2906 - val_accuracy: 0.3014\n",
      "Epoch 83/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1493 - accuracy: 0.3360 - val_loss: 2.2926 - val_accuracy: 0.3033\n",
      "Epoch 84/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1498 - accuracy: 0.3361 - val_loss: 2.2857 - val_accuracy: 0.3002\n",
      "Epoch 85/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1488 - accuracy: 0.3373 - val_loss: 2.2863 - val_accuracy: 0.3016\n",
      "Epoch 86/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1457 - accuracy: 0.3383 - val_loss: 2.2898 - val_accuracy: 0.2988\n",
      "Epoch 87/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1461 - accuracy: 0.3377 - val_loss: 2.2894 - val_accuracy: 0.3016\n",
      "Epoch 88/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1452 - accuracy: 0.3380 - val_loss: 2.2893 - val_accuracy: 0.3016\n",
      "Epoch 89/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1419 - accuracy: 0.3399 - val_loss: 2.2962 - val_accuracy: 0.3000\n",
      "Epoch 90/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1439 - accuracy: 0.3378 - val_loss: 2.2933 - val_accuracy: 0.3009\n",
      "Epoch 91/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1439 - accuracy: 0.3387 - val_loss: 2.2903 - val_accuracy: 0.2996\n",
      "Epoch 92/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1388 - accuracy: 0.3394 - val_loss: 2.2966 - val_accuracy: 0.3002\n",
      "Epoch 93/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1406 - accuracy: 0.3383 - val_loss: 2.2943 - val_accuracy: 0.2998\n",
      "Epoch 94/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1390 - accuracy: 0.3391 - val_loss: 2.2942 - val_accuracy: 0.3013\n",
      "Epoch 95/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1383 - accuracy: 0.3397 - val_loss: 2.2929 - val_accuracy: 0.3009\n",
      "Epoch 96/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1363 - accuracy: 0.3398 - val_loss: 2.2865 - val_accuracy: 0.3006\n",
      "Epoch 97/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1352 - accuracy: 0.3404 - val_loss: 2.2914 - val_accuracy: 0.3015\n",
      "Epoch 98/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1325 - accuracy: 0.3412 - val_loss: 2.2867 - val_accuracy: 0.3017\n",
      "Epoch 99/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1328 - accuracy: 0.3405 - val_loss: 2.2911 - val_accuracy: 0.3001\n",
      "Epoch 100/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1313 - accuracy: 0.3418 - val_loss: 2.2918 - val_accuracy: 0.3029\n",
      "Epoch 101/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1320 - accuracy: 0.3429 - val_loss: 2.2863 - val_accuracy: 0.3020\n",
      "Epoch 102/110\n",
      "143641/143641 [==============================] - 1s 6us/sample - loss: 2.1320 - accuracy: 0.3408 - val_loss: 2.2944 - val_accuracy: 0.2989\n",
      "Epoch 103/110\n",
      "143641/143641 [==============================] - 1s 6us/sample - loss: 2.1297 - accuracy: 0.3415 - val_loss: 2.2904 - val_accuracy: 0.2988\n",
      "Epoch 104/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1303 - accuracy: 0.3415 - val_loss: 2.2891 - val_accuracy: 0.3022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1274 - accuracy: 0.3421 - val_loss: 2.2942 - val_accuracy: 0.3006\n",
      "Epoch 106/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1283 - accuracy: 0.3432 - val_loss: 2.2923 - val_accuracy: 0.2991\n",
      "Epoch 107/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1285 - accuracy: 0.3418 - val_loss: 2.2958 - val_accuracy: 0.2997\n",
      "Epoch 108/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1274 - accuracy: 0.3439 - val_loss: 2.2906 - val_accuracy: 0.3019\n",
      "Epoch 109/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1254 - accuracy: 0.3436 - val_loss: 2.2949 - val_accuracy: 0.3044\n",
      "Epoch 110/110\n",
      "143641/143641 [==============================] - 1s 5us/sample - loss: 2.1264 - accuracy: 0.3435 - val_loss: 2.2895 - val_accuracy: 0.3014\n",
      "Restoring the best weights from epoch 110\n",
      "Training fold 3\n",
      "Train on 143644 samples, validate on 35911 samples\n",
      "Epoch 1/110\n",
      "143644/143644 [==============================] - 3s 18us/sample - loss: 2.9894 - accuracy: 0.1180 - val_loss: 2.9684 - val_accuracy: 0.1666\n",
      "Epoch 2/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.6706 - accuracy: 0.1969 - val_loss: 2.6741 - val_accuracy: 0.2066\n",
      "Epoch 3/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.5653 - accuracy: 0.2194 - val_loss: 2.5613 - val_accuracy: 0.2204\n",
      "Epoch 4/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.5103 - accuracy: 0.2352 - val_loss: 2.4926 - val_accuracy: 0.2426\n",
      "Epoch 5/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.4721 - accuracy: 0.2471 - val_loss: 2.4337 - val_accuracy: 0.2583\n",
      "Epoch 6/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.4460 - accuracy: 0.2547 - val_loss: 2.4190 - val_accuracy: 0.2630\n",
      "Epoch 7/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.4255 - accuracy: 0.2617 - val_loss: 2.3963 - val_accuracy: 0.2695\n",
      "Epoch 8/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.4080 - accuracy: 0.2654 - val_loss: 2.3768 - val_accuracy: 0.2755\n",
      "Epoch 9/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.3922 - accuracy: 0.2728 - val_loss: 2.3629 - val_accuracy: 0.2787\n",
      "Epoch 10/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.3753 - accuracy: 0.2760 - val_loss: 2.3580 - val_accuracy: 0.2812\n",
      "Epoch 11/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.3654 - accuracy: 0.2792 - val_loss: 2.3519 - val_accuracy: 0.2813\n",
      "Epoch 12/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.3569 - accuracy: 0.2815 - val_loss: 2.3482 - val_accuracy: 0.2824\n",
      "Epoch 13/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.3478 - accuracy: 0.2836 - val_loss: 2.3367 - val_accuracy: 0.2860\n",
      "Epoch 14/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.3375 - accuracy: 0.2864 - val_loss: 2.3323 - val_accuracy: 0.2886\n",
      "Epoch 15/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.3287 - accuracy: 0.2892 - val_loss: 2.3286 - val_accuracy: 0.2898\n",
      "Epoch 16/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.3206 - accuracy: 0.2908 - val_loss: 2.3203 - val_accuracy: 0.2923\n",
      "Epoch 17/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.3158 - accuracy: 0.2933 - val_loss: 2.3188 - val_accuracy: 0.2916\n",
      "Epoch 18/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.3082 - accuracy: 0.2948 - val_loss: 2.3186 - val_accuracy: 0.2923\n",
      "Epoch 19/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.3023 - accuracy: 0.2957 - val_loss: 2.3085 - val_accuracy: 0.2944\n",
      "Epoch 20/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2960 - accuracy: 0.2981 - val_loss: 2.3028 - val_accuracy: 0.2972\n",
      "Epoch 21/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2922 - accuracy: 0.2986 - val_loss: 2.3039 - val_accuracy: 0.2945\n",
      "Epoch 22/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2882 - accuracy: 0.2990 - val_loss: 2.3164 - val_accuracy: 0.2916\n",
      "Epoch 23/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2805 - accuracy: 0.3026 - val_loss: 2.2947 - val_accuracy: 0.3004\n",
      "Epoch 24/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2765 - accuracy: 0.3035 - val_loss: 2.2921 - val_accuracy: 0.3001\n",
      "Epoch 25/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2732 - accuracy: 0.3041 - val_loss: 2.2956 - val_accuracy: 0.2980\n",
      "Epoch 26/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2668 - accuracy: 0.3052 - val_loss: 2.2935 - val_accuracy: 0.3008\n",
      "Epoch 27/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2628 - accuracy: 0.3073 - val_loss: 2.2898 - val_accuracy: 0.2992\n",
      "Epoch 28/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2588 - accuracy: 0.3077 - val_loss: 2.2850 - val_accuracy: 0.3021\n",
      "Epoch 29/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2529 - accuracy: 0.3096 - val_loss: 2.2917 - val_accuracy: 0.3005\n",
      "Epoch 30/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2524 - accuracy: 0.3093 - val_loss: 2.2839 - val_accuracy: 0.3013\n",
      "Epoch 31/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2479 - accuracy: 0.3101 - val_loss: 2.2794 - val_accuracy: 0.3027\n",
      "Epoch 32/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2448 - accuracy: 0.3119 - val_loss: 2.2822 - val_accuracy: 0.3015\n",
      "Epoch 33/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2430 - accuracy: 0.3128 - val_loss: 2.2879 - val_accuracy: 0.3004\n",
      "Epoch 34/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2388 - accuracy: 0.3120 - val_loss: 2.2868 - val_accuracy: 0.3010\n",
      "Epoch 35/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2350 - accuracy: 0.3150 - val_loss: 2.2806 - val_accuracy: 0.3035\n",
      "Epoch 36/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2336 - accuracy: 0.3148 - val_loss: 2.2759 - val_accuracy: 0.3028\n",
      "Epoch 37/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2319 - accuracy: 0.3154 - val_loss: 2.2791 - val_accuracy: 0.3055\n",
      "Epoch 38/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2262 - accuracy: 0.3158 - val_loss: 2.2738 - val_accuracy: 0.3021\n",
      "Epoch 39/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2246 - accuracy: 0.3160 - val_loss: 2.2722 - val_accuracy: 0.3043\n",
      "Epoch 40/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2207 - accuracy: 0.3172 - val_loss: 2.2770 - val_accuracy: 0.3030\n",
      "Epoch 41/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2196 - accuracy: 0.3177 - val_loss: 2.2736 - val_accuracy: 0.3041\n",
      "Epoch 42/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2182 - accuracy: 0.3173 - val_loss: 2.2784 - val_accuracy: 0.3023\n",
      "Epoch 43/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2132 - accuracy: 0.3203 - val_loss: 2.2789 - val_accuracy: 0.3017\n",
      "Epoch 44/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2129 - accuracy: 0.3192 - val_loss: 2.2751 - val_accuracy: 0.3054\n",
      "Epoch 45/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2094 - accuracy: 0.3208 - val_loss: 2.2733 - val_accuracy: 0.3050\n",
      "Epoch 46/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2084 - accuracy: 0.3204 - val_loss: 2.2792 - val_accuracy: 0.3038\n",
      "Epoch 47/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2039 - accuracy: 0.3216 - val_loss: 2.2873 - val_accuracy: 0.3009\n",
      "Epoch 48/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2044 - accuracy: 0.3228 - val_loss: 2.2736 - val_accuracy: 0.3060\n",
      "Epoch 49/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2014 - accuracy: 0.3225 - val_loss: 2.2731 - val_accuracy: 0.3071\n",
      "Epoch 50/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.2002 - accuracy: 0.3230 - val_loss: 2.2756 - val_accuracy: 0.3045\n",
      "Epoch 51/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1971 - accuracy: 0.3238 - val_loss: 2.2832 - val_accuracy: 0.3021\n",
      "Epoch 52/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1956 - accuracy: 0.3244 - val_loss: 2.2716 - val_accuracy: 0.3047\n",
      "Epoch 53/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1925 - accuracy: 0.3255 - val_loss: 2.2753 - val_accuracy: 0.3053\n",
      "Epoch 54/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1911 - accuracy: 0.3264 - val_loss: 2.2780 - val_accuracy: 0.3030\n",
      "Epoch 55/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1898 - accuracy: 0.3241 - val_loss: 2.2738 - val_accuracy: 0.3050\n",
      "Epoch 56/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1885 - accuracy: 0.3259 - val_loss: 2.2760 - val_accuracy: 0.3047\n",
      "Epoch 57/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1857 - accuracy: 0.3265 - val_loss: 2.2699 - val_accuracy: 0.3061\n",
      "Epoch 58/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1846 - accuracy: 0.3273 - val_loss: 2.2818 - val_accuracy: 0.3038\n",
      "Epoch 59/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1830 - accuracy: 0.3279 - val_loss: 2.2739 - val_accuracy: 0.3085\n",
      "Epoch 60/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1821 - accuracy: 0.3283 - val_loss: 2.2740 - val_accuracy: 0.3053\n",
      "Epoch 61/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1793 - accuracy: 0.3279 - val_loss: 2.2697 - val_accuracy: 0.3058\n",
      "Epoch 62/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1786 - accuracy: 0.3268 - val_loss: 2.2732 - val_accuracy: 0.3043\n",
      "Epoch 63/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1774 - accuracy: 0.3285 - val_loss: 2.2699 - val_accuracy: 0.3065\n",
      "Epoch 64/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1749 - accuracy: 0.3293 - val_loss: 2.2801 - val_accuracy: 0.3073\n",
      "Epoch 65/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1754 - accuracy: 0.3286 - val_loss: 2.2753 - val_accuracy: 0.3078\n",
      "Epoch 66/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1711 - accuracy: 0.3305 - val_loss: 2.2775 - val_accuracy: 0.3070\n",
      "Epoch 67/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1717 - accuracy: 0.3289 - val_loss: 2.2699 - val_accuracy: 0.3071\n",
      "Epoch 68/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1688 - accuracy: 0.3319 - val_loss: 2.2708 - val_accuracy: 0.3072\n",
      "Epoch 69/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1678 - accuracy: 0.3328 - val_loss: 2.2678 - val_accuracy: 0.3070\n",
      "Epoch 70/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1684 - accuracy: 0.3304 - val_loss: 2.2751 - val_accuracy: 0.3043\n",
      "Epoch 71/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1644 - accuracy: 0.3330 - val_loss: 2.2758 - val_accuracy: 0.3061\n",
      "Epoch 72/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1659 - accuracy: 0.3328 - val_loss: 2.2721 - val_accuracy: 0.3104\n",
      "Epoch 73/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1638 - accuracy: 0.3332 - val_loss: 2.2836 - val_accuracy: 0.3041\n",
      "Epoch 74/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1604 - accuracy: 0.3337 - val_loss: 2.2780 - val_accuracy: 0.3060\n",
      "Epoch 75/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1614 - accuracy: 0.3332 - val_loss: 2.2687 - val_accuracy: 0.3093\n",
      "Epoch 76/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1568 - accuracy: 0.3339 - val_loss: 2.2803 - val_accuracy: 0.3073\n",
      "Epoch 77/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1587 - accuracy: 0.3335 - val_loss: 2.2791 - val_accuracy: 0.3062\n",
      "Epoch 78/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1568 - accuracy: 0.3341 - val_loss: 2.2752 - val_accuracy: 0.3052\n",
      "Epoch 79/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1560 - accuracy: 0.3352 - val_loss: 2.2753 - val_accuracy: 0.3066\n",
      "Epoch 80/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1549 - accuracy: 0.3358 - val_loss: 2.2721 - val_accuracy: 0.3080\n",
      "Epoch 81/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1522 - accuracy: 0.3359 - val_loss: 2.2763 - val_accuracy: 0.3074\n",
      "Epoch 82/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1532 - accuracy: 0.3346 - val_loss: 2.2756 - val_accuracy: 0.3079\n",
      "Epoch 83/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1521 - accuracy: 0.3355 - val_loss: 2.2715 - val_accuracy: 0.3085\n",
      "Epoch 84/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1501 - accuracy: 0.3352 - val_loss: 2.2752 - val_accuracy: 0.3060\n",
      "Epoch 85/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1497 - accuracy: 0.3355 - val_loss: 2.2751 - val_accuracy: 0.3085\n",
      "Epoch 86/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1479 - accuracy: 0.3366 - val_loss: 2.2741 - val_accuracy: 0.3061\n",
      "Epoch 87/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1438 - accuracy: 0.3381 - val_loss: 2.2745 - val_accuracy: 0.3082\n",
      "Epoch 88/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1457 - accuracy: 0.3371 - val_loss: 2.2805 - val_accuracy: 0.3047\n",
      "Epoch 89/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1445 - accuracy: 0.3392 - val_loss: 2.2833 - val_accuracy: 0.3046\n",
      "Epoch 90/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1427 - accuracy: 0.3394 - val_loss: 2.2766 - val_accuracy: 0.3092\n",
      "Epoch 91/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1436 - accuracy: 0.3378 - val_loss: 2.2746 - val_accuracy: 0.3069\n",
      "Epoch 92/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1430 - accuracy: 0.3376 - val_loss: 2.2761 - val_accuracy: 0.3092\n",
      "Epoch 93/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1386 - accuracy: 0.3383 - val_loss: 2.2768 - val_accuracy: 0.3079\n",
      "Epoch 94/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1405 - accuracy: 0.3374 - val_loss: 2.2755 - val_accuracy: 0.3068\n",
      "Epoch 95/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1412 - accuracy: 0.3393 - val_loss: 2.2785 - val_accuracy: 0.3075\n",
      "Epoch 96/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1368 - accuracy: 0.3404 - val_loss: 2.2773 - val_accuracy: 0.3087\n",
      "Epoch 97/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1373 - accuracy: 0.3393 - val_loss: 2.2845 - val_accuracy: 0.3057\n",
      "Epoch 98/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1372 - accuracy: 0.3395 - val_loss: 2.2841 - val_accuracy: 0.3061\n",
      "Epoch 99/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1352 - accuracy: 0.3404 - val_loss: 2.2741 - val_accuracy: 0.3095\n",
      "Epoch 100/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1331 - accuracy: 0.3409 - val_loss: 2.2803 - val_accuracy: 0.3081\n",
      "Epoch 101/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1334 - accuracy: 0.3403 - val_loss: 2.2765 - val_accuracy: 0.3060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1305 - accuracy: 0.3410 - val_loss: 2.2786 - val_accuracy: 0.3086\n",
      "Epoch 103/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1300 - accuracy: 0.3417 - val_loss: 2.2835 - val_accuracy: 0.3065\n",
      "Epoch 104/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1302 - accuracy: 0.3413 - val_loss: 2.2789 - val_accuracy: 0.3065\n",
      "Epoch 105/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1273 - accuracy: 0.3421 - val_loss: 2.2806 - val_accuracy: 0.3079\n",
      "Epoch 106/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1302 - accuracy: 0.3414 - val_loss: 2.2807 - val_accuracy: 0.3093\n",
      "Epoch 107/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1285 - accuracy: 0.3418 - val_loss: 2.2885 - val_accuracy: 0.3097\n",
      "Epoch 108/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1291 - accuracy: 0.3418 - val_loss: 2.2763 - val_accuracy: 0.3094\n",
      "Epoch 109/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1243 - accuracy: 0.3429 - val_loss: 2.2842 - val_accuracy: 0.3077\n",
      "Epoch 110/110\n",
      "143644/143644 [==============================] - 1s 5us/sample - loss: 2.1238 - accuracy: 0.3433 - val_loss: 2.2820 - val_accuracy: 0.3087\n",
      "Restoring the best weights from epoch 110\n",
      "Training fold 4\n",
      "Train on 143650 samples, validate on 35905 samples\n",
      "Epoch 1/110\n",
      "143650/143650 [==============================] - 3s 19us/sample - loss: 3.0134 - accuracy: 0.1108 - val_loss: 2.9751 - val_accuracy: 0.1578\n",
      "Epoch 2/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.7113 - accuracy: 0.1829 - val_loss: 2.7150 - val_accuracy: 0.2042\n",
      "Epoch 3/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.5704 - accuracy: 0.2211 - val_loss: 2.5649 - val_accuracy: 0.2289\n",
      "Epoch 4/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.5132 - accuracy: 0.2362 - val_loss: 2.5103 - val_accuracy: 0.2401\n",
      "Epoch 5/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.4780 - accuracy: 0.2477 - val_loss: 2.4521 - val_accuracy: 0.2544\n",
      "Epoch 6/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.4529 - accuracy: 0.2547 - val_loss: 2.4376 - val_accuracy: 0.2572\n",
      "Epoch 7/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.4295 - accuracy: 0.2607 - val_loss: 2.4060 - val_accuracy: 0.2682\n",
      "Epoch 8/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.4125 - accuracy: 0.2667 - val_loss: 2.3938 - val_accuracy: 0.2721\n",
      "Epoch 9/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.3954 - accuracy: 0.2718 - val_loss: 2.3782 - val_accuracy: 0.2764\n",
      "Epoch 10/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.3824 - accuracy: 0.2745 - val_loss: 2.3687 - val_accuracy: 0.2778\n",
      "Epoch 11/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.3683 - accuracy: 0.2780 - val_loss: 2.3712 - val_accuracy: 0.2771\n",
      "Epoch 12/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.3559 - accuracy: 0.2816 - val_loss: 2.3505 - val_accuracy: 0.2822\n",
      "Epoch 13/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.3474 - accuracy: 0.2835 - val_loss: 2.3447 - val_accuracy: 0.2830\n",
      "Epoch 14/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.3365 - accuracy: 0.2873 - val_loss: 2.3495 - val_accuracy: 0.2837\n",
      "Epoch 15/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.3324 - accuracy: 0.2878 - val_loss: 2.3364 - val_accuracy: 0.2877\n",
      "Epoch 16/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.3209 - accuracy: 0.2909 - val_loss: 2.3350 - val_accuracy: 0.2855\n",
      "Epoch 17/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.3140 - accuracy: 0.2917 - val_loss: 2.3327 - val_accuracy: 0.2875\n",
      "Epoch 18/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.3094 - accuracy: 0.2938 - val_loss: 2.3293 - val_accuracy: 0.2879\n",
      "Epoch 19/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.2999 - accuracy: 0.2954 - val_loss: 2.3201 - val_accuracy: 0.2893\n",
      "Epoch 20/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.2971 - accuracy: 0.2967 - val_loss: 2.3255 - val_accuracy: 0.2898\n",
      "Epoch 21/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.2909 - accuracy: 0.2982 - val_loss: 2.3237 - val_accuracy: 0.2900\n",
      "Epoch 22/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.2842 - accuracy: 0.2985 - val_loss: 2.3160 - val_accuracy: 0.2902\n",
      "Epoch 23/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.2815 - accuracy: 0.3003 - val_loss: 2.3123 - val_accuracy: 0.2907\n",
      "Epoch 24/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.2749 - accuracy: 0.3014 - val_loss: 2.3117 - val_accuracy: 0.2931\n",
      "Epoch 25/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.2720 - accuracy: 0.3033 - val_loss: 2.3143 - val_accuracy: 0.2920\n",
      "Epoch 26/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.2686 - accuracy: 0.3032 - val_loss: 2.3117 - val_accuracy: 0.2921\n",
      "Epoch 27/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.2625 - accuracy: 0.3050 - val_loss: 2.3023 - val_accuracy: 0.2936\n",
      "Epoch 28/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.2615 - accuracy: 0.3056 - val_loss: 2.3075 - val_accuracy: 0.2945\n",
      "Epoch 29/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.2552 - accuracy: 0.3071 - val_loss: 2.3003 - val_accuracy: 0.2936\n",
      "Epoch 30/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.2484 - accuracy: 0.3084 - val_loss: 2.3080 - val_accuracy: 0.2925\n",
      "Epoch 31/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.2510 - accuracy: 0.3081 - val_loss: 2.2986 - val_accuracy: 0.2955\n",
      "Epoch 32/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.2468 - accuracy: 0.3108 - val_loss: 2.2971 - val_accuracy: 0.2979\n",
      "Epoch 33/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.2413 - accuracy: 0.3110 - val_loss: 2.3012 - val_accuracy: 0.2973\n",
      "Epoch 34/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.2381 - accuracy: 0.3124 - val_loss: 2.3008 - val_accuracy: 0.2980\n",
      "Epoch 35/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.2384 - accuracy: 0.3122 - val_loss: 2.2994 - val_accuracy: 0.2949\n",
      "Epoch 36/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.2335 - accuracy: 0.3121 - val_loss: 2.2913 - val_accuracy: 0.3000\n",
      "Epoch 37/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.2320 - accuracy: 0.3138 - val_loss: 2.2898 - val_accuracy: 0.3008\n",
      "Epoch 38/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.2276 - accuracy: 0.3140 - val_loss: 2.3049 - val_accuracy: 0.2934\n",
      "Epoch 39/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.2266 - accuracy: 0.3130 - val_loss: 2.2927 - val_accuracy: 0.2985\n",
      "Epoch 40/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.2214 - accuracy: 0.3165 - val_loss: 2.2874 - val_accuracy: 0.2994\n",
      "Epoch 41/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.2200 - accuracy: 0.3164 - val_loss: 2.2976 - val_accuracy: 0.2996\n",
      "Epoch 42/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.2184 - accuracy: 0.3170 - val_loss: 2.2973 - val_accuracy: 0.2991\n",
      "Epoch 43/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.2172 - accuracy: 0.3177 - val_loss: 2.2907 - val_accuracy: 0.2993\n",
      "Epoch 44/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.2109 - accuracy: 0.3192 - val_loss: 2.2965 - val_accuracy: 0.2981\n",
      "Epoch 45/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.2122 - accuracy: 0.3176 - val_loss: 2.2941 - val_accuracy: 0.2973\n",
      "Epoch 46/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.2060 - accuracy: 0.3199 - val_loss: 2.2905 - val_accuracy: 0.3002\n",
      "Epoch 47/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.2073 - accuracy: 0.3199 - val_loss: 2.2930 - val_accuracy: 0.2972\n",
      "Epoch 48/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.2022 - accuracy: 0.3216 - val_loss: 2.2927 - val_accuracy: 0.3000\n",
      "Epoch 49/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.2015 - accuracy: 0.3207 - val_loss: 2.2966 - val_accuracy: 0.3010\n",
      "Epoch 50/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.2006 - accuracy: 0.3221 - val_loss: 2.2891 - val_accuracy: 0.2999\n",
      "Epoch 51/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1983 - accuracy: 0.3210 - val_loss: 2.3017 - val_accuracy: 0.2996\n",
      "Epoch 52/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1971 - accuracy: 0.3237 - val_loss: 2.2902 - val_accuracy: 0.3002\n",
      "Epoch 53/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1939 - accuracy: 0.3234 - val_loss: 2.2934 - val_accuracy: 0.3003\n",
      "Epoch 54/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1904 - accuracy: 0.3238 - val_loss: 2.2871 - val_accuracy: 0.3020\n",
      "Epoch 55/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1906 - accuracy: 0.3236 - val_loss: 2.2903 - val_accuracy: 0.2990\n",
      "Epoch 56/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1866 - accuracy: 0.3242 - val_loss: 2.2907 - val_accuracy: 0.3023\n",
      "Epoch 57/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1885 - accuracy: 0.3243 - val_loss: 2.2853 - val_accuracy: 0.3002\n",
      "Epoch 58/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1850 - accuracy: 0.3254 - val_loss: 2.2886 - val_accuracy: 0.3000\n",
      "Epoch 59/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1839 - accuracy: 0.3273 - val_loss: 2.2874 - val_accuracy: 0.2989\n",
      "Epoch 60/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1829 - accuracy: 0.3268 - val_loss: 2.2851 - val_accuracy: 0.3032\n",
      "Epoch 61/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1789 - accuracy: 0.3286 - val_loss: 2.2879 - val_accuracy: 0.3033\n",
      "Epoch 62/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1776 - accuracy: 0.3282 - val_loss: 2.2959 - val_accuracy: 0.3000\n",
      "Epoch 63/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1788 - accuracy: 0.3269 - val_loss: 2.2895 - val_accuracy: 0.3016\n",
      "Epoch 64/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1786 - accuracy: 0.3280 - val_loss: 2.2914 - val_accuracy: 0.3022\n",
      "Epoch 65/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1744 - accuracy: 0.3288 - val_loss: 2.2865 - val_accuracy: 0.2996\n",
      "Epoch 66/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1718 - accuracy: 0.3295 - val_loss: 2.2877 - val_accuracy: 0.2995\n",
      "Epoch 67/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1705 - accuracy: 0.3293 - val_loss: 2.2864 - val_accuracy: 0.3022\n",
      "Epoch 68/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1683 - accuracy: 0.3292 - val_loss: 2.2925 - val_accuracy: 0.2992\n",
      "Epoch 69/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1681 - accuracy: 0.3315 - val_loss: 2.2902 - val_accuracy: 0.3019\n",
      "Epoch 70/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1666 - accuracy: 0.3301 - val_loss: 2.2859 - val_accuracy: 0.3040\n",
      "Epoch 71/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1652 - accuracy: 0.3318 - val_loss: 2.2886 - val_accuracy: 0.3015\n",
      "Epoch 72/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1616 - accuracy: 0.3316 - val_loss: 2.2937 - val_accuracy: 0.3015\n",
      "Epoch 73/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1649 - accuracy: 0.3306 - val_loss: 2.2892 - val_accuracy: 0.3008\n",
      "Epoch 74/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1613 - accuracy: 0.3309 - val_loss: 2.2876 - val_accuracy: 0.3051\n",
      "Epoch 75/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1623 - accuracy: 0.3324 - val_loss: 2.2844 - val_accuracy: 0.3036\n",
      "Epoch 76/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1604 - accuracy: 0.3342 - val_loss: 2.2926 - val_accuracy: 0.3019\n",
      "Epoch 77/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1567 - accuracy: 0.3321 - val_loss: 2.3003 - val_accuracy: 0.2990\n",
      "Epoch 78/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1573 - accuracy: 0.3334 - val_loss: 2.2899 - val_accuracy: 0.3027\n",
      "Epoch 79/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1563 - accuracy: 0.3338 - val_loss: 2.2903 - val_accuracy: 0.3032\n",
      "Epoch 80/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1561 - accuracy: 0.3342 - val_loss: 2.2889 - val_accuracy: 0.3010\n",
      "Epoch 81/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1524 - accuracy: 0.3345 - val_loss: 2.2865 - val_accuracy: 0.3035\n",
      "Epoch 82/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1536 - accuracy: 0.3348 - val_loss: 2.2893 - val_accuracy: 0.3020\n",
      "Epoch 83/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1516 - accuracy: 0.3349 - val_loss: 2.2899 - val_accuracy: 0.3036\n",
      "Epoch 84/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1482 - accuracy: 0.3359 - val_loss: 2.2823 - val_accuracy: 0.3037\n",
      "Epoch 85/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1509 - accuracy: 0.3352 - val_loss: 2.2926 - val_accuracy: 0.3014\n",
      "Epoch 86/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1476 - accuracy: 0.3359 - val_loss: 2.2925 - val_accuracy: 0.3031\n",
      "Epoch 87/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1470 - accuracy: 0.3357 - val_loss: 2.2948 - val_accuracy: 0.3018\n",
      "Epoch 88/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1465 - accuracy: 0.3362 - val_loss: 2.2974 - val_accuracy: 0.3000\n",
      "Epoch 89/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1453 - accuracy: 0.3362 - val_loss: 2.2946 - val_accuracy: 0.3020\n",
      "Epoch 90/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1426 - accuracy: 0.3374 - val_loss: 2.2936 - val_accuracy: 0.3022\n",
      "Epoch 91/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1419 - accuracy: 0.3369 - val_loss: 2.2944 - val_accuracy: 0.3013\n",
      "Epoch 92/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1428 - accuracy: 0.3359 - val_loss: 2.2908 - val_accuracy: 0.3035\n",
      "Epoch 93/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1407 - accuracy: 0.3377 - val_loss: 2.2924 - val_accuracy: 0.2999\n",
      "Epoch 94/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1384 - accuracy: 0.3379 - val_loss: 2.2914 - val_accuracy: 0.3028\n",
      "Epoch 95/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1393 - accuracy: 0.3387 - val_loss: 2.2930 - val_accuracy: 0.3027\n",
      "Epoch 96/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1389 - accuracy: 0.3389 - val_loss: 2.2969 - val_accuracy: 0.3037\n",
      "Epoch 97/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1366 - accuracy: 0.3400 - val_loss: 2.2909 - val_accuracy: 0.3021\n",
      "Epoch 98/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1351 - accuracy: 0.3401 - val_loss: 2.2960 - val_accuracy: 0.2979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1358 - accuracy: 0.3379 - val_loss: 2.2908 - val_accuracy: 0.3019\n",
      "Epoch 100/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1330 - accuracy: 0.3399 - val_loss: 2.2922 - val_accuracy: 0.3036\n",
      "Epoch 101/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1337 - accuracy: 0.3399 - val_loss: 2.3003 - val_accuracy: 0.3011\n",
      "Epoch 102/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1321 - accuracy: 0.3405 - val_loss: 2.2885 - val_accuracy: 0.3007\n",
      "Epoch 103/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1316 - accuracy: 0.3418 - val_loss: 2.2957 - val_accuracy: 0.3017\n",
      "Epoch 104/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1314 - accuracy: 0.3391 - val_loss: 2.3010 - val_accuracy: 0.3035\n",
      "Epoch 105/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1292 - accuracy: 0.3397 - val_loss: 2.2945 - val_accuracy: 0.3029\n",
      "Epoch 106/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1292 - accuracy: 0.3413 - val_loss: 2.2957 - val_accuracy: 0.3029\n",
      "Epoch 107/110\n",
      "143650/143650 [==============================] - 1s 5us/sample - loss: 2.1270 - accuracy: 0.3420 - val_loss: 2.3013 - val_accuracy: 0.2985\n",
      "Epoch 108/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1276 - accuracy: 0.3416 - val_loss: 2.2927 - val_accuracy: 0.3026\n",
      "Epoch 109/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1265 - accuracy: 0.3423 - val_loss: 2.2965 - val_accuracy: 0.3002\n",
      "Epoch 110/110\n",
      "143650/143650 [==============================] - 1s 6us/sample - loss: 2.1255 - accuracy: 0.3427 - val_loss: 2.2926 - val_accuracy: 0.3013\n",
      "Restoring the best weights from epoch 110\n",
      "Training fold 5\n",
      "Train on 143652 samples, validate on 35903 samples\n",
      "Epoch 1/110\n",
      "143652/143652 [==============================] - 3s 20us/sample - loss: 3.0105 - accuracy: 0.1155 - val_loss: 2.9762 - val_accuracy: 0.1716\n",
      "Epoch 2/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.6715 - accuracy: 0.1959 - val_loss: 2.7275 - val_accuracy: 0.2052\n",
      "Epoch 3/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.5629 - accuracy: 0.2234 - val_loss: 2.5548 - val_accuracy: 0.2388\n",
      "Epoch 4/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.5057 - accuracy: 0.2397 - val_loss: 2.4617 - val_accuracy: 0.2543\n",
      "Epoch 5/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.4728 - accuracy: 0.2499 - val_loss: 2.4357 - val_accuracy: 0.2587\n",
      "Epoch 6/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.4457 - accuracy: 0.2566 - val_loss: 2.4105 - val_accuracy: 0.2646\n",
      "Epoch 7/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.4233 - accuracy: 0.2639 - val_loss: 2.3917 - val_accuracy: 0.2685\n",
      "Epoch 8/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.4082 - accuracy: 0.2673 - val_loss: 2.3771 - val_accuracy: 0.2716\n",
      "Epoch 9/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.3941 - accuracy: 0.2709 - val_loss: 2.3722 - val_accuracy: 0.2729\n",
      "Epoch 10/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.3801 - accuracy: 0.2746 - val_loss: 2.3550 - val_accuracy: 0.2773\n",
      "Epoch 11/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.3692 - accuracy: 0.2776 - val_loss: 2.3615 - val_accuracy: 0.2778\n",
      "Epoch 12/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.3596 - accuracy: 0.2809 - val_loss: 2.3402 - val_accuracy: 0.2830\n",
      "Epoch 13/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.3493 - accuracy: 0.2832 - val_loss: 2.3358 - val_accuracy: 0.2856\n",
      "Epoch 14/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.3415 - accuracy: 0.2856 - val_loss: 2.3400 - val_accuracy: 0.2854\n",
      "Epoch 15/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.3356 - accuracy: 0.2872 - val_loss: 2.3225 - val_accuracy: 0.2886\n",
      "Epoch 16/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.3263 - accuracy: 0.2889 - val_loss: 2.3201 - val_accuracy: 0.2891\n",
      "Epoch 17/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.3206 - accuracy: 0.2912 - val_loss: 2.3124 - val_accuracy: 0.2906\n",
      "Epoch 18/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.3123 - accuracy: 0.2928 - val_loss: 2.3126 - val_accuracy: 0.2900\n",
      "Epoch 19/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.3069 - accuracy: 0.2953 - val_loss: 2.3101 - val_accuracy: 0.2910\n",
      "Epoch 20/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.3012 - accuracy: 0.2962 - val_loss: 2.3091 - val_accuracy: 0.2923\n",
      "Epoch 21/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.2938 - accuracy: 0.2981 - val_loss: 2.3015 - val_accuracy: 0.2928\n",
      "Epoch 22/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.2893 - accuracy: 0.2992 - val_loss: 2.2925 - val_accuracy: 0.2945\n",
      "Epoch 23/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.2837 - accuracy: 0.3004 - val_loss: 2.2920 - val_accuracy: 0.2972\n",
      "Epoch 24/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.2802 - accuracy: 0.3018 - val_loss: 2.2941 - val_accuracy: 0.2952\n",
      "Epoch 25/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.2763 - accuracy: 0.3022 - val_loss: 2.2829 - val_accuracy: 0.2981\n",
      "Epoch 26/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.2697 - accuracy: 0.3045 - val_loss: 2.2909 - val_accuracy: 0.2949\n",
      "Epoch 27/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.2674 - accuracy: 0.3061 - val_loss: 2.2871 - val_accuracy: 0.2954\n",
      "Epoch 28/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.2636 - accuracy: 0.3066 - val_loss: 2.2842 - val_accuracy: 0.2977\n",
      "Epoch 29/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.2566 - accuracy: 0.3074 - val_loss: 2.2869 - val_accuracy: 0.2947\n",
      "Epoch 30/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.2565 - accuracy: 0.3085 - val_loss: 2.2843 - val_accuracy: 0.2975\n",
      "Epoch 31/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.2515 - accuracy: 0.3089 - val_loss: 2.2803 - val_accuracy: 0.3001\n",
      "Epoch 32/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.2496 - accuracy: 0.3096 - val_loss: 2.2777 - val_accuracy: 0.2996\n",
      "Epoch 33/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.2458 - accuracy: 0.3105 - val_loss: 2.2780 - val_accuracy: 0.2996\n",
      "Epoch 34/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.2439 - accuracy: 0.3116 - val_loss: 2.2812 - val_accuracy: 0.2992\n",
      "Epoch 35/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.2402 - accuracy: 0.3124 - val_loss: 2.2782 - val_accuracy: 0.2996\n",
      "Epoch 36/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.2371 - accuracy: 0.3121 - val_loss: 2.2751 - val_accuracy: 0.3009\n",
      "Epoch 37/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.2346 - accuracy: 0.3133 - val_loss: 2.2799 - val_accuracy: 0.2995\n",
      "Epoch 38/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.2314 - accuracy: 0.3130 - val_loss: 2.2733 - val_accuracy: 0.3002\n",
      "Epoch 39/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.2301 - accuracy: 0.3144 - val_loss: 2.2743 - val_accuracy: 0.2992\n",
      "Epoch 40/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.2259 - accuracy: 0.3159 - val_loss: 2.2793 - val_accuracy: 0.3024\n",
      "Epoch 41/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.2232 - accuracy: 0.3157 - val_loss: 2.2741 - val_accuracy: 0.3021\n",
      "Epoch 42/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.2201 - accuracy: 0.3167 - val_loss: 2.2768 - val_accuracy: 0.3028\n",
      "Epoch 43/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.2198 - accuracy: 0.3176 - val_loss: 2.2717 - val_accuracy: 0.3024\n",
      "Epoch 44/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.2164 - accuracy: 0.3169 - val_loss: 2.2683 - val_accuracy: 0.3005\n",
      "Epoch 45/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.2139 - accuracy: 0.3177 - val_loss: 2.2710 - val_accuracy: 0.3020\n",
      "Epoch 46/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.2123 - accuracy: 0.3183 - val_loss: 2.2777 - val_accuracy: 0.3037\n",
      "Epoch 47/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.2102 - accuracy: 0.3184 - val_loss: 2.2694 - val_accuracy: 0.3030\n",
      "Epoch 48/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.2057 - accuracy: 0.3202 - val_loss: 2.2725 - val_accuracy: 0.3013\n",
      "Epoch 49/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.2045 - accuracy: 0.3203 - val_loss: 2.2673 - val_accuracy: 0.3038\n",
      "Epoch 50/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.2020 - accuracy: 0.3214 - val_loss: 2.2694 - val_accuracy: 0.3035\n",
      "Epoch 51/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.2010 - accuracy: 0.3221 - val_loss: 2.2668 - val_accuracy: 0.3039\n",
      "Epoch 52/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1999 - accuracy: 0.3209 - val_loss: 2.2688 - val_accuracy: 0.3025\n",
      "Epoch 53/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1974 - accuracy: 0.3225 - val_loss: 2.2776 - val_accuracy: 0.3003\n",
      "Epoch 54/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1945 - accuracy: 0.3230 - val_loss: 2.2740 - val_accuracy: 0.3036\n",
      "Epoch 55/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1928 - accuracy: 0.3230 - val_loss: 2.2765 - val_accuracy: 0.3029\n",
      "Epoch 56/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.1912 - accuracy: 0.3248 - val_loss: 2.2658 - val_accuracy: 0.3048\n",
      "Epoch 57/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.1911 - accuracy: 0.3240 - val_loss: 2.2715 - val_accuracy: 0.3024\n",
      "Epoch 58/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1888 - accuracy: 0.3242 - val_loss: 2.2810 - val_accuracy: 0.3014\n",
      "Epoch 59/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1885 - accuracy: 0.3247 - val_loss: 2.2667 - val_accuracy: 0.3056\n",
      "Epoch 60/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.1887 - accuracy: 0.3249 - val_loss: 2.2658 - val_accuracy: 0.3060\n",
      "Epoch 61/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.1844 - accuracy: 0.3248 - val_loss: 2.2671 - val_accuracy: 0.3035\n",
      "Epoch 62/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1829 - accuracy: 0.3271 - val_loss: 2.2642 - val_accuracy: 0.3065\n",
      "Epoch 63/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1805 - accuracy: 0.3269 - val_loss: 2.2638 - val_accuracy: 0.3032\n",
      "Epoch 64/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1781 - accuracy: 0.3274 - val_loss: 2.2662 - val_accuracy: 0.3047\n",
      "Epoch 65/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.1788 - accuracy: 0.3269 - val_loss: 2.2753 - val_accuracy: 0.3018\n",
      "Epoch 66/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.1759 - accuracy: 0.3275 - val_loss: 2.2665 - val_accuracy: 0.3049\n",
      "Epoch 67/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1742 - accuracy: 0.3283 - val_loss: 2.2722 - val_accuracy: 0.3046\n",
      "Epoch 68/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1739 - accuracy: 0.3283 - val_loss: 2.2710 - val_accuracy: 0.3058\n",
      "Epoch 69/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1722 - accuracy: 0.3285 - val_loss: 2.2777 - val_accuracy: 0.3026\n",
      "Epoch 70/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.1724 - accuracy: 0.3286 - val_loss: 2.2675 - val_accuracy: 0.3046\n",
      "Epoch 71/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.1703 - accuracy: 0.3297 - val_loss: 2.2688 - val_accuracy: 0.3051\n",
      "Epoch 72/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.1681 - accuracy: 0.3293 - val_loss: 2.2712 - val_accuracy: 0.3030\n",
      "Epoch 73/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1664 - accuracy: 0.3308 - val_loss: 2.2714 - val_accuracy: 0.3055\n",
      "Epoch 74/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1671 - accuracy: 0.3319 - val_loss: 2.2737 - val_accuracy: 0.3074\n",
      "Epoch 75/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1651 - accuracy: 0.3308 - val_loss: 2.2644 - val_accuracy: 0.3060\n",
      "Epoch 76/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1627 - accuracy: 0.3309 - val_loss: 2.2729 - val_accuracy: 0.3040\n",
      "Epoch 77/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1639 - accuracy: 0.3307 - val_loss: 2.2675 - val_accuracy: 0.3068\n",
      "Epoch 78/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1618 - accuracy: 0.3308 - val_loss: 2.2672 - val_accuracy: 0.3055\n",
      "Epoch 79/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.1608 - accuracy: 0.3323 - val_loss: 2.2726 - val_accuracy: 0.3019\n",
      "Epoch 80/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1580 - accuracy: 0.3334 - val_loss: 2.2687 - val_accuracy: 0.3056\n",
      "Epoch 81/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1583 - accuracy: 0.3307 - val_loss: 2.2719 - val_accuracy: 0.3041\n",
      "Epoch 82/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.1560 - accuracy: 0.3341 - val_loss: 2.2747 - val_accuracy: 0.3066\n",
      "Epoch 83/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.1579 - accuracy: 0.3329 - val_loss: 2.2707 - val_accuracy: 0.3058\n",
      "Epoch 84/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.1545 - accuracy: 0.3341 - val_loss: 2.2712 - val_accuracy: 0.3056\n",
      "Epoch 85/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.1536 - accuracy: 0.3334 - val_loss: 2.2735 - val_accuracy: 0.3064\n",
      "Epoch 86/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.1513 - accuracy: 0.3352 - val_loss: 2.2717 - val_accuracy: 0.3047\n",
      "Epoch 87/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1506 - accuracy: 0.3348 - val_loss: 2.2665 - val_accuracy: 0.3065\n",
      "Epoch 88/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.1511 - accuracy: 0.3343 - val_loss: 2.2705 - val_accuracy: 0.3042\n",
      "Epoch 89/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.1496 - accuracy: 0.3347 - val_loss: 2.2720 - val_accuracy: 0.3038\n",
      "Epoch 90/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.1492 - accuracy: 0.3356 - val_loss: 2.2731 - val_accuracy: 0.3064\n",
      "Epoch 91/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1468 - accuracy: 0.3366 - val_loss: 2.2747 - val_accuracy: 0.3057\n",
      "Epoch 92/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.1467 - accuracy: 0.3358 - val_loss: 2.2731 - val_accuracy: 0.3034\n",
      "Epoch 93/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1456 - accuracy: 0.3365 - val_loss: 2.2687 - val_accuracy: 0.3074\n",
      "Epoch 94/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1471 - accuracy: 0.3363 - val_loss: 2.2717 - val_accuracy: 0.3057\n",
      "Epoch 95/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1447 - accuracy: 0.3354 - val_loss: 2.2769 - val_accuracy: 0.3016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1443 - accuracy: 0.3355 - val_loss: 2.2767 - val_accuracy: 0.3047\n",
      "Epoch 97/110\n",
      "143652/143652 [==============================] - 1s 5us/sample - loss: 2.1443 - accuracy: 0.3376 - val_loss: 2.2826 - val_accuracy: 0.3044\n",
      "Epoch 98/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1395 - accuracy: 0.3379 - val_loss: 2.2757 - val_accuracy: 0.3065\n",
      "Epoch 99/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1398 - accuracy: 0.3377 - val_loss: 2.2791 - val_accuracy: 0.3063\n",
      "Epoch 100/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1413 - accuracy: 0.3380 - val_loss: 2.2705 - val_accuracy: 0.3054\n",
      "Epoch 101/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1361 - accuracy: 0.3377 - val_loss: 2.2709 - val_accuracy: 0.3072\n",
      "Epoch 102/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1371 - accuracy: 0.3386 - val_loss: 2.2767 - val_accuracy: 0.3050\n",
      "Epoch 103/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1413 - accuracy: 0.3381 - val_loss: 2.2729 - val_accuracy: 0.3044\n",
      "Epoch 104/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1353 - accuracy: 0.3397 - val_loss: 2.2834 - val_accuracy: 0.3053\n",
      "Epoch 105/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1363 - accuracy: 0.3392 - val_loss: 2.2730 - val_accuracy: 0.3062\n",
      "Epoch 106/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1354 - accuracy: 0.3379 - val_loss: 2.2690 - val_accuracy: 0.3046\n",
      "Epoch 107/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1341 - accuracy: 0.3391 - val_loss: 2.2710 - val_accuracy: 0.3061\n",
      "Epoch 108/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1342 - accuracy: 0.3406 - val_loss: 2.2757 - val_accuracy: 0.3030\n",
      "Epoch 109/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1339 - accuracy: 0.3402 - val_loss: 2.2699 - val_accuracy: 0.3064\n",
      "Epoch 110/110\n",
      "143652/143652 [==============================] - 1s 6us/sample - loss: 2.1298 - accuracy: 0.3412 - val_loss: 2.2736 - val_accuracy: 0.3057\n",
      "Restoring the best weights from epoch 110\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>avg_accuracy</th>\n",
       "      <td>0.332491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_batch_val_accuracy</th>\n",
       "      <td>0.304302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_loss</th>\n",
       "      <td>2.164948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_val_accuracy</th>\n",
       "      <td>0.304302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_val_f1_macro</th>\n",
       "      <td>0.286062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_val_f1_micro</th>\n",
       "      <td>0.304302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_val_loss</th>\n",
       "      <td>2.271458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0\n",
       "avg_accuracy            0.332491\n",
       "avg_batch_val_accuracy  0.304302\n",
       "avg_loss                2.164948\n",
       "avg_val_accuracy        0.304302\n",
       "avg_val_f1_macro        0.286062\n",
       "avg_val_f1_micro        0.304302\n",
       "avg_val_loss            2.271458"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_fold(x_train, y_train, x_val, y_val, run_name, fold=None):\n",
    "\n",
    "    input_shape = x_train[0].shape\n",
    "    model = create_model(input_shape)\n",
    "    \n",
    "    if fold is not None:\n",
    "        run_name += '_fold' + str(fold)\n",
    "        print('Training fold ' + str(fold))\n",
    "    \n",
    "    tk_board = TensorBoard(log_dir=constants.LOGS_PATH + run_name)\n",
    "    weight_restorer = BestWeightsRestorer(monitor='val_loss', mode='min', verbose=1)\n",
    "    \n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        validation_data=[x_val, y_val],\n",
    "        epochs=110,\n",
    "        batch_size=2000,\n",
    "        shuffle=True,\n",
    "        callbacks=[tk_board, weight_restorer],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    prob_predictions = model.predict(x_val)\n",
    "    predictions = prob_predictions.argmax(axis=1)\n",
    "    \n",
    "    return {\n",
    "        'history': history.history,\n",
    "        'predictions': predictions\n",
    "    }\n",
    "\n",
    "experiment_number += 1\n",
    "run_name = 'base_mlp_marsyas_cv_' + str(experiment_number)\n",
    "\n",
    "results = resultsUtil.cross_validate(\n",
    "    x=ftrs,\n",
    "    y=lbls_oh,\n",
    "    y_label_encoded=lbl_encoded_lbls,\n",
    "    n_splits=5,\n",
    "    train_func=train_fold,\n",
    "    run_name=run_name\n",
    ")\n",
    "\n",
    "runUtil.save_run_results(run_name, results)\n",
    "\n",
    "resultsdf = pd.DataFrame([results]).transpose()\n",
    "\n",
    "display(resultsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>avg_accuracy</th>\n",
       "      <td>0.184512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_batch_val_accuracy</th>\n",
       "      <td>0.179610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_loss</th>\n",
       "      <td>2.706497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_val_accuracy</th>\n",
       "      <td>0.179611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_val_f1_macro</th>\n",
       "      <td>0.109814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_val_f1_micro</th>\n",
       "      <td>0.179611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_val_loss</th>\n",
       "      <td>2.845150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0\n",
       "avg_accuracy            0.184512\n",
       "avg_batch_val_accuracy  0.179610\n",
       "avg_loss                2.706497\n",
       "avg_val_accuracy        0.179611\n",
       "avg_val_f1_macro        0.109814\n",
       "avg_val_f1_micro        0.179611\n",
       "avg_val_loss            2.845150"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resultsdf = pd.DataFrame([results]).transpose()\n",
    "\n",
    "display(resultsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/110\n",
      "179555/179555 [==============================] - 3s 14us/sample - loss: 2.9494 - accuracy: 0.1338\n",
      "Epoch 2/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.6211 - accuracy: 0.2082\n",
      "Epoch 3/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.5307 - accuracy: 0.2327\n",
      "Epoch 4/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.4850 - accuracy: 0.2468\n",
      "Epoch 5/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.4536 - accuracy: 0.2559\n",
      "Epoch 6/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.4280 - accuracy: 0.2642\n",
      "Epoch 7/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.4047 - accuracy: 0.2713\n",
      "Epoch 8/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.3850 - accuracy: 0.2749\n",
      "Epoch 9/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.3698 - accuracy: 0.2777\n",
      "Epoch 10/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.3548 - accuracy: 0.2830\n",
      "Epoch 11/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.3424 - accuracy: 0.2857\n",
      "Epoch 12/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.3321 - accuracy: 0.2891\n",
      "Epoch 13/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.3226 - accuracy: 0.2910\n",
      "Epoch 14/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.3153 - accuracy: 0.2934\n",
      "Epoch 15/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.3071 - accuracy: 0.2938\n",
      "Epoch 16/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2995 - accuracy: 0.2961\n",
      "Epoch 17/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2965 - accuracy: 0.2972\n",
      "Epoch 18/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2922 - accuracy: 0.2984\n",
      "Epoch 19/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2850 - accuracy: 0.3007\n",
      "Epoch 20/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2802 - accuracy: 0.3017\n",
      "Epoch 21/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2762 - accuracy: 0.3024\n",
      "Epoch 22/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2718 - accuracy: 0.3041\n",
      "Epoch 23/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2664 - accuracy: 0.3060\n",
      "Epoch 24/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2641 - accuracy: 0.3070\n",
      "Epoch 25/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2600 - accuracy: 0.3079\n",
      "Epoch 26/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2566 - accuracy: 0.3084\n",
      "Epoch 27/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2520 - accuracy: 0.3095\n",
      "Epoch 28/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2485 - accuracy: 0.3100\n",
      "Epoch 29/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2464 - accuracy: 0.3100\n",
      "Epoch 30/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2464 - accuracy: 0.3107\n",
      "Epoch 31/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2422 - accuracy: 0.3124\n",
      "Epoch 32/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2356 - accuracy: 0.3139\n",
      "Epoch 33/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2341 - accuracy: 0.3134\n",
      "Epoch 34/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2322 - accuracy: 0.3145\n",
      "Epoch 35/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2301 - accuracy: 0.3155\n",
      "Epoch 36/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2273 - accuracy: 0.3162\n",
      "Epoch 37/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2235 - accuracy: 0.3176\n",
      "Epoch 38/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2219 - accuracy: 0.3177\n",
      "Epoch 39/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2231 - accuracy: 0.3175\n",
      "Epoch 40/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2191 - accuracy: 0.3176\n",
      "Epoch 41/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2162 - accuracy: 0.3190\n",
      "Epoch 42/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2152 - accuracy: 0.3189\n",
      "Epoch 43/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2121 - accuracy: 0.3218\n",
      "Epoch 44/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2092 - accuracy: 0.3213\n",
      "Epoch 45/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2102 - accuracy: 0.3206\n",
      "Epoch 46/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2066 - accuracy: 0.3213\n",
      "Epoch 47/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2067 - accuracy: 0.3211\n",
      "Epoch 48/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2024 - accuracy: 0.3230\n",
      "Epoch 49/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.2019 - accuracy: 0.3228\n",
      "Epoch 50/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1998 - accuracy: 0.3243\n",
      "Epoch 51/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1978 - accuracy: 0.3235\n",
      "Epoch 52/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1953 - accuracy: 0.3243\n",
      "Epoch 53/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1938 - accuracy: 0.3257\n",
      "Epoch 54/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1950 - accuracy: 0.3251\n",
      "Epoch 55/110\n",
      "179555/179555 [==============================] - 1s 8us/sample - loss: 2.1924 - accuracy: 0.3258\n",
      "Epoch 56/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1912 - accuracy: 0.3253\n",
      "Epoch 57/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1913 - accuracy: 0.3254\n",
      "Epoch 58/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1868 - accuracy: 0.3273\n",
      "Epoch 59/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1878 - accuracy: 0.3272\n",
      "Epoch 60/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1863 - accuracy: 0.3277\n",
      "Epoch 61/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1841 - accuracy: 0.3271\n",
      "Epoch 62/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1825 - accuracy: 0.3284\n",
      "Epoch 63/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1801 - accuracy: 0.3281\n",
      "Epoch 64/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1811 - accuracy: 0.3306\n",
      "Epoch 65/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1773 - accuracy: 0.3295\n",
      "Epoch 66/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1765 - accuracy: 0.3306\n",
      "Epoch 67/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1752 - accuracy: 0.3299\n",
      "Epoch 68/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1739 - accuracy: 0.3301\n",
      "Epoch 69/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1741 - accuracy: 0.3304\n",
      "Epoch 70/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1729 - accuracy: 0.3311\n",
      "Epoch 71/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1709 - accuracy: 0.3316\n",
      "Epoch 72/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1710 - accuracy: 0.3311\n",
      "Epoch 73/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1680 - accuracy: 0.3324\n",
      "Epoch 74/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1687 - accuracy: 0.3326\n",
      "Epoch 75/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1675 - accuracy: 0.3328\n",
      "Epoch 76/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1661 - accuracy: 0.3321\n",
      "Epoch 77/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1644 - accuracy: 0.3318\n",
      "Epoch 78/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1651 - accuracy: 0.3327\n",
      "Epoch 79/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1638 - accuracy: 0.3332\n",
      "Epoch 80/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1597 - accuracy: 0.3344\n",
      "Epoch 81/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1618 - accuracy: 0.3341\n",
      "Epoch 82/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1590 - accuracy: 0.3355\n",
      "Epoch 83/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1607 - accuracy: 0.3336\n",
      "Epoch 84/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1569 - accuracy: 0.3348\n",
      "Epoch 85/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1558 - accuracy: 0.3346\n",
      "Epoch 86/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1561 - accuracy: 0.3347\n",
      "Epoch 87/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1552 - accuracy: 0.3357\n",
      "Epoch 88/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1538 - accuracy: 0.3351\n",
      "Epoch 89/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1519 - accuracy: 0.3373\n",
      "Epoch 90/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1517 - accuracy: 0.3361\n",
      "Epoch 91/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1513 - accuracy: 0.3375\n",
      "Epoch 92/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1508 - accuracy: 0.3368\n",
      "Epoch 93/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1507 - accuracy: 0.3367\n",
      "Epoch 94/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1506 - accuracy: 0.3371\n",
      "Epoch 95/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1473 - accuracy: 0.3381\n",
      "Epoch 96/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1472 - accuracy: 0.3378\n",
      "Epoch 97/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1456 - accuracy: 0.3371\n",
      "Epoch 98/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1461 - accuracy: 0.3378\n",
      "Epoch 99/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1439 - accuracy: 0.3390\n",
      "Epoch 100/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1425 - accuracy: 0.3390\n",
      "Epoch 101/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1442 - accuracy: 0.3388\n",
      "Epoch 102/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1418 - accuracy: 0.3382\n",
      "Epoch 103/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1414 - accuracy: 0.3383\n",
      "Epoch 104/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1419 - accuracy: 0.3395\n",
      "Epoch 105/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1384 - accuracy: 0.3400\n",
      "Epoch 106/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1401 - accuracy: 0.3387\n",
      "Epoch 107/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1393 - accuracy: 0.3396\n",
      "Epoch 108/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1398 - accuracy: 0.3411\n",
      "Epoch 109/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1367 - accuracy: 0.3401\n",
      "Epoch 110/110\n",
      "179555/179555 [==============================] - 1s 7us/sample - loss: 2.1345 - accuracy: 0.3410\n"
     ]
    }
   ],
   "source": [
    "def train_final_model(x, y, run_name):\n",
    "    \n",
    "    save_path = constants.MODELS_PATH + run_name + '.h5' \n",
    "    \n",
    "    input_shape = x[0].shape\n",
    "    model = create_model(input_shape)\n",
    "    \n",
    "    tk_board = TensorBoard(log_dir=constants.LOGS_PATH + run_name)\n",
    "    \n",
    "    model.fit(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        epochs=110,\n",
    "        batch_size=2000,\n",
    "        shuffle=True,\n",
    "        callbacks=[tk_board],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model.save(save_path)\n",
    "\n",
    "\n",
    "train_final_model(ftrs, lbls_oh, 'base_mlp_marsyas_final')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
