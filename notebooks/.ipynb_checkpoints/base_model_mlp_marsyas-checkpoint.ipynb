{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload chaque module spécifié par %aimport\n",
    "%autoreload 1\n",
    "\n",
    "%aimport src.kerasCallbacks\n",
    "%aimport src.results\n",
    "\n",
    "from IPython.display import display\n",
    "from os import path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "from src import constants\n",
    "from src.kerasCallbacks import BestWeightsRestorer\n",
    "import src.results as resultsUtil\n",
    "import src.runUtil as runUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_set():\n",
    "\n",
    "    file_path = path.join(constants.DATA_PATH, 'marsyas_base_split.csv')\n",
    "    \n",
    "    ftrs = np.array(pd.read_csv(file_path, header=None).values[:,2:-1])\n",
    "    lbls = np.array(pd.read_csv(file_path, header=None).values[:,-1])\n",
    "    \n",
    "    return ftrs, lbls\n",
    "\n",
    "def scale_features(ftrs):\n",
    "    \n",
    "    standardscaler = StandardScaler()\n",
    "    ftrs_scld = standardscaler.fit_transform(ftrs)\n",
    "    \n",
    "    return ftrs_scld\n",
    "\n",
    "def one_hot_labels(lbls):\n",
    "    \n",
    "    lbls_1d = lbls.reshape(len(lbls), 1)\n",
    "        \n",
    "    oh_encoder = OneHotEncoder(sparse=False)\n",
    "    lbls_oh = oh_encoder.fit_transform(lbls_1d)\n",
    "    \n",
    "    return lbls_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the features and the labels\n",
    "raw_ftrs, raw_lbls = load_feature_set()\n",
    "\n",
    "lbls_oh = one_hot_labels(raw_lbls)\n",
    "lbl_encoder = LabelEncoder()\n",
    "lbl_encoded_lbls = lbl_encoder.fit_transform(raw_lbls)\n",
    "\n",
    "ftrs = scale_features(raw_ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    \n",
    "    inputs = Input(shape=input_shape, name='mlb_marsyas_input')\n",
    "    \n",
    "    layer = Dense(90, activation='relu', name='mlb_marsyas_dense_1')(inputs)\n",
    "    layer = Dense(85, activation='relu', name='mlb_marsyas_dense_2')(layer)\n",
    "    \n",
    "    layer = Dropout(0.10, name='mlb_marsyas_droupout_1')(layer)\n",
    "    \n",
    "    layer = Dense(80, activation='relu', name='mlb_marsyas_dense_3')(layer)\n",
    "    layer = Dense(75, activation='relu', name='mlb_marsyas_dense_4')(layer)\n",
    "    layer = Dense(70, activation='relu', name='mlb_marsyas_dense_5')(layer)\n",
    "    \n",
    "    layer = BatchNormalization(name='mlb_marsyas_batch_norm_1')(layer)\n",
    "    \n",
    "    layer = Dense(65, activation='relu', name='mlb_marsyas_dense_6')(layer)\n",
    "    layer = Dense(60, activation='relu', name='mlb_marsyas_dense_7')(layer)\n",
    "    \n",
    "    layer = BatchNormalization(name='mlb_marsyas_batch_norm_2')(layer)\n",
    "    \n",
    "    layer = Dense(55, activation='relu', name='mlb_marsyas_dense_8')(layer)\n",
    "    layer = Dense(50, activation='relu', name='mlb_marsyas_dense_9')(layer)\n",
    "    \n",
    "    layer = Dropout(0.10, name='mlb_marsyas_droupout_2')(layer)\n",
    "    \n",
    "    layer = Dense(45, activation='relu', name='mlb_marsyas_dense_10')(layer)\n",
    "    layer = Dense(40, activation='relu', name='mlb_marsyas_dense_11')(layer)\n",
    "    \n",
    "    layer = Dropout(0.10, name='mlb_marsyas_droupout_3')(layer)\n",
    "    \n",
    "    layer = Dense(30, activation='relu', name='mlb_marsyas_dense_12')(layer)\n",
    "    \n",
    "    outputs = Dense(25, activation='softmax', name='mlb_marsyas_output')(layer)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(lr=0.001), \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=[\n",
    "            CategoricalAccuracy(name='accuracy')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/marc/anaconda3/envs/ml_main/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/marc/anaconda3/envs/ml_main/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Training fold 1\n",
      "Train on 114904 samples, validate on 28740 samples\n",
      "WARNING:tensorflow:From /home/marc/anaconda3/envs/ml_main/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/110\n",
      "114904/114904 [==============================] - 2s 13us/sample - loss: 3.0071 - accuracy: 0.1129 - val_loss: 2.9964 - val_accuracy: 0.1472\n",
      "Epoch 2/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.7419 - accuracy: 0.1785 - val_loss: 2.8229 - val_accuracy: 0.2025\n",
      "Epoch 3/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.6115 - accuracy: 0.2101 - val_loss: 2.6793 - val_accuracy: 0.2100\n",
      "Epoch 4/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.5436 - accuracy: 0.2292 - val_loss: 2.5864 - val_accuracy: 0.2276\n",
      "Epoch 5/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.5014 - accuracy: 0.2413 - val_loss: 2.4955 - val_accuracy: 0.2435\n",
      "Epoch 6/110\n",
      "114904/114904 [==============================] - 0s 4us/sample - loss: 2.4692 - accuracy: 0.2512 - val_loss: 2.4477 - val_accuracy: 0.2572\n",
      "Epoch 7/110\n",
      "114904/114904 [==============================] - 0s 4us/sample - loss: 2.4454 - accuracy: 0.2586 - val_loss: 2.4243 - val_accuracy: 0.2605\n",
      "Epoch 8/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.4240 - accuracy: 0.2663 - val_loss: 2.4028 - val_accuracy: 0.2651\n",
      "Epoch 9/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.4082 - accuracy: 0.2690 - val_loss: 2.3889 - val_accuracy: 0.2701\n",
      "Epoch 10/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.3936 - accuracy: 0.2726 - val_loss: 2.3779 - val_accuracy: 0.2740\n",
      "Epoch 11/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.3808 - accuracy: 0.2776 - val_loss: 2.3641 - val_accuracy: 0.2777\n",
      "Epoch 12/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.3706 - accuracy: 0.2799 - val_loss: 2.3595 - val_accuracy: 0.2777\n",
      "Epoch 13/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.3574 - accuracy: 0.2822 - val_loss: 2.3488 - val_accuracy: 0.2815\n",
      "Epoch 14/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.3494 - accuracy: 0.2852 - val_loss: 2.3439 - val_accuracy: 0.2800\n",
      "Epoch 15/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.3396 - accuracy: 0.2862 - val_loss: 2.3458 - val_accuracy: 0.2840\n",
      "Epoch 16/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.3294 - accuracy: 0.2883 - val_loss: 2.3423 - val_accuracy: 0.2834\n",
      "Epoch 17/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.3259 - accuracy: 0.2905 - val_loss: 2.3281 - val_accuracy: 0.2856\n",
      "Epoch 18/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.3178 - accuracy: 0.2924 - val_loss: 2.3329 - val_accuracy: 0.2860\n",
      "Epoch 19/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.3139 - accuracy: 0.2931 - val_loss: 2.3227 - val_accuracy: 0.2888\n",
      "Epoch 20/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.3036 - accuracy: 0.2963 - val_loss: 2.3294 - val_accuracy: 0.2898\n",
      "Epoch 21/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.3001 - accuracy: 0.2973 - val_loss: 2.3197 - val_accuracy: 0.2875\n",
      "Epoch 22/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.2955 - accuracy: 0.2977 - val_loss: 2.3176 - val_accuracy: 0.2902\n",
      "Epoch 23/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2885 - accuracy: 0.2984 - val_loss: 2.3086 - val_accuracy: 0.2928\n",
      "Epoch 24/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2829 - accuracy: 0.3003 - val_loss: 2.3309 - val_accuracy: 0.2864\n",
      "Epoch 25/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2804 - accuracy: 0.3010 - val_loss: 2.3139 - val_accuracy: 0.2911\n",
      "Epoch 26/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2730 - accuracy: 0.3036 - val_loss: 2.3080 - val_accuracy: 0.2929\n",
      "Epoch 27/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2719 - accuracy: 0.3027 - val_loss: 2.3070 - val_accuracy: 0.2929\n",
      "Epoch 28/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2679 - accuracy: 0.3050 - val_loss: 2.3144 - val_accuracy: 0.2906\n",
      "Epoch 29/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2611 - accuracy: 0.3066 - val_loss: 2.3083 - val_accuracy: 0.2938\n",
      "Epoch 30/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2565 - accuracy: 0.3081 - val_loss: 2.3016 - val_accuracy: 0.2933\n",
      "Epoch 31/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2555 - accuracy: 0.3087 - val_loss: 2.2994 - val_accuracy: 0.2962\n",
      "Epoch 32/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2523 - accuracy: 0.3105 - val_loss: 2.3038 - val_accuracy: 0.2950\n",
      "Epoch 33/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2479 - accuracy: 0.3109 - val_loss: 2.2983 - val_accuracy: 0.2958\n",
      "Epoch 34/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2441 - accuracy: 0.3116 - val_loss: 2.3018 - val_accuracy: 0.2979\n",
      "Epoch 35/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2420 - accuracy: 0.3115 - val_loss: 2.3058 - val_accuracy: 0.2929\n",
      "Epoch 36/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2401 - accuracy: 0.3133 - val_loss: 2.3028 - val_accuracy: 0.2935\n",
      "Epoch 37/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2355 - accuracy: 0.3134 - val_loss: 2.3007 - val_accuracy: 0.2937\n",
      "Epoch 38/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2349 - accuracy: 0.3138 - val_loss: 2.3010 - val_accuracy: 0.2958\n",
      "Epoch 39/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2287 - accuracy: 0.3144 - val_loss: 2.2987 - val_accuracy: 0.2960\n",
      "Epoch 40/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2287 - accuracy: 0.3160 - val_loss: 2.3013 - val_accuracy: 0.2938\n",
      "Epoch 41/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2249 - accuracy: 0.3149 - val_loss: 2.3058 - val_accuracy: 0.2925\n",
      "Epoch 42/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2235 - accuracy: 0.3164 - val_loss: 2.2964 - val_accuracy: 0.2990\n",
      "Epoch 43/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2220 - accuracy: 0.3145 - val_loss: 2.2985 - val_accuracy: 0.2970\n",
      "Epoch 44/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2189 - accuracy: 0.3189 - val_loss: 2.3091 - val_accuracy: 0.2946\n",
      "Epoch 45/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2165 - accuracy: 0.3184 - val_loss: 2.3004 - val_accuracy: 0.2931\n",
      "Epoch 46/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2118 - accuracy: 0.3211 - val_loss: 2.2996 - val_accuracy: 0.2967\n",
      "Epoch 47/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2118 - accuracy: 0.3203 - val_loss: 2.3062 - val_accuracy: 0.2952\n",
      "Epoch 48/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2072 - accuracy: 0.3210 - val_loss: 2.3018 - val_accuracy: 0.2965\n",
      "Epoch 49/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2027 - accuracy: 0.3220 - val_loss: 2.3049 - val_accuracy: 0.2955\n",
      "Epoch 50/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.2035 - accuracy: 0.3204 - val_loss: 2.3027 - val_accuracy: 0.2934\n",
      "Epoch 51/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.2021 - accuracy: 0.3207 - val_loss: 2.2985 - val_accuracy: 0.2978\n",
      "Epoch 52/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1992 - accuracy: 0.3219 - val_loss: 2.3002 - val_accuracy: 0.2980\n",
      "Epoch 53/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1964 - accuracy: 0.3232 - val_loss: 2.2952 - val_accuracy: 0.2972\n",
      "Epoch 54/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1933 - accuracy: 0.3241 - val_loss: 2.2994 - val_accuracy: 0.2969\n",
      "Epoch 55/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1949 - accuracy: 0.3252 - val_loss: 2.3120 - val_accuracy: 0.2953\n",
      "Epoch 56/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1888 - accuracy: 0.3249 - val_loss: 2.2977 - val_accuracy: 0.2973\n",
      "Epoch 57/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1878 - accuracy: 0.3247 - val_loss: 2.3095 - val_accuracy: 0.2930\n",
      "Epoch 58/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1860 - accuracy: 0.3250 - val_loss: 2.3070 - val_accuracy: 0.2934\n",
      "Epoch 59/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.1841 - accuracy: 0.3260 - val_loss: 2.2983 - val_accuracy: 0.2994\n",
      "Epoch 60/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1838 - accuracy: 0.3269 - val_loss: 2.3113 - val_accuracy: 0.2976\n",
      "Epoch 61/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1807 - accuracy: 0.3272 - val_loss: 2.3059 - val_accuracy: 0.2976\n",
      "Epoch 62/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1795 - accuracy: 0.3268 - val_loss: 2.3001 - val_accuracy: 0.3004\n",
      "Epoch 63/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1775 - accuracy: 0.3271 - val_loss: 2.3008 - val_accuracy: 0.2977\n",
      "Epoch 64/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1746 - accuracy: 0.3283 - val_loss: 2.2983 - val_accuracy: 0.2997\n",
      "Epoch 65/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1749 - accuracy: 0.3284 - val_loss: 2.3004 - val_accuracy: 0.2978\n",
      "Epoch 66/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1733 - accuracy: 0.3298 - val_loss: 2.3003 - val_accuracy: 0.2993\n",
      "Epoch 67/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1693 - accuracy: 0.3291 - val_loss: 2.3065 - val_accuracy: 0.2983\n",
      "Epoch 68/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1705 - accuracy: 0.3309 - val_loss: 2.3140 - val_accuracy: 0.2952\n",
      "Epoch 69/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.1688 - accuracy: 0.3304 - val_loss: 2.3041 - val_accuracy: 0.2993\n",
      "Epoch 70/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1672 - accuracy: 0.3311 - val_loss: 2.3087 - val_accuracy: 0.2970\n",
      "Epoch 71/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1651 - accuracy: 0.3310 - val_loss: 2.3003 - val_accuracy: 0.3014\n",
      "Epoch 72/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.1630 - accuracy: 0.3321 - val_loss: 2.3051 - val_accuracy: 0.3005\n",
      "Epoch 73/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.1613 - accuracy: 0.3313 - val_loss: 2.3043 - val_accuracy: 0.3002\n",
      "Epoch 74/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1621 - accuracy: 0.3328 - val_loss: 2.3050 - val_accuracy: 0.2978\n",
      "Epoch 75/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.1591 - accuracy: 0.3324 - val_loss: 2.3003 - val_accuracy: 0.2989\n",
      "Epoch 76/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.1602 - accuracy: 0.3335 - val_loss: 2.3082 - val_accuracy: 0.2966\n",
      "Epoch 77/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.1586 - accuracy: 0.3329 - val_loss: 2.3013 - val_accuracy: 0.3007\n",
      "Epoch 78/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1538 - accuracy: 0.3340 - val_loss: 2.3108 - val_accuracy: 0.2979\n",
      "Epoch 79/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.1545 - accuracy: 0.3355 - val_loss: 2.3135 - val_accuracy: 0.2961\n",
      "Epoch 80/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1516 - accuracy: 0.3357 - val_loss: 2.3030 - val_accuracy: 0.3007\n",
      "Epoch 81/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.1518 - accuracy: 0.3356 - val_loss: 2.3099 - val_accuracy: 0.2983\n",
      "Epoch 82/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.1496 - accuracy: 0.3343 - val_loss: 2.3032 - val_accuracy: 0.3002\n",
      "Epoch 83/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.1439 - accuracy: 0.3359 - val_loss: 2.3039 - val_accuracy: 0.3007\n",
      "Epoch 84/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1431 - accuracy: 0.3377 - val_loss: 2.3108 - val_accuracy: 0.2990\n",
      "Epoch 85/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1432 - accuracy: 0.3360 - val_loss: 2.3083 - val_accuracy: 0.2966\n",
      "Epoch 86/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.1419 - accuracy: 0.3350 - val_loss: 2.3087 - val_accuracy: 0.2982\n",
      "Epoch 87/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1454 - accuracy: 0.3358 - val_loss: 2.3020 - val_accuracy: 0.2979\n",
      "Epoch 88/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.1422 - accuracy: 0.3374 - val_loss: 2.3086 - val_accuracy: 0.2977\n",
      "Epoch 89/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1395 - accuracy: 0.3373 - val_loss: 2.3009 - val_accuracy: 0.3024\n",
      "Epoch 90/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1382 - accuracy: 0.3368 - val_loss: 2.3202 - val_accuracy: 0.2976\n",
      "Epoch 91/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1397 - accuracy: 0.3374 - val_loss: 2.3099 - val_accuracy: 0.3006\n",
      "Epoch 92/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1368 - accuracy: 0.3382 - val_loss: 2.3156 - val_accuracy: 0.2947\n",
      "Epoch 93/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.1348 - accuracy: 0.3387 - val_loss: 2.3078 - val_accuracy: 0.2989\n",
      "Epoch 94/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1309 - accuracy: 0.3409 - val_loss: 2.3167 - val_accuracy: 0.2963\n",
      "Epoch 95/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1317 - accuracy: 0.3397 - val_loss: 2.3163 - val_accuracy: 0.2961\n",
      "Epoch 96/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1313 - accuracy: 0.3412 - val_loss: 2.3024 - val_accuracy: 0.2985\n",
      "Epoch 97/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.1299 - accuracy: 0.3397 - val_loss: 2.3073 - val_accuracy: 0.2983\n",
      "Epoch 98/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.1278 - accuracy: 0.3423 - val_loss: 2.3166 - val_accuracy: 0.2968\n",
      "Epoch 99/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1287 - accuracy: 0.3406 - val_loss: 2.3129 - val_accuracy: 0.2995\n",
      "Epoch 100/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1278 - accuracy: 0.3404 - val_loss: 2.3133 - val_accuracy: 0.2975\n",
      "Epoch 101/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.1280 - accuracy: 0.3407 - val_loss: 2.3137 - val_accuracy: 0.2970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1267 - accuracy: 0.3415 - val_loss: 2.3165 - val_accuracy: 0.2962\n",
      "Epoch 103/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1239 - accuracy: 0.3432 - val_loss: 2.3103 - val_accuracy: 0.3002\n",
      "Epoch 104/110\n",
      "114904/114904 [==============================] - 1s 4us/sample - loss: 2.1220 - accuracy: 0.3428 - val_loss: 2.3181 - val_accuracy: 0.2941\n",
      "Epoch 105/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1263 - accuracy: 0.3418 - val_loss: 2.3295 - val_accuracy: 0.2963\n",
      "Epoch 106/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1225 - accuracy: 0.3431 - val_loss: 2.3195 - val_accuracy: 0.2983\n",
      "Epoch 107/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1196 - accuracy: 0.3433 - val_loss: 2.3162 - val_accuracy: 0.2971\n",
      "Epoch 108/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1186 - accuracy: 0.3424 - val_loss: 2.3174 - val_accuracy: 0.2957\n",
      "Epoch 109/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1183 - accuracy: 0.3435 - val_loss: 2.3177 - val_accuracy: 0.2966\n",
      "Epoch 110/110\n",
      "114904/114904 [==============================] - 1s 5us/sample - loss: 2.1175 - accuracy: 0.3429 - val_loss: 2.3139 - val_accuracy: 0.2978\n",
      "Restoring the best weights from epoch 110\n",
      "Training fold 2\n",
      "Train on 114911 samples, validate on 28733 samples\n",
      "Epoch 1/110\n",
      "114911/114911 [==============================] - 1s 11us/sample - loss: 3.0643 - accuracy: 0.1180 - val_loss: 3.0737 - val_accuracy: 0.1721\n",
      "Epoch 2/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.7273 - accuracy: 0.1906 - val_loss: 2.8140 - val_accuracy: 0.1870\n",
      "Epoch 3/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.5949 - accuracy: 0.2130 - val_loss: 2.6605 - val_accuracy: 0.2073\n",
      "Epoch 4/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.5391 - accuracy: 0.2294 - val_loss: 2.5497 - val_accuracy: 0.2334\n",
      "Epoch 5/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.5046 - accuracy: 0.2415 - val_loss: 2.5034 - val_accuracy: 0.2436\n",
      "Epoch 6/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.4757 - accuracy: 0.2486 - val_loss: 2.4517 - val_accuracy: 0.2596\n",
      "Epoch 7/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.4519 - accuracy: 0.2574 - val_loss: 2.4259 - val_accuracy: 0.2644\n",
      "Epoch 8/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.4340 - accuracy: 0.2614 - val_loss: 2.4052 - val_accuracy: 0.2699\n",
      "Epoch 9/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.4169 - accuracy: 0.2672 - val_loss: 2.4010 - val_accuracy: 0.2707\n",
      "Epoch 10/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.4040 - accuracy: 0.2705 - val_loss: 2.3774 - val_accuracy: 0.2762\n",
      "Epoch 11/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.3911 - accuracy: 0.2742 - val_loss: 2.3744 - val_accuracy: 0.2755\n",
      "Epoch 12/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.3792 - accuracy: 0.2778 - val_loss: 2.3717 - val_accuracy: 0.2759\n",
      "Epoch 13/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.3686 - accuracy: 0.2796 - val_loss: 2.3656 - val_accuracy: 0.2801\n",
      "Epoch 14/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.3579 - accuracy: 0.2833 - val_loss: 2.3467 - val_accuracy: 0.2829\n",
      "Epoch 15/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.3491 - accuracy: 0.2841 - val_loss: 2.3482 - val_accuracy: 0.2844\n",
      "Epoch 16/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.3416 - accuracy: 0.2891 - val_loss: 2.3441 - val_accuracy: 0.2866\n",
      "Epoch 17/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.3337 - accuracy: 0.2897 - val_loss: 2.3392 - val_accuracy: 0.2852\n",
      "Epoch 18/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.3269 - accuracy: 0.2905 - val_loss: 2.3423 - val_accuracy: 0.2841\n",
      "Epoch 19/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.3186 - accuracy: 0.2919 - val_loss: 2.3328 - val_accuracy: 0.2864\n",
      "Epoch 20/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.3134 - accuracy: 0.2961 - val_loss: 2.3397 - val_accuracy: 0.2866\n",
      "Epoch 21/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.3064 - accuracy: 0.2959 - val_loss: 2.3322 - val_accuracy: 0.2878\n",
      "Epoch 22/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.3025 - accuracy: 0.2958 - val_loss: 2.3259 - val_accuracy: 0.2870\n",
      "Epoch 23/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2949 - accuracy: 0.2997 - val_loss: 2.3178 - val_accuracy: 0.2916\n",
      "Epoch 24/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2896 - accuracy: 0.3010 - val_loss: 2.3249 - val_accuracy: 0.2898\n",
      "Epoch 25/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2847 - accuracy: 0.3026 - val_loss: 2.3216 - val_accuracy: 0.2897\n",
      "Epoch 26/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2812 - accuracy: 0.3011 - val_loss: 2.3165 - val_accuracy: 0.2937\n",
      "Epoch 27/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2743 - accuracy: 0.3041 - val_loss: 2.3135 - val_accuracy: 0.2944\n",
      "Epoch 28/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2703 - accuracy: 0.3057 - val_loss: 2.3166 - val_accuracy: 0.2918\n",
      "Epoch 29/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2673 - accuracy: 0.3064 - val_loss: 2.3157 - val_accuracy: 0.2933\n",
      "Epoch 30/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2648 - accuracy: 0.3068 - val_loss: 2.3105 - val_accuracy: 0.2925\n",
      "Epoch 31/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2603 - accuracy: 0.3086 - val_loss: 2.3186 - val_accuracy: 0.2928\n",
      "Epoch 32/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2567 - accuracy: 0.3098 - val_loss: 2.3115 - val_accuracy: 0.2945\n",
      "Epoch 33/110\n",
      "114911/114911 [==============================] - 1s 4us/sample - loss: 2.2512 - accuracy: 0.3104 - val_loss: 2.3163 - val_accuracy: 0.2931\n",
      "Epoch 34/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2502 - accuracy: 0.3099 - val_loss: 2.3155 - val_accuracy: 0.2932\n",
      "Epoch 35/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2480 - accuracy: 0.3102 - val_loss: 2.3126 - val_accuracy: 0.2931\n",
      "Epoch 36/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2437 - accuracy: 0.3121 - val_loss: 2.3093 - val_accuracy: 0.2959\n",
      "Epoch 37/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2382 - accuracy: 0.3138 - val_loss: 2.3067 - val_accuracy: 0.2946\n",
      "Epoch 38/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2369 - accuracy: 0.3152 - val_loss: 2.3132 - val_accuracy: 0.2950\n",
      "Epoch 39/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2332 - accuracy: 0.3155 - val_loss: 2.3084 - val_accuracy: 0.2952\n",
      "Epoch 40/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2311 - accuracy: 0.3160 - val_loss: 2.3039 - val_accuracy: 0.2975\n",
      "Epoch 41/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2267 - accuracy: 0.3161 - val_loss: 2.3003 - val_accuracy: 0.2961\n",
      "Epoch 42/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2264 - accuracy: 0.3171 - val_loss: 2.3092 - val_accuracy: 0.2973\n",
      "Epoch 43/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2231 - accuracy: 0.3180 - val_loss: 2.3054 - val_accuracy: 0.2939\n",
      "Epoch 44/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2197 - accuracy: 0.3187 - val_loss: 2.3047 - val_accuracy: 0.2955\n",
      "Epoch 45/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2173 - accuracy: 0.3196 - val_loss: 2.3070 - val_accuracy: 0.2953\n",
      "Epoch 46/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2151 - accuracy: 0.3206 - val_loss: 2.3092 - val_accuracy: 0.2938\n",
      "Epoch 47/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2119 - accuracy: 0.3210 - val_loss: 2.3136 - val_accuracy: 0.2982\n",
      "Epoch 48/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2089 - accuracy: 0.3222 - val_loss: 2.3118 - val_accuracy: 0.2963\n",
      "Epoch 49/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2057 - accuracy: 0.3224 - val_loss: 2.3115 - val_accuracy: 0.2967\n",
      "Epoch 50/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2059 - accuracy: 0.3237 - val_loss: 2.3013 - val_accuracy: 0.2988\n",
      "Epoch 51/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1992 - accuracy: 0.3244 - val_loss: 2.3015 - val_accuracy: 0.2964\n",
      "Epoch 52/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.2007 - accuracy: 0.3247 - val_loss: 2.3174 - val_accuracy: 0.2955\n",
      "Epoch 53/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1975 - accuracy: 0.3255 - val_loss: 2.3181 - val_accuracy: 0.2944\n",
      "Epoch 54/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1974 - accuracy: 0.3251 - val_loss: 2.3006 - val_accuracy: 0.2970\n",
      "Epoch 55/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1942 - accuracy: 0.3254 - val_loss: 2.3051 - val_accuracy: 0.2957\n",
      "Epoch 56/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1922 - accuracy: 0.3265 - val_loss: 2.3057 - val_accuracy: 0.2975\n",
      "Epoch 57/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1908 - accuracy: 0.3267 - val_loss: 2.3083 - val_accuracy: 0.2964\n",
      "Epoch 58/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1879 - accuracy: 0.3270 - val_loss: 2.3060 - val_accuracy: 0.2957\n",
      "Epoch 59/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1863 - accuracy: 0.3278 - val_loss: 2.3046 - val_accuracy: 0.2943\n",
      "Epoch 60/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1835 - accuracy: 0.3282 - val_loss: 2.3054 - val_accuracy: 0.2984\n",
      "Epoch 61/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1829 - accuracy: 0.3291 - val_loss: 2.3001 - val_accuracy: 0.2989\n",
      "Epoch 62/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1787 - accuracy: 0.3314 - val_loss: 2.3036 - val_accuracy: 0.2980\n",
      "Epoch 63/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1763 - accuracy: 0.3305 - val_loss: 2.3110 - val_accuracy: 0.2988\n",
      "Epoch 64/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1753 - accuracy: 0.3294 - val_loss: 2.3026 - val_accuracy: 0.2984\n",
      "Epoch 65/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1734 - accuracy: 0.3316 - val_loss: 2.3070 - val_accuracy: 0.2976\n",
      "Epoch 66/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1741 - accuracy: 0.3303 - val_loss: 2.3056 - val_accuracy: 0.2962\n",
      "Epoch 67/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1723 - accuracy: 0.3324 - val_loss: 2.3049 - val_accuracy: 0.2974\n",
      "Epoch 68/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1678 - accuracy: 0.3320 - val_loss: 2.3100 - val_accuracy: 0.2941\n",
      "Epoch 69/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1667 - accuracy: 0.3332 - val_loss: 2.3052 - val_accuracy: 0.2973\n",
      "Epoch 70/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1656 - accuracy: 0.3331 - val_loss: 2.3039 - val_accuracy: 0.2983\n",
      "Epoch 71/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1641 - accuracy: 0.3333 - val_loss: 2.3117 - val_accuracy: 0.2957\n",
      "Epoch 72/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1633 - accuracy: 0.3328 - val_loss: 2.3108 - val_accuracy: 0.2959\n",
      "Epoch 73/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1590 - accuracy: 0.3348 - val_loss: 2.3086 - val_accuracy: 0.2971\n",
      "Epoch 74/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1591 - accuracy: 0.3345 - val_loss: 2.3022 - val_accuracy: 0.2961\n",
      "Epoch 75/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1586 - accuracy: 0.3351 - val_loss: 2.3058 - val_accuracy: 0.2961\n",
      "Epoch 76/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1584 - accuracy: 0.3358 - val_loss: 2.3192 - val_accuracy: 0.2928\n",
      "Epoch 77/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1591 - accuracy: 0.3337 - val_loss: 2.3109 - val_accuracy: 0.2960\n",
      "Epoch 78/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1511 - accuracy: 0.3357 - val_loss: 2.3106 - val_accuracy: 0.2978\n",
      "Epoch 79/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1527 - accuracy: 0.3357 - val_loss: 2.3126 - val_accuracy: 0.2984\n",
      "Epoch 80/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1502 - accuracy: 0.3363 - val_loss: 2.3066 - val_accuracy: 0.2973\n",
      "Epoch 81/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1490 - accuracy: 0.3369 - val_loss: 2.3123 - val_accuracy: 0.2958\n",
      "Epoch 82/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1496 - accuracy: 0.3372 - val_loss: 2.3091 - val_accuracy: 0.2980\n",
      "Epoch 83/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1473 - accuracy: 0.3388 - val_loss: 2.3113 - val_accuracy: 0.2972\n",
      "Epoch 84/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1445 - accuracy: 0.3388 - val_loss: 2.3106 - val_accuracy: 0.2986\n",
      "Epoch 85/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1450 - accuracy: 0.3403 - val_loss: 2.3137 - val_accuracy: 0.2990\n",
      "Epoch 86/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1447 - accuracy: 0.3386 - val_loss: 2.3118 - val_accuracy: 0.2940\n",
      "Epoch 87/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1418 - accuracy: 0.3393 - val_loss: 2.3108 - val_accuracy: 0.2953\n",
      "Epoch 88/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1397 - accuracy: 0.3383 - val_loss: 2.3087 - val_accuracy: 0.2958\n",
      "Epoch 89/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1405 - accuracy: 0.3407 - val_loss: 2.3269 - val_accuracy: 0.2940\n",
      "Epoch 90/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1433 - accuracy: 0.3376 - val_loss: 2.3185 - val_accuracy: 0.2962\n",
      "Epoch 91/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1357 - accuracy: 0.3415 - val_loss: 2.3271 - val_accuracy: 0.2942\n",
      "Epoch 92/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1386 - accuracy: 0.3393 - val_loss: 2.3095 - val_accuracy: 0.2966\n",
      "Epoch 93/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1314 - accuracy: 0.3423 - val_loss: 2.3156 - val_accuracy: 0.2956\n",
      "Epoch 94/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1324 - accuracy: 0.3412 - val_loss: 2.3129 - val_accuracy: 0.2952\n",
      "Epoch 95/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1326 - accuracy: 0.3413 - val_loss: 2.3152 - val_accuracy: 0.2963\n",
      "Epoch 96/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1289 - accuracy: 0.3434 - val_loss: 2.3189 - val_accuracy: 0.2978\n",
      "Epoch 97/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1283 - accuracy: 0.3429 - val_loss: 2.3199 - val_accuracy: 0.2970\n",
      "Epoch 98/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1288 - accuracy: 0.3437 - val_loss: 2.3189 - val_accuracy: 0.2972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1309 - accuracy: 0.3422 - val_loss: 2.3104 - val_accuracy: 0.2983\n",
      "Epoch 100/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1262 - accuracy: 0.3428 - val_loss: 2.3191 - val_accuracy: 0.2978\n",
      "Epoch 101/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1258 - accuracy: 0.3434 - val_loss: 2.3206 - val_accuracy: 0.2937\n",
      "Epoch 102/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1280 - accuracy: 0.3429 - val_loss: 2.3186 - val_accuracy: 0.2968\n",
      "Epoch 103/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1270 - accuracy: 0.3430 - val_loss: 2.3148 - val_accuracy: 0.2960\n",
      "Epoch 104/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1223 - accuracy: 0.3448 - val_loss: 2.3309 - val_accuracy: 0.2959\n",
      "Epoch 105/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1193 - accuracy: 0.3440 - val_loss: 2.3151 - val_accuracy: 0.2974\n",
      "Epoch 106/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1197 - accuracy: 0.3423 - val_loss: 2.3234 - val_accuracy: 0.2955\n",
      "Epoch 107/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1210 - accuracy: 0.3456 - val_loss: 2.3181 - val_accuracy: 0.2949\n",
      "Epoch 108/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1165 - accuracy: 0.3459 - val_loss: 2.3253 - val_accuracy: 0.2911\n",
      "Epoch 109/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1189 - accuracy: 0.3445 - val_loss: 2.3285 - val_accuracy: 0.2952\n",
      "Epoch 110/110\n",
      "114911/114911 [==============================] - 1s 5us/sample - loss: 2.1165 - accuracy: 0.3451 - val_loss: 2.3304 - val_accuracy: 0.2938\n",
      "Restoring the best weights from epoch 110\n",
      "Training fold 3\n",
      "Train on 114914 samples, validate on 28730 samples\n",
      "Epoch 1/110\n",
      "114914/114914 [==============================] - 1s 11us/sample - loss: 3.0424 - accuracy: 0.1092 - val_loss: 3.0035 - val_accuracy: 0.1495\n",
      "Epoch 2/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.7328 - accuracy: 0.1804 - val_loss: 2.7861 - val_accuracy: 0.1874\n",
      "Epoch 3/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.6132 - accuracy: 0.2073 - val_loss: 2.6164 - val_accuracy: 0.2070\n",
      "Epoch 4/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.5476 - accuracy: 0.2239 - val_loss: 2.5619 - val_accuracy: 0.2224\n",
      "Epoch 5/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.5079 - accuracy: 0.2375 - val_loss: 2.4869 - val_accuracy: 0.2451\n",
      "Epoch 6/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.4789 - accuracy: 0.2453 - val_loss: 2.4662 - val_accuracy: 0.2500\n",
      "Epoch 7/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.4546 - accuracy: 0.2541 - val_loss: 2.4313 - val_accuracy: 0.2580\n",
      "Epoch 8/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.4336 - accuracy: 0.2608 - val_loss: 2.4154 - val_accuracy: 0.2644\n",
      "Epoch 9/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.4157 - accuracy: 0.2666 - val_loss: 2.3948 - val_accuracy: 0.2712\n",
      "Epoch 10/110\n",
      "114914/114914 [==============================] - 1s 4us/sample - loss: 2.3974 - accuracy: 0.2708 - val_loss: 2.3861 - val_accuracy: 0.2697\n",
      "Epoch 11/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.3841 - accuracy: 0.2742 - val_loss: 2.3692 - val_accuracy: 0.2737\n",
      "Epoch 12/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.3698 - accuracy: 0.2779 - val_loss: 2.3667 - val_accuracy: 0.2753\n",
      "Epoch 13/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.3587 - accuracy: 0.2807 - val_loss: 2.3537 - val_accuracy: 0.2802\n",
      "Epoch 14/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.3529 - accuracy: 0.2819 - val_loss: 2.3511 - val_accuracy: 0.2824\n",
      "Epoch 15/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.3397 - accuracy: 0.2852 - val_loss: 2.3463 - val_accuracy: 0.2846\n",
      "Epoch 16/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.3298 - accuracy: 0.2870 - val_loss: 2.3353 - val_accuracy: 0.2866\n",
      "Epoch 17/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.3252 - accuracy: 0.2893 - val_loss: 2.3350 - val_accuracy: 0.2866\n",
      "Epoch 18/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.3162 - accuracy: 0.2923 - val_loss: 2.3477 - val_accuracy: 0.2804\n",
      "Epoch 19/110\n",
      "114914/114914 [==============================] - 1s 6us/sample - loss: 2.3088 - accuracy: 0.2934 - val_loss: 2.3410 - val_accuracy: 0.2830\n",
      "Epoch 20/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.3030 - accuracy: 0.2955 - val_loss: 2.3358 - val_accuracy: 0.2850\n",
      "Epoch 21/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2984 - accuracy: 0.2959 - val_loss: 2.3183 - val_accuracy: 0.2889\n",
      "Epoch 22/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2947 - accuracy: 0.2976 - val_loss: 2.3176 - val_accuracy: 0.2909\n",
      "Epoch 23/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2886 - accuracy: 0.2990 - val_loss: 2.3191 - val_accuracy: 0.2891\n",
      "Epoch 24/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2820 - accuracy: 0.3013 - val_loss: 2.3251 - val_accuracy: 0.2887\n",
      "Epoch 25/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2784 - accuracy: 0.3013 - val_loss: 2.3108 - val_accuracy: 0.2918\n",
      "Epoch 26/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2728 - accuracy: 0.3028 - val_loss: 2.3124 - val_accuracy: 0.2913\n",
      "Epoch 27/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2676 - accuracy: 0.3044 - val_loss: 2.3133 - val_accuracy: 0.2908\n",
      "Epoch 28/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2631 - accuracy: 0.3034 - val_loss: 2.3094 - val_accuracy: 0.2911\n",
      "Epoch 29/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2600 - accuracy: 0.3052 - val_loss: 2.3057 - val_accuracy: 0.2927\n",
      "Epoch 30/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2575 - accuracy: 0.3072 - val_loss: 2.3111 - val_accuracy: 0.2912\n",
      "Epoch 31/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2543 - accuracy: 0.3073 - val_loss: 2.3088 - val_accuracy: 0.2932\n",
      "Epoch 32/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2475 - accuracy: 0.3095 - val_loss: 2.2997 - val_accuracy: 0.2949\n",
      "Epoch 33/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2437 - accuracy: 0.3097 - val_loss: 2.3040 - val_accuracy: 0.2964\n",
      "Epoch 34/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2379 - accuracy: 0.3107 - val_loss: 2.2993 - val_accuracy: 0.2953\n",
      "Epoch 35/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2370 - accuracy: 0.3104 - val_loss: 2.3013 - val_accuracy: 0.2948\n",
      "Epoch 36/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2377 - accuracy: 0.3125 - val_loss: 2.3059 - val_accuracy: 0.2943\n",
      "Epoch 37/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2324 - accuracy: 0.3132 - val_loss: 2.2930 - val_accuracy: 0.2975\n",
      "Epoch 38/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2291 - accuracy: 0.3138 - val_loss: 2.2994 - val_accuracy: 0.2946\n",
      "Epoch 39/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2271 - accuracy: 0.3156 - val_loss: 2.3061 - val_accuracy: 0.2961\n",
      "Epoch 40/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2239 - accuracy: 0.3153 - val_loss: 2.3014 - val_accuracy: 0.2938\n",
      "Epoch 41/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2223 - accuracy: 0.3168 - val_loss: 2.3035 - val_accuracy: 0.2955\n",
      "Epoch 42/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2186 - accuracy: 0.3165 - val_loss: 2.2945 - val_accuracy: 0.2966\n",
      "Epoch 43/110\n",
      "114914/114914 [==============================] - 1s 4us/sample - loss: 2.2145 - accuracy: 0.3174 - val_loss: 2.2940 - val_accuracy: 0.2964\n",
      "Epoch 44/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2112 - accuracy: 0.3200 - val_loss: 2.3046 - val_accuracy: 0.2945\n",
      "Epoch 45/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2122 - accuracy: 0.3178 - val_loss: 2.2998 - val_accuracy: 0.2951\n",
      "Epoch 46/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2067 - accuracy: 0.3191 - val_loss: 2.3066 - val_accuracy: 0.2964\n",
      "Epoch 47/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2055 - accuracy: 0.3216 - val_loss: 2.3059 - val_accuracy: 0.2936\n",
      "Epoch 48/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.2034 - accuracy: 0.3215 - val_loss: 2.2938 - val_accuracy: 0.2975\n",
      "Epoch 49/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1981 - accuracy: 0.3227 - val_loss: 2.2964 - val_accuracy: 0.2972\n",
      "Epoch 50/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1951 - accuracy: 0.3228 - val_loss: 2.2977 - val_accuracy: 0.2969\n",
      "Epoch 51/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1986 - accuracy: 0.3217 - val_loss: 2.2924 - val_accuracy: 0.2979\n",
      "Epoch 52/110\n",
      "114914/114914 [==============================] - 1s 4us/sample - loss: 2.1940 - accuracy: 0.3221 - val_loss: 2.2929 - val_accuracy: 0.2952\n",
      "Epoch 53/110\n",
      "114914/114914 [==============================] - 1s 4us/sample - loss: 2.1927 - accuracy: 0.3244 - val_loss: 2.2982 - val_accuracy: 0.2942\n",
      "Epoch 54/110\n",
      "114914/114914 [==============================] - 1s 4us/sample - loss: 2.1880 - accuracy: 0.3242 - val_loss: 2.3014 - val_accuracy: 0.2945\n",
      "Epoch 55/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1871 - accuracy: 0.3251 - val_loss: 2.2949 - val_accuracy: 0.2989\n",
      "Epoch 56/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1840 - accuracy: 0.3249 - val_loss: 2.3022 - val_accuracy: 0.2976\n",
      "Epoch 57/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1852 - accuracy: 0.3244 - val_loss: 2.2970 - val_accuracy: 0.2960\n",
      "Epoch 58/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1815 - accuracy: 0.3267 - val_loss: 2.3020 - val_accuracy: 0.2947\n",
      "Epoch 59/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1793 - accuracy: 0.3266 - val_loss: 2.2976 - val_accuracy: 0.2982\n",
      "Epoch 60/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1758 - accuracy: 0.3276 - val_loss: 2.2976 - val_accuracy: 0.2971\n",
      "Epoch 61/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1745 - accuracy: 0.3287 - val_loss: 2.3002 - val_accuracy: 0.2984\n",
      "Epoch 62/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1761 - accuracy: 0.3271 - val_loss: 2.3018 - val_accuracy: 0.2977\n",
      "Epoch 63/110\n",
      "114914/114914 [==============================] - 1s 4us/sample - loss: 2.1717 - accuracy: 0.3280 - val_loss: 2.3009 - val_accuracy: 0.2974\n",
      "Epoch 64/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1677 - accuracy: 0.3290 - val_loss: 2.3040 - val_accuracy: 0.2981\n",
      "Epoch 65/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1716 - accuracy: 0.3289 - val_loss: 2.2993 - val_accuracy: 0.2968\n",
      "Epoch 66/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1691 - accuracy: 0.3300 - val_loss: 2.3045 - val_accuracy: 0.2987\n",
      "Epoch 67/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1643 - accuracy: 0.3308 - val_loss: 2.3065 - val_accuracy: 0.2983\n",
      "Epoch 68/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1644 - accuracy: 0.3306 - val_loss: 2.3044 - val_accuracy: 0.2950\n",
      "Epoch 69/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1628 - accuracy: 0.3296 - val_loss: 2.3041 - val_accuracy: 0.2977\n",
      "Epoch 70/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1603 - accuracy: 0.3325 - val_loss: 2.3010 - val_accuracy: 0.2982\n",
      "Epoch 71/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1600 - accuracy: 0.3309 - val_loss: 2.3019 - val_accuracy: 0.2961\n",
      "Epoch 72/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1604 - accuracy: 0.3310 - val_loss: 2.2972 - val_accuracy: 0.2989\n",
      "Epoch 73/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1565 - accuracy: 0.3318 - val_loss: 2.3035 - val_accuracy: 0.2955\n",
      "Epoch 74/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1558 - accuracy: 0.3326 - val_loss: 2.2990 - val_accuracy: 0.2994\n",
      "Epoch 75/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1538 - accuracy: 0.3330 - val_loss: 2.3004 - val_accuracy: 0.2969\n",
      "Epoch 76/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1528 - accuracy: 0.3326 - val_loss: 2.3059 - val_accuracy: 0.2976\n",
      "Epoch 77/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1514 - accuracy: 0.3338 - val_loss: 2.3082 - val_accuracy: 0.2974\n",
      "Epoch 78/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1510 - accuracy: 0.3339 - val_loss: 2.3121 - val_accuracy: 0.2954\n",
      "Epoch 79/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1485 - accuracy: 0.3349 - val_loss: 2.3060 - val_accuracy: 0.2941\n",
      "Epoch 80/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1469 - accuracy: 0.3345 - val_loss: 2.3049 - val_accuracy: 0.2992\n",
      "Epoch 81/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1478 - accuracy: 0.3350 - val_loss: 2.3087 - val_accuracy: 0.2952\n",
      "Epoch 82/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1451 - accuracy: 0.3346 - val_loss: 2.3093 - val_accuracy: 0.2945\n",
      "Epoch 83/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1440 - accuracy: 0.3354 - val_loss: 2.3028 - val_accuracy: 0.2974\n",
      "Epoch 84/110\n",
      "114914/114914 [==============================] - 1s 4us/sample - loss: 2.1402 - accuracy: 0.3380 - val_loss: 2.3154 - val_accuracy: 0.2953\n",
      "Epoch 85/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1389 - accuracy: 0.3387 - val_loss: 2.3035 - val_accuracy: 0.2979\n",
      "Epoch 86/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1382 - accuracy: 0.3371 - val_loss: 2.3041 - val_accuracy: 0.3001\n",
      "Epoch 87/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1353 - accuracy: 0.3369 - val_loss: 2.3054 - val_accuracy: 0.2957\n",
      "Epoch 88/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1363 - accuracy: 0.3368 - val_loss: 2.3083 - val_accuracy: 0.2964\n",
      "Epoch 89/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1329 - accuracy: 0.3389 - val_loss: 2.3180 - val_accuracy: 0.2956\n",
      "Epoch 90/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1357 - accuracy: 0.3370 - val_loss: 2.3016 - val_accuracy: 0.2962\n",
      "Epoch 91/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1342 - accuracy: 0.3372 - val_loss: 2.3111 - val_accuracy: 0.2967\n",
      "Epoch 92/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1338 - accuracy: 0.3386 - val_loss: 2.3059 - val_accuracy: 0.2968\n",
      "Epoch 93/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1310 - accuracy: 0.3386 - val_loss: 2.3095 - val_accuracy: 0.2958\n",
      "Epoch 94/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1276 - accuracy: 0.3391 - val_loss: 2.3150 - val_accuracy: 0.2956\n",
      "Epoch 95/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1280 - accuracy: 0.3394 - val_loss: 2.3205 - val_accuracy: 0.2953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1287 - accuracy: 0.3385 - val_loss: 2.3058 - val_accuracy: 0.2974\n",
      "Epoch 97/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1257 - accuracy: 0.3400 - val_loss: 2.3116 - val_accuracy: 0.2977\n",
      "Epoch 98/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1276 - accuracy: 0.3386 - val_loss: 2.3154 - val_accuracy: 0.2985\n",
      "Epoch 99/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1241 - accuracy: 0.3396 - val_loss: 2.3080 - val_accuracy: 0.2984\n",
      "Epoch 100/110\n",
      "114914/114914 [==============================] - 1s 4us/sample - loss: 2.1251 - accuracy: 0.3410 - val_loss: 2.3214 - val_accuracy: 0.2961\n",
      "Epoch 101/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1211 - accuracy: 0.3418 - val_loss: 2.3110 - val_accuracy: 0.2982\n",
      "Epoch 102/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1218 - accuracy: 0.3402 - val_loss: 2.3207 - val_accuracy: 0.2957\n",
      "Epoch 103/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1197 - accuracy: 0.3418 - val_loss: 2.3152 - val_accuracy: 0.2983\n",
      "Epoch 104/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1212 - accuracy: 0.3409 - val_loss: 2.3151 - val_accuracy: 0.2984\n",
      "Epoch 105/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1163 - accuracy: 0.3423 - val_loss: 2.3111 - val_accuracy: 0.2985\n",
      "Epoch 106/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1156 - accuracy: 0.3447 - val_loss: 2.3193 - val_accuracy: 0.2971\n",
      "Epoch 107/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1147 - accuracy: 0.3440 - val_loss: 2.3142 - val_accuracy: 0.2988\n",
      "Epoch 108/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1180 - accuracy: 0.3421 - val_loss: 2.3092 - val_accuracy: 0.2978\n",
      "Epoch 109/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1156 - accuracy: 0.3422 - val_loss: 2.3143 - val_accuracy: 0.2988\n",
      "Epoch 110/110\n",
      "114914/114914 [==============================] - 1s 5us/sample - loss: 2.1163 - accuracy: 0.3427 - val_loss: 2.3145 - val_accuracy: 0.2973\n",
      "Restoring the best weights from epoch 110\n",
      "Training fold 4\n",
      "Train on 114922 samples, validate on 28722 samples\n",
      "Epoch 1/110\n",
      "114922/114922 [==============================] - 1s 12us/sample - loss: 3.0549 - accuracy: 0.1062 - val_loss: 3.0360 - val_accuracy: 0.1525\n",
      "Epoch 2/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.7400 - accuracy: 0.1764 - val_loss: 2.8106 - val_accuracy: 0.1932\n",
      "Epoch 3/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.6054 - accuracy: 0.2077 - val_loss: 2.6588 - val_accuracy: 0.2097\n",
      "Epoch 4/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.5510 - accuracy: 0.2228 - val_loss: 2.5489 - val_accuracy: 0.2263\n",
      "Epoch 5/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.5183 - accuracy: 0.2324 - val_loss: 2.5056 - val_accuracy: 0.2391\n",
      "Epoch 6/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.4911 - accuracy: 0.2412 - val_loss: 2.4677 - val_accuracy: 0.2497\n",
      "Epoch 7/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.4698 - accuracy: 0.2485 - val_loss: 2.4455 - val_accuracy: 0.2546\n",
      "Epoch 8/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.4498 - accuracy: 0.2556 - val_loss: 2.4209 - val_accuracy: 0.2628\n",
      "Epoch 9/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.4301 - accuracy: 0.2617 - val_loss: 2.4058 - val_accuracy: 0.2624\n",
      "Epoch 10/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.4151 - accuracy: 0.2647 - val_loss: 2.3948 - val_accuracy: 0.2690\n",
      "Epoch 11/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.4031 - accuracy: 0.2687 - val_loss: 2.3913 - val_accuracy: 0.2698\n",
      "Epoch 12/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.3893 - accuracy: 0.2725 - val_loss: 2.3744 - val_accuracy: 0.2733\n",
      "Epoch 13/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.3775 - accuracy: 0.2772 - val_loss: 2.3657 - val_accuracy: 0.2798\n",
      "Epoch 14/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.3700 - accuracy: 0.2778 - val_loss: 2.3636 - val_accuracy: 0.2776\n",
      "Epoch 15/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.3603 - accuracy: 0.2800 - val_loss: 2.3623 - val_accuracy: 0.2792\n",
      "Epoch 16/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.3494 - accuracy: 0.2835 - val_loss: 2.3524 - val_accuracy: 0.2824\n",
      "Epoch 17/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.3386 - accuracy: 0.2862 - val_loss: 2.3441 - val_accuracy: 0.2839\n",
      "Epoch 18/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.3360 - accuracy: 0.2879 - val_loss: 2.3437 - val_accuracy: 0.2849\n",
      "Epoch 19/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.3259 - accuracy: 0.2892 - val_loss: 2.3424 - val_accuracy: 0.2835\n",
      "Epoch 20/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.3180 - accuracy: 0.2911 - val_loss: 2.3370 - val_accuracy: 0.2872\n",
      "Epoch 21/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.3125 - accuracy: 0.2927 - val_loss: 2.3277 - val_accuracy: 0.2872\n",
      "Epoch 22/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.3062 - accuracy: 0.2955 - val_loss: 2.3233 - val_accuracy: 0.2871\n",
      "Epoch 23/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.3034 - accuracy: 0.2960 - val_loss: 2.3262 - val_accuracy: 0.2905\n",
      "Epoch 24/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2931 - accuracy: 0.2986 - val_loss: 2.3282 - val_accuracy: 0.2887\n",
      "Epoch 25/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2927 - accuracy: 0.2981 - val_loss: 2.3236 - val_accuracy: 0.2878\n",
      "Epoch 26/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2878 - accuracy: 0.3004 - val_loss: 2.3172 - val_accuracy: 0.2894\n",
      "Epoch 27/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2805 - accuracy: 0.3012 - val_loss: 2.3173 - val_accuracy: 0.2875\n",
      "Epoch 28/110\n",
      "114922/114922 [==============================] - 1s 6us/sample - loss: 2.2751 - accuracy: 0.3037 - val_loss: 2.3182 - val_accuracy: 0.2925\n",
      "Epoch 29/110\n",
      "114922/114922 [==============================] - 1s 6us/sample - loss: 2.2718 - accuracy: 0.3060 - val_loss: 2.3207 - val_accuracy: 0.2899\n",
      "Epoch 30/110\n",
      "114922/114922 [==============================] - 1s 6us/sample - loss: 2.2682 - accuracy: 0.3055 - val_loss: 2.3145 - val_accuracy: 0.2908\n",
      "Epoch 31/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2638 - accuracy: 0.3067 - val_loss: 2.3160 - val_accuracy: 0.2884\n",
      "Epoch 32/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2606 - accuracy: 0.3082 - val_loss: 2.3119 - val_accuracy: 0.2929\n",
      "Epoch 33/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2583 - accuracy: 0.3086 - val_loss: 2.3193 - val_accuracy: 0.2893\n",
      "Epoch 34/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2523 - accuracy: 0.3106 - val_loss: 2.3140 - val_accuracy: 0.2904\n",
      "Epoch 35/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2490 - accuracy: 0.3091 - val_loss: 2.3134 - val_accuracy: 0.2902\n",
      "Epoch 36/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2488 - accuracy: 0.3117 - val_loss: 2.3160 - val_accuracy: 0.2934\n",
      "Epoch 37/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2469 - accuracy: 0.3109 - val_loss: 2.3130 - val_accuracy: 0.2910\n",
      "Epoch 38/110\n",
      "114922/114922 [==============================] - 1s 6us/sample - loss: 2.2415 - accuracy: 0.3132 - val_loss: 2.3041 - val_accuracy: 0.2978\n",
      "Epoch 39/110\n",
      "114922/114922 [==============================] - 1s 6us/sample - loss: 2.2372 - accuracy: 0.3158 - val_loss: 2.3098 - val_accuracy: 0.2914\n",
      "Epoch 40/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2375 - accuracy: 0.3147 - val_loss: 2.3130 - val_accuracy: 0.2917\n",
      "Epoch 41/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2334 - accuracy: 0.3154 - val_loss: 2.3086 - val_accuracy: 0.2921\n",
      "Epoch 42/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2297 - accuracy: 0.3161 - val_loss: 2.3113 - val_accuracy: 0.2941\n",
      "Epoch 43/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2270 - accuracy: 0.3158 - val_loss: 2.3140 - val_accuracy: 0.2952\n",
      "Epoch 44/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2272 - accuracy: 0.3162 - val_loss: 2.3022 - val_accuracy: 0.2956\n",
      "Epoch 45/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2215 - accuracy: 0.3188 - val_loss: 2.3097 - val_accuracy: 0.2951\n",
      "Epoch 46/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2176 - accuracy: 0.3170 - val_loss: 2.3075 - val_accuracy: 0.2951\n",
      "Epoch 47/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2198 - accuracy: 0.3185 - val_loss: 2.3061 - val_accuracy: 0.2936\n",
      "Epoch 48/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2134 - accuracy: 0.3190 - val_loss: 2.3071 - val_accuracy: 0.2972\n",
      "Epoch 49/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2126 - accuracy: 0.3190 - val_loss: 2.3051 - val_accuracy: 0.2964\n",
      "Epoch 50/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2117 - accuracy: 0.3202 - val_loss: 2.3069 - val_accuracy: 0.2959\n",
      "Epoch 51/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2053 - accuracy: 0.3215 - val_loss: 2.3056 - val_accuracy: 0.2945\n",
      "Epoch 52/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2032 - accuracy: 0.3233 - val_loss: 2.3044 - val_accuracy: 0.2970\n",
      "Epoch 53/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.2010 - accuracy: 0.3238 - val_loss: 2.3027 - val_accuracy: 0.2996\n",
      "Epoch 54/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1991 - accuracy: 0.3233 - val_loss: 2.3111 - val_accuracy: 0.2966\n",
      "Epoch 55/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1951 - accuracy: 0.3230 - val_loss: 2.3042 - val_accuracy: 0.2990\n",
      "Epoch 56/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1957 - accuracy: 0.3240 - val_loss: 2.3073 - val_accuracy: 0.2974\n",
      "Epoch 57/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1916 - accuracy: 0.3246 - val_loss: 2.3140 - val_accuracy: 0.2947\n",
      "Epoch 58/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1952 - accuracy: 0.3262 - val_loss: 2.3007 - val_accuracy: 0.3006\n",
      "Epoch 59/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1907 - accuracy: 0.3263 - val_loss: 2.3143 - val_accuracy: 0.2963\n",
      "Epoch 60/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1892 - accuracy: 0.3253 - val_loss: 2.3038 - val_accuracy: 0.3007\n",
      "Epoch 61/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1891 - accuracy: 0.3257 - val_loss: 2.3102 - val_accuracy: 0.2977\n",
      "Epoch 62/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1864 - accuracy: 0.3253 - val_loss: 2.3049 - val_accuracy: 0.2972\n",
      "Epoch 63/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1825 - accuracy: 0.3257 - val_loss: 2.3027 - val_accuracy: 0.3008\n",
      "Epoch 64/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1807 - accuracy: 0.3281 - val_loss: 2.3048 - val_accuracy: 0.2965\n",
      "Epoch 65/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1784 - accuracy: 0.3291 - val_loss: 2.3003 - val_accuracy: 0.2967\n",
      "Epoch 66/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1759 - accuracy: 0.3293 - val_loss: 2.3060 - val_accuracy: 0.2977\n",
      "Epoch 67/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1736 - accuracy: 0.3305 - val_loss: 2.3063 - val_accuracy: 0.2968\n",
      "Epoch 68/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1731 - accuracy: 0.3300 - val_loss: 2.3136 - val_accuracy: 0.2970\n",
      "Epoch 69/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1720 - accuracy: 0.3296 - val_loss: 2.3070 - val_accuracy: 0.2972\n",
      "Epoch 70/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1721 - accuracy: 0.3305 - val_loss: 2.3097 - val_accuracy: 0.2964\n",
      "Epoch 71/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1706 - accuracy: 0.3304 - val_loss: 2.3063 - val_accuracy: 0.2991\n",
      "Epoch 72/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1677 - accuracy: 0.3319 - val_loss: 2.3070 - val_accuracy: 0.2976\n",
      "Epoch 73/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1672 - accuracy: 0.3323 - val_loss: 2.3073 - val_accuracy: 0.2985\n",
      "Epoch 74/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1647 - accuracy: 0.3325 - val_loss: 2.3062 - val_accuracy: 0.2989\n",
      "Epoch 75/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1624 - accuracy: 0.3327 - val_loss: 2.3138 - val_accuracy: 0.2974\n",
      "Epoch 76/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1624 - accuracy: 0.3337 - val_loss: 2.3067 - val_accuracy: 0.2982\n",
      "Epoch 77/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1602 - accuracy: 0.3325 - val_loss: 2.3099 - val_accuracy: 0.2958\n",
      "Epoch 78/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1577 - accuracy: 0.3334 - val_loss: 2.3075 - val_accuracy: 0.2963\n",
      "Epoch 79/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1582 - accuracy: 0.3336 - val_loss: 2.3033 - val_accuracy: 0.2986\n",
      "Epoch 80/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1548 - accuracy: 0.3345 - val_loss: 2.3067 - val_accuracy: 0.2979\n",
      "Epoch 81/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1540 - accuracy: 0.3353 - val_loss: 2.3117 - val_accuracy: 0.2979\n",
      "Epoch 82/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1524 - accuracy: 0.3333 - val_loss: 2.3124 - val_accuracy: 0.2961\n",
      "Epoch 83/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1496 - accuracy: 0.3359 - val_loss: 2.3091 - val_accuracy: 0.2978\n",
      "Epoch 84/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1477 - accuracy: 0.3386 - val_loss: 2.3114 - val_accuracy: 0.2981\n",
      "Epoch 85/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1498 - accuracy: 0.3363 - val_loss: 2.3151 - val_accuracy: 0.2957\n",
      "Epoch 86/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1498 - accuracy: 0.3363 - val_loss: 2.3120 - val_accuracy: 0.2955\n",
      "Epoch 87/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1441 - accuracy: 0.3391 - val_loss: 2.3204 - val_accuracy: 0.2982\n",
      "Epoch 88/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1438 - accuracy: 0.3368 - val_loss: 2.3135 - val_accuracy: 0.2994\n",
      "Epoch 89/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1425 - accuracy: 0.3385 - val_loss: 2.3059 - val_accuracy: 0.2982\n",
      "Epoch 90/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1426 - accuracy: 0.3382 - val_loss: 2.3121 - val_accuracy: 0.2990\n",
      "Epoch 91/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1416 - accuracy: 0.3370 - val_loss: 2.3121 - val_accuracy: 0.2974\n",
      "Epoch 92/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1391 - accuracy: 0.3401 - val_loss: 2.3255 - val_accuracy: 0.2981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1404 - accuracy: 0.3392 - val_loss: 2.3142 - val_accuracy: 0.2988\n",
      "Epoch 94/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1388 - accuracy: 0.3401 - val_loss: 2.3170 - val_accuracy: 0.3011\n",
      "Epoch 95/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1368 - accuracy: 0.3405 - val_loss: 2.3137 - val_accuracy: 0.2953\n",
      "Epoch 96/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1346 - accuracy: 0.3402 - val_loss: 2.3111 - val_accuracy: 0.2976\n",
      "Epoch 97/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1327 - accuracy: 0.3415 - val_loss: 2.3156 - val_accuracy: 0.2983\n",
      "Epoch 98/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1345 - accuracy: 0.3394 - val_loss: 2.3121 - val_accuracy: 0.2967\n",
      "Epoch 99/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1310 - accuracy: 0.3408 - val_loss: 2.3104 - val_accuracy: 0.2972\n",
      "Epoch 100/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1330 - accuracy: 0.3406 - val_loss: 2.3137 - val_accuracy: 0.2928\n",
      "Epoch 101/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1290 - accuracy: 0.3408 - val_loss: 2.3155 - val_accuracy: 0.2982\n",
      "Epoch 102/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1276 - accuracy: 0.3415 - val_loss: 2.3159 - val_accuracy: 0.2975\n",
      "Epoch 103/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1284 - accuracy: 0.3420 - val_loss: 2.3098 - val_accuracy: 0.2970\n",
      "Epoch 104/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1273 - accuracy: 0.3420 - val_loss: 2.3215 - val_accuracy: 0.2969\n",
      "Epoch 105/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1249 - accuracy: 0.3427 - val_loss: 2.3171 - val_accuracy: 0.2978\n",
      "Epoch 106/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1259 - accuracy: 0.3417 - val_loss: 2.3180 - val_accuracy: 0.2967\n",
      "Epoch 107/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1221 - accuracy: 0.3435 - val_loss: 2.3170 - val_accuracy: 0.2958\n",
      "Epoch 108/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1203 - accuracy: 0.3444 - val_loss: 2.3127 - val_accuracy: 0.2978\n",
      "Epoch 109/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1231 - accuracy: 0.3422 - val_loss: 2.3153 - val_accuracy: 0.2972\n",
      "Epoch 110/110\n",
      "114922/114922 [==============================] - 1s 5us/sample - loss: 2.1205 - accuracy: 0.3425 - val_loss: 2.3227 - val_accuracy: 0.2942\n",
      "Restoring the best weights from epoch 110\n",
      "Training fold 5\n",
      "Train on 114925 samples, validate on 28719 samples\n",
      "Epoch 1/110\n",
      "114925/114925 [==============================] - 2s 13us/sample - loss: 3.0313 - accuracy: 0.1113 - val_loss: 2.9802 - val_accuracy: 0.1690\n",
      "Epoch 2/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.7047 - accuracy: 0.1940 - val_loss: 2.7808 - val_accuracy: 0.1891\n",
      "Epoch 3/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.5994 - accuracy: 0.2183 - val_loss: 2.6301 - val_accuracy: 0.2192\n",
      "Epoch 4/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.5385 - accuracy: 0.2323 - val_loss: 2.5560 - val_accuracy: 0.2349\n",
      "Epoch 5/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.5026 - accuracy: 0.2410 - val_loss: 2.4799 - val_accuracy: 0.2496\n",
      "Epoch 6/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.4743 - accuracy: 0.2504 - val_loss: 2.4593 - val_accuracy: 0.2567\n",
      "Epoch 7/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.4533 - accuracy: 0.2548 - val_loss: 2.4287 - val_accuracy: 0.2621\n",
      "Epoch 8/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.4381 - accuracy: 0.2594 - val_loss: 2.4153 - val_accuracy: 0.2655\n",
      "Epoch 9/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.4210 - accuracy: 0.2635 - val_loss: 2.4100 - val_accuracy: 0.2661\n",
      "Epoch 10/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.4081 - accuracy: 0.2675 - val_loss: 2.3937 - val_accuracy: 0.2715\n",
      "Epoch 11/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.3946 - accuracy: 0.2724 - val_loss: 2.3930 - val_accuracy: 0.2701\n",
      "Epoch 12/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.3865 - accuracy: 0.2746 - val_loss: 2.3830 - val_accuracy: 0.2711\n",
      "Epoch 13/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.3738 - accuracy: 0.2770 - val_loss: 2.3757 - val_accuracy: 0.2764\n",
      "Epoch 14/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.3638 - accuracy: 0.2803 - val_loss: 2.3725 - val_accuracy: 0.2762\n",
      "Epoch 15/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.3531 - accuracy: 0.2839 - val_loss: 2.3642 - val_accuracy: 0.2781\n",
      "Epoch 16/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.3461 - accuracy: 0.2841 - val_loss: 2.3624 - val_accuracy: 0.2813\n",
      "Epoch 17/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.3382 - accuracy: 0.2862 - val_loss: 2.3546 - val_accuracy: 0.2784\n",
      "Epoch 18/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.3316 - accuracy: 0.2889 - val_loss: 2.3516 - val_accuracy: 0.2809\n",
      "Epoch 19/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.3252 - accuracy: 0.2900 - val_loss: 2.3464 - val_accuracy: 0.2844\n",
      "Epoch 20/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.3170 - accuracy: 0.2934 - val_loss: 2.3456 - val_accuracy: 0.2831\n",
      "Epoch 21/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.3120 - accuracy: 0.2927 - val_loss: 2.3399 - val_accuracy: 0.2864\n",
      "Epoch 22/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.3037 - accuracy: 0.2964 - val_loss: 2.3344 - val_accuracy: 0.2867\n",
      "Epoch 23/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2998 - accuracy: 0.2980 - val_loss: 2.3364 - val_accuracy: 0.2851\n",
      "Epoch 24/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2927 - accuracy: 0.2996 - val_loss: 2.3313 - val_accuracy: 0.2855\n",
      "Epoch 25/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2869 - accuracy: 0.3013 - val_loss: 2.3293 - val_accuracy: 0.2863\n",
      "Epoch 26/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2825 - accuracy: 0.3015 - val_loss: 2.3179 - val_accuracy: 0.2904\n",
      "Epoch 27/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2764 - accuracy: 0.3043 - val_loss: 2.3317 - val_accuracy: 0.2893\n",
      "Epoch 28/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2744 - accuracy: 0.3040 - val_loss: 2.3235 - val_accuracy: 0.2907\n",
      "Epoch 29/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2678 - accuracy: 0.3048 - val_loss: 2.3200 - val_accuracy: 0.2911\n",
      "Epoch 30/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2640 - accuracy: 0.3049 - val_loss: 2.3164 - val_accuracy: 0.2949\n",
      "Epoch 31/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2595 - accuracy: 0.3068 - val_loss: 2.3210 - val_accuracy: 0.2925\n",
      "Epoch 32/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2576 - accuracy: 0.3079 - val_loss: 2.3307 - val_accuracy: 0.2897\n",
      "Epoch 33/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2522 - accuracy: 0.3098 - val_loss: 2.3152 - val_accuracy: 0.2928\n",
      "Epoch 34/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2479 - accuracy: 0.3104 - val_loss: 2.3172 - val_accuracy: 0.2943\n",
      "Epoch 35/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2438 - accuracy: 0.3120 - val_loss: 2.3204 - val_accuracy: 0.2936\n",
      "Epoch 36/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2414 - accuracy: 0.3123 - val_loss: 2.3190 - val_accuracy: 0.2922\n",
      "Epoch 37/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2401 - accuracy: 0.3131 - val_loss: 2.3132 - val_accuracy: 0.2926\n",
      "Epoch 38/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2345 - accuracy: 0.3134 - val_loss: 2.3130 - val_accuracy: 0.2930\n",
      "Epoch 39/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2332 - accuracy: 0.3142 - val_loss: 2.3148 - val_accuracy: 0.2957\n",
      "Epoch 40/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2264 - accuracy: 0.3160 - val_loss: 2.3161 - val_accuracy: 0.2957\n",
      "Epoch 41/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2251 - accuracy: 0.3156 - val_loss: 2.3119 - val_accuracy: 0.2945\n",
      "Epoch 42/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2205 - accuracy: 0.3181 - val_loss: 2.3097 - val_accuracy: 0.2948\n",
      "Epoch 43/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2187 - accuracy: 0.3181 - val_loss: 2.3177 - val_accuracy: 0.2938\n",
      "Epoch 44/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2161 - accuracy: 0.3181 - val_loss: 2.3203 - val_accuracy: 0.2959\n",
      "Epoch 45/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2143 - accuracy: 0.3196 - val_loss: 2.3109 - val_accuracy: 0.2971\n",
      "Epoch 46/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2101 - accuracy: 0.3205 - val_loss: 2.3081 - val_accuracy: 0.2982\n",
      "Epoch 47/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2117 - accuracy: 0.3189 - val_loss: 2.3168 - val_accuracy: 0.2950\n",
      "Epoch 48/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2064 - accuracy: 0.3213 - val_loss: 2.3089 - val_accuracy: 0.2977\n",
      "Epoch 49/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2044 - accuracy: 0.3210 - val_loss: 2.3131 - val_accuracy: 0.2976\n",
      "Epoch 50/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2031 - accuracy: 0.3221 - val_loss: 2.3078 - val_accuracy: 0.2972\n",
      "Epoch 51/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.2005 - accuracy: 0.3227 - val_loss: 2.3153 - val_accuracy: 0.2970\n",
      "Epoch 52/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1970 - accuracy: 0.3238 - val_loss: 2.3161 - val_accuracy: 0.2957\n",
      "Epoch 53/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1971 - accuracy: 0.3248 - val_loss: 2.3205 - val_accuracy: 0.2960\n",
      "Epoch 54/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1937 - accuracy: 0.3248 - val_loss: 2.3277 - val_accuracy: 0.2906\n",
      "Epoch 55/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1913 - accuracy: 0.3256 - val_loss: 2.3121 - val_accuracy: 0.2979\n",
      "Epoch 56/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1878 - accuracy: 0.3255 - val_loss: 2.3063 - val_accuracy: 0.2975\n",
      "Epoch 57/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1870 - accuracy: 0.3266 - val_loss: 2.3068 - val_accuracy: 0.2963\n",
      "Epoch 58/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1838 - accuracy: 0.3270 - val_loss: 2.3152 - val_accuracy: 0.2953\n",
      "Epoch 59/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1833 - accuracy: 0.3276 - val_loss: 2.3152 - val_accuracy: 0.2915\n",
      "Epoch 60/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1809 - accuracy: 0.3269 - val_loss: 2.3173 - val_accuracy: 0.2961\n",
      "Epoch 61/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1782 - accuracy: 0.3288 - val_loss: 2.3084 - val_accuracy: 0.3015\n",
      "Epoch 62/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1780 - accuracy: 0.3294 - val_loss: 2.3093 - val_accuracy: 0.2996\n",
      "Epoch 63/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1737 - accuracy: 0.3291 - val_loss: 2.3153 - val_accuracy: 0.2951\n",
      "Epoch 64/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1724 - accuracy: 0.3300 - val_loss: 2.3126 - val_accuracy: 0.2979\n",
      "Epoch 65/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1755 - accuracy: 0.3286 - val_loss: 2.3188 - val_accuracy: 0.2978\n",
      "Epoch 66/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1666 - accuracy: 0.3320 - val_loss: 2.3111 - val_accuracy: 0.2967\n",
      "Epoch 67/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1690 - accuracy: 0.3309 - val_loss: 2.3189 - val_accuracy: 0.2963\n",
      "Epoch 68/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1666 - accuracy: 0.3299 - val_loss: 2.3089 - val_accuracy: 0.2981\n",
      "Epoch 69/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1675 - accuracy: 0.3309 - val_loss: 2.3166 - val_accuracy: 0.2962\n",
      "Epoch 70/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1667 - accuracy: 0.3305 - val_loss: 2.3128 - val_accuracy: 0.2992\n",
      "Epoch 71/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1617 - accuracy: 0.3334 - val_loss: 2.3141 - val_accuracy: 0.2975\n",
      "Epoch 72/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1590 - accuracy: 0.3336 - val_loss: 2.3180 - val_accuracy: 0.2976\n",
      "Epoch 73/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1594 - accuracy: 0.3337 - val_loss: 2.3141 - val_accuracy: 0.2976\n",
      "Epoch 74/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1554 - accuracy: 0.3346 - val_loss: 2.3128 - val_accuracy: 0.2967\n",
      "Epoch 75/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1565 - accuracy: 0.3345 - val_loss: 2.3218 - val_accuracy: 0.2961\n",
      "Epoch 76/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1529 - accuracy: 0.3356 - val_loss: 2.3220 - val_accuracy: 0.2979\n",
      "Epoch 77/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1502 - accuracy: 0.3360 - val_loss: 2.3150 - val_accuracy: 0.2983\n",
      "Epoch 78/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1533 - accuracy: 0.3350 - val_loss: 2.3136 - val_accuracy: 0.2993\n",
      "Epoch 79/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1495 - accuracy: 0.3356 - val_loss: 2.3169 - val_accuracy: 0.2977\n",
      "Epoch 80/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1486 - accuracy: 0.3360 - val_loss: 2.3162 - val_accuracy: 0.2951\n",
      "Epoch 81/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1471 - accuracy: 0.3361 - val_loss: 2.3197 - val_accuracy: 0.2960\n",
      "Epoch 82/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1449 - accuracy: 0.3379 - val_loss: 2.3260 - val_accuracy: 0.2954\n",
      "Epoch 83/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1452 - accuracy: 0.3374 - val_loss: 2.3185 - val_accuracy: 0.2988\n",
      "Epoch 84/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1398 - accuracy: 0.3374 - val_loss: 2.3162 - val_accuracy: 0.2999\n",
      "Epoch 85/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1422 - accuracy: 0.3391 - val_loss: 2.3236 - val_accuracy: 0.2950\n",
      "Epoch 86/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1390 - accuracy: 0.3386 - val_loss: 2.3167 - val_accuracy: 0.2990\n",
      "Epoch 87/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1413 - accuracy: 0.3374 - val_loss: 2.3255 - val_accuracy: 0.2948\n",
      "Epoch 88/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1364 - accuracy: 0.3404 - val_loss: 2.3243 - val_accuracy: 0.2970\n",
      "Epoch 89/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1370 - accuracy: 0.3396 - val_loss: 2.3212 - val_accuracy: 0.2957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1358 - accuracy: 0.3406 - val_loss: 2.3237 - val_accuracy: 0.2957\n",
      "Epoch 91/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1341 - accuracy: 0.3401 - val_loss: 2.3159 - val_accuracy: 0.2963\n",
      "Epoch 92/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1335 - accuracy: 0.3408 - val_loss: 2.3273 - val_accuracy: 0.2958\n",
      "Epoch 93/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1327 - accuracy: 0.3411 - val_loss: 2.3181 - val_accuracy: 0.2965\n",
      "Epoch 94/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1333 - accuracy: 0.3410 - val_loss: 2.3222 - val_accuracy: 0.2967\n",
      "Epoch 95/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1300 - accuracy: 0.3419 - val_loss: 2.3190 - val_accuracy: 0.2954\n",
      "Epoch 96/110\n",
      "114925/114925 [==============================] - 1s 4us/sample - loss: 2.1300 - accuracy: 0.3428 - val_loss: 2.3199 - val_accuracy: 0.2957\n",
      "Epoch 97/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1264 - accuracy: 0.3426 - val_loss: 2.3259 - val_accuracy: 0.2951\n",
      "Epoch 98/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1244 - accuracy: 0.3435 - val_loss: 2.3220 - val_accuracy: 0.2959\n",
      "Epoch 99/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1276 - accuracy: 0.3421 - val_loss: 2.3220 - val_accuracy: 0.2932\n",
      "Epoch 100/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1230 - accuracy: 0.3431 - val_loss: 2.3219 - val_accuracy: 0.2981\n",
      "Epoch 101/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1209 - accuracy: 0.3446 - val_loss: 2.3294 - val_accuracy: 0.2974\n",
      "Epoch 102/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1233 - accuracy: 0.3428 - val_loss: 2.3232 - val_accuracy: 0.2963\n",
      "Epoch 103/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1225 - accuracy: 0.3425 - val_loss: 2.3240 - val_accuracy: 0.2950\n",
      "Epoch 104/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1202 - accuracy: 0.3427 - val_loss: 2.3285 - val_accuracy: 0.2968\n",
      "Epoch 105/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1198 - accuracy: 0.3436 - val_loss: 2.3273 - val_accuracy: 0.2944\n",
      "Epoch 106/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1213 - accuracy: 0.3424 - val_loss: 2.3221 - val_accuracy: 0.2986\n",
      "Epoch 107/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1172 - accuracy: 0.3441 - val_loss: 2.3282 - val_accuracy: 0.2949\n",
      "Epoch 108/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1167 - accuracy: 0.3453 - val_loss: 2.3318 - val_accuracy: 0.2956\n",
      "Epoch 109/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1131 - accuracy: 0.3452 - val_loss: 2.3229 - val_accuracy: 0.2981\n",
      "Epoch 110/110\n",
      "114925/114925 [==============================] - 1s 5us/sample - loss: 2.1139 - accuracy: 0.3457 - val_loss: 2.3292 - val_accuracy: 0.2958\n",
      "Restoring the best weights from epoch 110\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>avg_val_f1_micro</th>\n",
       "      <td>0.297646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_val_f1_macro</th>\n",
       "      <td>0.274868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_val_accuracy</th>\n",
       "      <td>0.297646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_batch_val_accuracy</th>\n",
       "      <td>0.297646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_accuracy</th>\n",
       "      <td>0.325711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_val_loss</th>\n",
       "      <td>2.298860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_loss</th>\n",
       "      <td>2.188799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_time</th>\n",
       "      <td>329.797179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0\n",
       "avg_val_f1_micro          0.297646\n",
       "avg_val_f1_macro          0.274868\n",
       "avg_val_accuracy          0.297646\n",
       "avg_batch_val_accuracy    0.297646\n",
       "avg_accuracy              0.325711\n",
       "avg_val_loss              2.298860\n",
       "avg_loss                  2.188799\n",
       "train_time              329.797179"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_fold(x_train, y_train, x_val, y_val, run_name, fold=None):\n",
    "\n",
    "    input_shape = x_train[0].shape\n",
    "    model = create_model(input_shape)\n",
    "    \n",
    "    if fold is not None:\n",
    "        run_name += '_fold' + str(fold)\n",
    "        print('Training fold ' + str(fold))\n",
    "    \n",
    "    tk_board = TensorBoard(log_dir=constants.LOGS_PATH + run_name)\n",
    "    weight_restorer = BestWeightsRestorer(monitor='val_loss', mode='min', verbose=1)\n",
    "    \n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        validation_data=[x_val, y_val],\n",
    "        epochs=110,\n",
    "        batch_size=2000,\n",
    "        shuffle=True,\n",
    "        callbacks=[tk_board, weight_restorer],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    prob_predictions = model.predict(x_val)\n",
    "    predictions = prob_predictions.argmax(axis=1)\n",
    "    \n",
    "    return {\n",
    "        'history': history.history,\n",
    "        'predictions': predictions\n",
    "    }\n",
    "\n",
    "experiment_number += 1\n",
    "run_name = 'new_base_mlp_marsyas_cv_' + str(experiment_number)\n",
    "\n",
    "results = resultsUtil.cross_validate(\n",
    "    x=ftrs,\n",
    "    y=lbls_oh,\n",
    "    y_label_encoded=lbl_encoded_lbls,\n",
    "    n_splits=5,\n",
    "    train_func=train_fold,\n",
    "    run_name=run_name\n",
    ")\n",
    "\n",
    "runUtil.save_run_results(run_name, results)\n",
    "\n",
    "resultsdf = pd.DataFrame([results]).transpose()\n",
    "\n",
    "display(resultsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/marc/anaconda3/envs/ml_main/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/marc/anaconda3/envs/ml_main/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/marc/anaconda3/envs/ml_main/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/110\n",
      "143644/143644 [==============================] - 2s 11us/sample - loss: 2.9934 - accuracy: 0.1244\n",
      "Epoch 2/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.6700 - accuracy: 0.1967\n",
      "Epoch 3/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.5723 - accuracy: 0.2176\n",
      "Epoch 4/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.5252 - accuracy: 0.2315\n",
      "Epoch 5/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.4901 - accuracy: 0.2421\n",
      "Epoch 6/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.4645 - accuracy: 0.2506\n",
      "Epoch 7/110\n",
      "143644/143644 [==============================] - 1s 7us/sample - loss: 2.4398 - accuracy: 0.2566\n",
      "Epoch 8/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.4231 - accuracy: 0.2625\n",
      "Epoch 9/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.4078 - accuracy: 0.2671\n",
      "Epoch 10/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.3922 - accuracy: 0.2722\n",
      "Epoch 11/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.3768 - accuracy: 0.2777\n",
      "Epoch 12/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.3657 - accuracy: 0.2789\n",
      "Epoch 13/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.3550 - accuracy: 0.2826\n",
      "Epoch 14/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.3439 - accuracy: 0.2859\n",
      "Epoch 15/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.3369 - accuracy: 0.2875\n",
      "Epoch 16/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.3284 - accuracy: 0.2890\n",
      "Epoch 17/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.3178 - accuracy: 0.2910\n",
      "Epoch 18/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.3117 - accuracy: 0.2938\n",
      "Epoch 19/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.3042 - accuracy: 0.2956\n",
      "Epoch 20/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.3012 - accuracy: 0.2963\n",
      "Epoch 21/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2946 - accuracy: 0.2983\n",
      "Epoch 22/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2872 - accuracy: 0.2996\n",
      "Epoch 23/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2838 - accuracy: 0.3004\n",
      "Epoch 24/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2796 - accuracy: 0.3030\n",
      "Epoch 25/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2746 - accuracy: 0.3037\n",
      "Epoch 26/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2706 - accuracy: 0.3036\n",
      "Epoch 27/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2664 - accuracy: 0.3051\n",
      "Epoch 28/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2608 - accuracy: 0.3070\n",
      "Epoch 29/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2585 - accuracy: 0.3073\n",
      "Epoch 30/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2540 - accuracy: 0.3075\n",
      "Epoch 31/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2501 - accuracy: 0.3095\n",
      "Epoch 32/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2487 - accuracy: 0.3103\n",
      "Epoch 33/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2452 - accuracy: 0.3113\n",
      "Epoch 34/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2409 - accuracy: 0.3110\n",
      "Epoch 35/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2398 - accuracy: 0.3136\n",
      "Epoch 36/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2373 - accuracy: 0.3144\n",
      "Epoch 37/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2330 - accuracy: 0.3147\n",
      "Epoch 38/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2308 - accuracy: 0.3152\n",
      "Epoch 39/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2290 - accuracy: 0.3164\n",
      "Epoch 40/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2241 - accuracy: 0.3171\n",
      "Epoch 41/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2211 - accuracy: 0.3171\n",
      "Epoch 42/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2215 - accuracy: 0.3171\n",
      "Epoch 43/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2178 - accuracy: 0.3178\n",
      "Epoch 44/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2176 - accuracy: 0.3186\n",
      "Epoch 45/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2140 - accuracy: 0.3197\n",
      "Epoch 46/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2116 - accuracy: 0.3203\n",
      "Epoch 47/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2099 - accuracy: 0.3207\n",
      "Epoch 48/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2057 - accuracy: 0.3223\n",
      "Epoch 49/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2053 - accuracy: 0.3223\n",
      "Epoch 50/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2048 - accuracy: 0.3229\n",
      "Epoch 51/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2004 - accuracy: 0.3233\n",
      "Epoch 52/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2019 - accuracy: 0.3233\n",
      "Epoch 53/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.2001 - accuracy: 0.3234\n",
      "Epoch 54/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1950 - accuracy: 0.3244\n",
      "Epoch 55/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1975 - accuracy: 0.3242\n",
      "Epoch 56/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1938 - accuracy: 0.3244\n",
      "Epoch 57/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1902 - accuracy: 0.3246\n",
      "Epoch 58/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1904 - accuracy: 0.3257\n",
      "Epoch 59/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1898 - accuracy: 0.3255\n",
      "Epoch 60/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1863 - accuracy: 0.3259\n",
      "Epoch 61/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1873 - accuracy: 0.3275\n",
      "Epoch 62/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1837 - accuracy: 0.3277\n",
      "Epoch 63/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1835 - accuracy: 0.3282\n",
      "Epoch 64/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1802 - accuracy: 0.3282\n",
      "Epoch 65/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1802 - accuracy: 0.3289\n",
      "Epoch 66/110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1761 - accuracy: 0.3300\n",
      "Epoch 67/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1771 - accuracy: 0.3300\n",
      "Epoch 68/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1756 - accuracy: 0.3298\n",
      "Epoch 69/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1741 - accuracy: 0.3300\n",
      "Epoch 70/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1724 - accuracy: 0.3310\n",
      "Epoch 71/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1713 - accuracy: 0.3301\n",
      "Epoch 72/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1695 - accuracy: 0.3307\n",
      "Epoch 73/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1686 - accuracy: 0.3312\n",
      "Epoch 74/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1686 - accuracy: 0.3315\n",
      "Epoch 75/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1666 - accuracy: 0.3317\n",
      "Epoch 76/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1639 - accuracy: 0.3326\n",
      "Epoch 77/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1644 - accuracy: 0.3326\n",
      "Epoch 78/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1639 - accuracy: 0.3325\n",
      "Epoch 79/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1619 - accuracy: 0.3337\n",
      "Epoch 80/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1605 - accuracy: 0.3336\n",
      "Epoch 81/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1607 - accuracy: 0.3341\n",
      "Epoch 82/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1579 - accuracy: 0.3347\n",
      "Epoch 83/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1583 - accuracy: 0.3338\n",
      "Epoch 84/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1584 - accuracy: 0.3346\n",
      "Epoch 85/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1556 - accuracy: 0.3352\n",
      "Epoch 86/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1543 - accuracy: 0.3352\n",
      "Epoch 87/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1523 - accuracy: 0.3361\n",
      "Epoch 88/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1531 - accuracy: 0.3343\n",
      "Epoch 89/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1509 - accuracy: 0.3355\n",
      "Epoch 90/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1498 - accuracy: 0.3355\n",
      "Epoch 91/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1501 - accuracy: 0.3352\n",
      "Epoch 92/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1455 - accuracy: 0.3383\n",
      "Epoch 93/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1455 - accuracy: 0.3371\n",
      "Epoch 94/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1468 - accuracy: 0.3374\n",
      "Epoch 95/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1459 - accuracy: 0.3389\n",
      "Epoch 96/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1435 - accuracy: 0.3384\n",
      "Epoch 97/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1445 - accuracy: 0.3379\n",
      "Epoch 98/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1425 - accuracy: 0.3392\n",
      "Epoch 99/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1413 - accuracy: 0.3391\n",
      "Epoch 100/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1415 - accuracy: 0.3401\n",
      "Epoch 101/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1377 - accuracy: 0.3405\n",
      "Epoch 102/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1394 - accuracy: 0.3394\n",
      "Epoch 103/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1408 - accuracy: 0.3382\n",
      "Epoch 104/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1393 - accuracy: 0.3396\n",
      "Epoch 105/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1359 - accuracy: 0.3401\n",
      "Epoch 106/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1371 - accuracy: 0.3409\n",
      "Epoch 107/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1361 - accuracy: 0.3411\n",
      "Epoch 108/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1327 - accuracy: 0.3424\n",
      "Epoch 109/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1326 - accuracy: 0.3409\n",
      "Epoch 110/110\n",
      "143644/143644 [==============================] - 1s 6us/sample - loss: 2.1339 - accuracy: 0.3408\n"
     ]
    }
   ],
   "source": [
    "def train_final_model(x, y, run_name):\n",
    "    \n",
    "    save_path = constants.MODELS_PATH + run_name + '.h5' \n",
    "    \n",
    "    input_shape = x[0].shape\n",
    "    model = create_model(input_shape)\n",
    "    \n",
    "    tk_board = TensorBoard(log_dir=constants.LOGS_PATH + run_name)\n",
    "    \n",
    "    model.fit(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        epochs=110,\n",
    "        batch_size=2000,\n",
    "        shuffle=True,\n",
    "        callbacks=[tk_board],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model.save(save_path)\n",
    "\n",
    "\n",
    "train_final_model(ftrs, lbls_oh, 'base_mlp_marsyas_final3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
