{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratoire 4 : Développement d’un système intelligent\n",
    "#### Département du génie logiciel et des technologies de l’information\n",
    "\n",
    "| Étudiants             | Alexandre Laroche - LARA12078907<br>Marc-Antoine Charland - CHAM16059609<br>Jonathan Croteau-Dicaire - CROJ10109402    |\n",
    "|-----------------------|---------------------------------------------------------|\n",
    "| Cours                 | GTI770 - Systèmes intelligents et apprentissage machine |\n",
    "| Session               | Été 2019                                            |\n",
    "| Groupe                | 02                                                      |\n",
    "| Numéro du laboratoire | TP-04                                                   |\n",
    "| Professeur            | Prof. Alessandro L. Koarich                             |\n",
    "| Chargé de laboratoire | Pierre-Luc Delisle                                                     |\n",
    "| Date                  | 5 août 2019 (23h55)                                                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\" Classes externes\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "# According to pep8, import should be structures as :\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "from os import path\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import time\n",
    "# A single empty line\n",
    "\n",
    "# 3rd party library imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# This project's modules imports\n",
    "import src.constants as constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\" Constantes\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "# Repertoires des fichiers CSV (musique)\n",
    "derivatives_file_path = constants.PROJECT_ROOT_PATH + 'data/raw/music/tagged_feature_sets/msd-jmirderivatives_dev/msd-jmirderivatives_dev.csv'\n",
    "marsyas_file_path = 'data/raw/music/tagged_feature_sets/msd-marsyas_dev_new/msd-marsyas_dev_new.csv'\n",
    "ssd_file_path = path.join(constants.DATA_PATH, 'ssd_base_split.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\" Functions\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "def load_feature_set(file_path):\n",
    "    # Lecture du fichier CSV\n",
    "    x = np.array(pd.read_csv(file_path, header=None).values[:,2:-1])\n",
    "    y = np.array(pd.read_csv(file_path, header=None).values[:,-1])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\n",
    "def scale_features(x):\n",
    "    # Normalisation des features\n",
    "    scaler = StandardScaler()\n",
    "    x = scaler.fit_transform(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def one_hot_labels(y):\n",
    "    # Hot encodage des labels\n",
    "    y_simple = y.reshape(len(y), 1)\n",
    "    hot_encoder = OneHotEncoder(sparse=False)\n",
    "    y_hot_encoded = hot_encoder.fit_transform(y_simple)\n",
    "    \n",
    "    return y_hot_encoded\n",
    "\n",
    "\n",
    "def label_encode(y):\n",
    "    # Encodage des labels\n",
    "    encoder = LabelEncoder()\n",
    "    y_encoded = encoder.fit_transform(y)\n",
    "    \n",
    "    return y_encoded\n",
    "\n",
    "\n",
    "def compare_lda_score(x, y, random_forest, lda_params):\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    accuracy_results = []\n",
    "    f1_results = []\n",
    "    \n",
    "    for param in lda_params:\n",
    "    \n",
    "        # Performing LDA\n",
    "        lda = LDA(n_components = lda_params[param])\n",
    "        x_train = lda.fit_transform(x_train, y_train)\n",
    "        x_test = lda.transform(x_test)\n",
    "\n",
    "        # Entrainement et prediction du modele\n",
    "        random_forest = rf\n",
    "        random_forest.fit(x_train, y_train)\n",
    "        y_pred = classifier.predict(x_test)\n",
    "\n",
    "        # Evalue la performance\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average = 'weighted', labels = np.unique(y_pred))\n",
    "        \n",
    "        # Insere les resultats dans les listes\n",
    "        accuracy_results.append(accuracy)\n",
    "        f1_results.append(f1)\n",
    "        \n",
    "    # Construction du tableau comparatif\n",
    "    data = {\n",
    "        \"LDA n_components\":lda_params,\n",
    "        \"Accuracy\":accuracy_results,\n",
    "        \"F1 score\":f1_results\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.set_index(\"LDA n_components\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectures CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivatives dimension :  96\n",
      "Derivatives songs qty. :  179555\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\" CSV - Derivatives\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "# Get the data from the CSV\n",
    "derivatives_x, derivatives_y = load_feature_set(derivatives_file_path)\n",
    "\n",
    "# Normalise les features\n",
    "derivatives_x = scale_features(derivatives_x)\n",
    "\n",
    "print(\"Derivatives dimension : \", derivatives_x.shape[1])\n",
    "print(\"Derivatives songs qty. : \", len(derivatives_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marsyas dimension :  124\n",
      "Marsyas songs qty. :  179555\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\" CSV - Marsyas\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "# Get the data from the CSV\n",
    "marsyas_x, marsyas_y = load_feature_set(marsyas_file_path)\n",
    "\n",
    "# Normalise les features\n",
    "marsyas_x = scale_features(marsyas_x)\n",
    "\n",
    "print(\"Marsyas dimension : \", marsyas_x.shape[1])\n",
    "print(\"Marsyas songs qty. : \", len(marsyas_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSD dimension :  168\n",
      "SSD songs qty. :  143644\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\" CSV - SSD\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "# Get the data from the CSV\n",
    "ssd_x, ssd_y = load_feature_set(ssd_file_path)\n",
    "\n",
    "# Normalise les features\n",
    "ssd_x = scale_features(ssd_x)\n",
    "\n",
    "print(\"SSD dimension : \", ssd_x.shape[1])\n",
    "print(\"SSD songs qty. : \", len(ssd_x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification des quantités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-68dddca19f06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Recupere les classes uniques\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0munique_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssd_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Affiche les classes uniques\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\" Class unique\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "# Recupere les classes uniques\n",
    "unique_class = list(set(ssd_y))\n",
    "encoder.fit(unique_class)\n",
    "\n",
    "# Affiche les classes uniques\n",
    "print(\"Classes musicales (\", len(unique_class), \") :\\n\\n\", np.array(unique_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['BIG_BAND', 'BLUES_CONTEMPORARY', 'COUNTRY_TRADITIONAL', 'DANCE',\n",
      "       'ELECTRONICA', 'EXPERIMENTAL', 'FOLK_INTERNATIONAL', 'GOSPEL',\n",
      "       'GRUNGE_EMO', 'HIP_HOP_RAP', 'JAZZ_CLASSIC', 'METAL_ALTERNATIVE',\n",
      "       'METAL_DEATH', 'METAL_HEAVY', 'POP_CONTEMPORARY', 'POP_INDIE',\n",
      "       'POP_LATIN', 'PUNK', 'REGGAE', 'RNB_SOUL', 'ROCK_ALTERNATIVE',\n",
      "       'ROCK_COLLEGE', 'ROCK_CONTEMPORARY', 'ROCK_HARD',\n",
      "       'ROCK_NEO_PSYCHEDELIA'], dtype=object), array([ 2047,  4511,  7316,  9885,  7148,  7932,  6465,  4580,  4096,\n",
      "       10581,  6568,  9195,  6485,  7031,  8959, 11858,  5048,  6306,\n",
      "        3433,  4107,  8333, 10856, 10829,  8720,  7266]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(derivatives_y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ssd_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9cf55c3a09a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Recupere la colonne des classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclass_proportion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssd_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssd_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Proportion des classes de SSD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ssd_data' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\" Class proportion\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "# Recupere la colonne des classes\n",
    "class_proportion = ssd_data[len(ssd_data.columns) - 1]\n",
    "\n",
    "# Proportion des classes de SSD\n",
    "pd.DataFrame(class_proportion.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - Evaluate best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\" SSD - Train & Test\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(ssd_x, ssd_y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest - Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marc/anaconda3/envs/ml_main/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SSD - Best default Random Forest score ***\n",
      "\n",
      "Accuracy:  0.2035866447606583\n",
      "F1 score:  0.1916519990896498\n",
      "\n",
      "*** Random Forest default parameters ***\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 10,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 42,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\" SSD - Default Random Forest\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "# Training and predictions\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "start_fit_time = time.perf_counter()\n",
    "\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "fit_time = time.perf_counter() - start_fit_time\n",
    "\n",
    "y_pred = rf.predict(x_test)\n",
    "\n",
    "# Evaluating the performance\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1_micro = f1_score(y_test, y_pred, average = 'micro', labels = np.unique(y_pred))\n",
    "f1_macro = f1_score(y_test, y_pred, average = 'macro', labels = np.unique(y_pred))\n",
    "\n",
    "print(\"*** SSD - Best default Random Forest score ***\\n\")\n",
    "print(\"Validation accuracy: \", accuracy)\n",
    "print(\"F1 score micro: \", f1_micro)\n",
    "print(\"F1 score macro: \", f1_macro)\n",
    "\n",
    "print('\\n*** Random Forest default parameters ***\\n')\n",
    "pprint(rf.get_params())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest - Find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\" SSD - Params to evaluate\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "model_params = {\n",
    "    'bootstrap': [True, False],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5],\n",
    "    'min_samples_split': [2, 4, 8, 10],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'criterion': [\"gini\", \"entropy\"],\n",
    "    'max_depth': [None, 10, 30, 50, 70, 80],\n",
    "    'n_estimators': [10, 30, 60, 80, 100]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed: 16.5min\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed: 119.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2 of 80\n",
      "building tree 3 of 80\n",
      "building tree 4 of 80\n",
      "building tree 5 of 80\n",
      "building tree 6 of 80\n",
      "building tree 7 of 80\n",
      "building tree 8 of 80\n",
      "building tree 9 of 80\n",
      "building tree 10 of 80\n",
      "building tree 11 of 80\n",
      "building tree 12 of 80\n",
      "building tree 13 of 80\n",
      "building tree 14 of 80\n",
      "building tree 15 of 80\n",
      "building tree 16 of 80\n",
      "building tree 17 of 80\n",
      "building tree 18 of 80\n",
      "building tree 19 of 80\n",
      "building tree 20 of 80\n",
      "building tree 21 of 80\n",
      "building tree 22 of 80\n",
      "building tree 23 of 80\n",
      "building tree 24 of 80\n",
      "building tree 25 of 80\n",
      "building tree 26 of 80\n",
      "building tree 27 of 80\n",
      "building tree 28 of 80\n",
      "building tree 29 of 80\n",
      "building tree 30 of 80\n",
      "building tree 31 of 80\n",
      "building tree 32 of 80\n",
      "building tree 33 of 80\n",
      "building tree 34 of 80\n",
      "building tree 35 of 80\n",
      "building tree 36 of 80\n",
      "building tree 37 of 80\n",
      "building tree 38 of 80\n",
      "building tree 39 of 80\n",
      "building tree 40 of 80\n",
      "building tree 41 of 80\n",
      "building tree 42 of 80\n",
      "building tree 43 of 80\n",
      "building tree 44 of 80\n",
      "building tree 45 of 80\n",
      "building tree 46 of 80\n",
      "building tree 47 of 80\n",
      "building tree 48 of 80\n",
      "building tree 49 of 80\n",
      "building tree 50 of 80\n",
      "building tree 51 of 80\n",
      "building tree 52 of 80\n",
      "building tree 53 of 80\n",
      "building tree 54 of 80\n",
      "building tree 55 of 80\n",
      "building tree 56 of 80\n",
      "building tree 57 of 80\n",
      "building tree 58 of 80\n",
      "building tree 59 of 80\n",
      "building tree 60 of 80\n",
      "building tree 61 of 80\n",
      "building tree 62 of 80\n",
      "building tree 63 of 80\n",
      "building tree 64 of 80\n",
      "building tree 65 of 80\n",
      "building tree 66 of 80\n",
      "building tree 67 of 80\n",
      "building tree 68 of 80\n",
      "building tree 69 of 80\n",
      "building tree 70 of 80\n",
      "building tree 71 of 80\n",
      "building tree 72 of 80\n",
      "building tree 73 of 80\n",
      "building tree 74 of 80\n",
      "building tree 75 of 80\n",
      "building tree 76 of 80\n",
      "building tree 77 of 80\n",
      "building tree 78 of 80\n",
      "building tree 79 of 80\n",
      "building tree 80 of 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:  4.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators='warn',\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_sc...\n",
       "                   iid='warn', n_iter=25, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'criterion': ['gini', 'entropy'],\n",
       "                                        'max_depth': [None, 10, 30, 50, 70, 80],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 3, 4, 5],\n",
       "                                        'min_samples_split': [2, 4, 8, 10],\n",
       "                                        'n_estimators': [10, 30, 60, 80, 100]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\" SSD - Find best params\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "# Initialisation du modele\n",
    "rf_model = RandomForestClassifier(verbose=2)\n",
    "\n",
    "# Cherche a travers 100 combinaisons avec un fold = 5\n",
    "clf = RandomizedSearchCV(\n",
    "    cv = 5,\n",
    "    n_jobs = -1,\n",
    "    n_iter = 25,\n",
    "    estimator = rf_model,\n",
    "    param_distributions = model_params,\n",
    "    verbose = 2\n",
    ")\n",
    "\n",
    "# Fit le model\n",
    "clf.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 80, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 80, 'criterion': 'gini', 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\" SSD - Get the best params\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "bootstrap         = clf.best_params_['bootstrap']\n",
    "min_samples_leaf  = clf.best_params_['min_samples_leaf']\n",
    "min_samples_split = clf.best_params_['min_samples_split']\n",
    "max_features      = clf.best_params_['max_features']   \n",
    "criterion         = clf.best_params_['criterion']\n",
    "max_depth         = clf.best_params_['max_depth']\n",
    "n_estimators      = clf.best_params_['n_estimators']\n",
    "\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest - Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SSD - Optimized Random Forest score ***\n",
      "\n",
      "Validation accuracy:  0.28462529151728216\n",
      "F1 score micro:  0.28462529151728216\n",
      "F1 score macro:  0.2583912545134321\n",
      "fit time:  199.56459134200122\n",
      "\n",
      "*** Random Forest optimized parameters ***\n",
      "\n",
      "{'bootstrap': False,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 80,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 4,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 80,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': None,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\" SSD - Optimized Random Forest\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "bootstrap = False\n",
    "min_samples_leaf = 1\n",
    "min_samples_split = 4\n",
    "max_features = 'auto'\n",
    "criterion = 'gini'\n",
    "max_depth = 80\n",
    "n_estimators = 80\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(bootstrap = bootstrap,\n",
    "                            min_samples_leaf = min_samples_leaf,\n",
    "                            min_samples_split = min_samples_split,\n",
    "                            max_features = max_features,\n",
    "                            criterion = criterion,\n",
    "                            max_depth = max_depth,\n",
    "                            n_estimators = n_estimators)\n",
    "\n",
    "# Garde une copie du modele original (optimisé) et on le Fit\n",
    "random_forest = rf\n",
    "\n",
    "start_fit_time = time.perf_counter()\n",
    "\n",
    "random_forest.fit(x_train, y_train)\n",
    "\n",
    "fit_time = time.perf_counter() - start_fit_time\n",
    "\n",
    "y_pred = rf.predict(x_test)\n",
    "\n",
    "# Evalue la performance\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "f1_micro = f1_score(y_test, y_pred, average = 'micro', labels = np.unique(y_pred))\n",
    "f1_macro = f1_score(y_test, y_pred, average = 'macro', labels = np.unique(y_pred))\n",
    "\n",
    "print(\"*** SSD - Optimized Random Forest score ***\\n\")\n",
    "print(\"Validation accuracy: \", accuracy)\n",
    "print(\"F1 score micro: \", f1_micro)\n",
    "print(\"F1 score macro: \", f1_macro)\n",
    "print('fit time: ', str(fit_time))\n",
    "\n",
    "print('\\n*** Random Forest optimized parameters ***\\n')\n",
    "pprint(rf.get_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 80building tree 2 of 80\n",
      "building tree 3 of 80\n",
      "\n",
      "building tree 4 of 80\n",
      "building tree 5 of 80\n",
      "building tree 6 of 80\n",
      "building tree 7 of 80\n",
      "building tree 8 of 80\n",
      "building tree 9 of 80\n",
      "building tree 10 of 80\n",
      "building tree 11 of 80\n",
      "building tree 12 of 80\n",
      "building tree 13 of 80\n",
      "building tree 14 of 80\n",
      "building tree 15 of 80\n",
      "building tree 16 of 80\n",
      "building tree 17 of 80\n",
      "building tree 18 of 80\n",
      "building tree 19 of 80\n",
      "building tree 20 of 80\n",
      "building tree 21 of 80\n",
      "building tree 22 of 80\n",
      "building tree 23 of 80\n",
      "building tree 24 of 80\n",
      "building tree 25 of 80\n",
      "building tree 26 of 80\n",
      "building tree 27 of 80\n",
      "building tree 28 of 80\n",
      "building tree 29 of 80\n",
      "building tree 30 of 80\n",
      "building tree 31 of 80\n",
      "building tree 32 of 80\n",
      "building tree 33 of 80\n",
      "building tree 34 of 80\n",
      "building tree 35 of 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   20.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 36 of 80\n",
      "building tree 37 of 80\n",
      "building tree 38 of 80\n",
      "building tree 39 of 80\n",
      "building tree 40 of 80\n",
      "building tree 41 of 80\n",
      "building tree 42 of 80\n",
      "building tree 43 of 80\n",
      "building tree 44 of 80\n",
      "building tree 45 of 80\n",
      "building tree 46 of 80\n",
      "building tree 47 of 80\n",
      "building tree 48 of 80\n",
      "building tree 49 of 80\n",
      "building tree 50 of 80\n",
      "building tree 51 of 80\n",
      "building tree 52 of 80\n",
      "building tree 53 of 80\n",
      "building tree 54 of 80\n",
      "building tree 55 of 80\n",
      "building tree 56 of 80\n",
      "building tree 57 of 80\n",
      "building tree 58 of 80\n",
      "building tree 59 of 80\n",
      "building tree 60 of 80\n",
      "building tree 61 of 80\n",
      "building tree 62 of 80\n",
      "building tree 63 of 80\n",
      "building tree 64 of 80\n",
      "building tree 65 of 80\n",
      "building tree 66 of 80\n",
      "building tree 67 of 80\n",
      "building tree 68 of 80\n",
      "building tree 69 of 80\n",
      "building tree 70 of 80\n",
      "building tree 71 of 80\n",
      "building tree 72 of 80\n",
      "building tree 73 of 80\n",
      "building tree 74 of 80\n",
      "building tree 75 of 80\n",
      "building tree 76 of 80\n",
      "building tree 77 of 80\n",
      "building tree 78 of 80\n",
      "building tree 79 of 80\n",
      "building tree 80 of 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:   53.5s finished\n"
     ]
    }
   ],
   "source": [
    "# train final RandomForest model on the whole dataset\n",
    "\n",
    "final_rf = RandomForestClassifier(\n",
    "    bootstrap = bootstrap,\n",
    "    min_samples_leaf = min_samples_leaf,\n",
    "    min_samples_split = min_samples_split,\n",
    "    max_features = max_features,\n",
    "    criterion = criterion,\n",
    "    max_depth = max_depth,\n",
    "    n_estimators = n_estimators,\n",
    "    n_jobs = -1,\n",
    "    verbose = 2\n",
    ")\n",
    "\n",
    "final_rf.fit(ssd_x, ssd_y)\n",
    "\n",
    "# Sérialise le modèle\n",
    "model_file_path = path.join(constants.MODELS_PATH, 'final_random_forest_2.pickle')\n",
    "pickle.dump(final_rf, open(model_file_path, 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - Evaluate best dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest - Default PCA & LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\" SSD - Default PCA\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "# Performing PCA\n",
    "pca = PCA()\n",
    "x_train = pca.fit_transform(x_train, y_train)\n",
    "x_test = pca.transform(x_test)\n",
    "\n",
    "# Evite d'utiliser la copie original du modele et on l'entraine\n",
    "random_forest =  RandomForestClassifier(\n",
    "    bootstrap = bootstrap,\n",
    "    min_samples_leaf = min_samples_leaf,\n",
    "    min_samples_split = min_samples_split,\n",
    "    max_features = max_features,\n",
    "    criterion = criterion,\n",
    "    max_depth = max_depth,\n",
    "    n_estimators = n_estimators,\n",
    "    n_jobs = -1,\n",
    "    verbose = 2\n",
    ")\n",
    "random_forest.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "# Evaluating the performance\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1_micro = f1_score(y_test, y_pred, average = 'micro', labels = np.unique(y_pred))\n",
    "f1_macro = f1_score(y_test, y_pred, average = 'macro', labels = np.unique(y_pred))\n",
    "\n",
    "print(\"*** Best SSD Score (default PCA) ***\\n\")\n",
    "print(\"Validation accuracy: \", accuracy)\n",
    "print(\"F1 score micro: \", f1_micro)\n",
    "print(\"F1 score macro: \", f1_macro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\" SSD - Default LDA\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "# Performing LDA\n",
    "lda = LDA()\n",
    "x_train = lda.fit_transform(x_train, y_train)\n",
    "x_test = lda.transform(x_test)\n",
    "\n",
    "# Evite d'utiliser la copie original du modele et on l'entraine\n",
    "random_forest = RandomForestClassifier(\n",
    "    bootstrap = bootstrap,\n",
    "    min_samples_leaf = min_samples_leaf,\n",
    "    min_samples_split = min_samples_split,\n",
    "    max_features = max_features,\n",
    "    criterion = criterion,\n",
    "    max_depth = max_depth,\n",
    "    n_estimators = n_estimators,\n",
    "    n_jobs = -1,\n",
    "    verbose = 2\n",
    ")\n",
    "random_forest.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "# Evalue la performance\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average = 'weighted', labels = np.unique(y_pred))\n",
    "\n",
    "print(\"*** Best SSD Score (default LDA) ***\\n\")\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"F1 score: \", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest - Evaluate best LDA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA n_components</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.241514</td>\n",
       "      <td>0.205522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.243017</td>\n",
       "      <td>0.212112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.241013</td>\n",
       "      <td>0.204155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Accuracy  F1 score\n",
       "LDA n_components                    \n",
       "14                0.241514  0.205522\n",
       "16                0.243017  0.212112\n",
       "18                0.241013  0.204155"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\" SSD - Best LDA params\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "# Evite d'utiliser la copie original du modele\n",
    "randomforest = rf\n",
    "\n",
    "# Affiche un tableau des resultats selon les parametres LDA entrés\n",
    "compare_lda_score(ssd_x, ssd_y, [20, 22, 24, 26, 28, 30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\"Derivatives - Best LDA params\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "# Evite d'utiliser la copie original du modele\n",
    "randomforest = rf\n",
    "\n",
    "# Affiche un tableau des resultats selon les parametres LDA entrés\n",
    "compare_lda_score(derivatives_x, derivatives_y, [12, 14, 16, 18, 20, 22])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\" Marsyas - Best LDA params\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "# Evite d'utiliser la copie original du modele\n",
    "randomforest = rf\n",
    "\n",
    "# Affiche un tableau des resultats selon les parametres LDA entrés\n",
    "compare_lda_score(marsyas_x, marsyas_y, [12, 14, 16, 18, 20, 22])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
